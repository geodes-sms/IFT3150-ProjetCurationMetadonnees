title	abstract	keywords	authors	venue	doi	references	pages	bibtex	source	year	meta_title	link	publisher
Role-Playing Computer Game to Improve Speech Ability of Down Syndrome Children	"Down syndrome individuals are known to have difficulties in speech, both in pronunciation of words and making
sentences, due to the innate physical impairment of their mouth muscles and their cognitive and intelligence
limitations. An interesting finding by other researchers shows that the vocabulary acquisition rates of these individuals fall
between the range of average to slow, and they hardly use good grammar, although they seem to have good
understanding of casual conversation. However, most research efforts have been dedicated to enhancing their words
recognition ability and computer applications have been developed for reading therapy, whereas the syntax and
expressive language training are rarely focused. In this paper, we propose the use of role playing computer game featuring
state-of-the-art speech recognition technology as an attempt to improve the speech accuracy of down syndrome children in
terms of grammar. The challenges involved the input received in coordinating the speech accuracy. We provide systematic
review on some of the base elements to consider while developing the game for the mildly-impaired down syndrome
individuals, which includes speech recognition technology, cognitive psychology, instructional design models, system
development life cycle, and multimedia in special education."	Down Syndrome; Multimedia Applications; Serious Game; Speech Training	Che Pee, A. N.; Abdullah, M. H. L.; Zakaria, M. H.; Asyrani, H.; Rahim, S. S.; Chuan, L. C.	International Journal of Advanced Trends in Computer Science and Engineering	https://www.warse.org/IJATCSE/static/pdf/file/ijatcse337942020.pdf				Google Scholar	2020	Role-Playing Computer Game To Improve Speech Ability Of Down Syndrome Children	https://www.warse.org/IJATCSE/static/pdf/file/ijatcse337942020.pdf	Warse
Usability evaluation of the domain specific language for spatial simulation scenarios	This article presents the results of a usability evaluation initiative conducted on the Domain Specific Language for Spatial Simulation Scenarios (short name DSL3S) and its supporting tools. This language applies a Model-Driven Development approach to spatial simulation, providing model development through the composition of graphical elements and the subsequent transformation to source code. Potential users trained in disciplines related to Geographic Information Systems were exposed for a first time to the language with an introductory exercise. After installing the supporting tools and developing a simple spatial simulation model, participants then evaluated the language and its tools by answering a questionnaire. The results of this evaluation point to a good degree of usability, with particularly positive appreciations of the DSL3S supporting tools. Notwithstanding, participants also show some reluctance in adopting such a development framework, hinting at some reminiscent scepticism towards domain specific modelling languages and Model-Driven Development.	spatial simulation; domain specific modelling language; UML profile; model-driven development; model driven architecture (MDA)	de Sousa, Luis Moreira; da Silva, Alberto Rodrigues	Cogent Engineering	https://doi.org/10.1080/23311916.2018.1436889				Google Scholar	2018	Usability Evaluation Of The Domain Specific Language For Spatial Simulation Scenarios	https://www.tandfonline.com/doi/full/10.1080/23311916.2018.1436889	Taylor & Francis
Stay Awhile and Listen to 3Buddy, a Co-creative Level Design Support Tool	"There is untapped potential in having a computer work as a colleague with the video game level designer as a source of
creative stimuli, instead of simply working as his slave. This paper presents 3Buddy, a co-creative level design tool exploring this digital peer paradigm, aimed at fostering creativity by allowing human and computer to work together in the context of level design, and describes a case study of the approach to
produce content using the Legend of Grimrock 2 level editor. Suggestions are generated and iteratively evolved by multiple inter-communicating genetic algorithms guiding three different domains: innovation (exploring new directions), guidelines (respecting specific design goals) and convergence (focusing on current co-proposal). The interface allows the designer to orient the tool behaviour in the space defined by these dimensions. This paper details the inner workings of the system and presents an exploratory study showing, on the one hand, how the tool was used differently by professional and amateur level designers, and on the other hand, how the nuances of the co-creative interaction through an intentionoriented interface may be a source of positive influence for the creative level design process."	Artificial intelligence; Computer games; Genetic algorithms; Iterative methods; Computer work; Design goal; Different domains; Exploratory studies; Level design; On currents; Tool behaviour; Video game levels; Design	Lucas, P.; Martinho, C.	Proceedings of the 8th International Conference on Computational Creativity, ICCC 2017	https://computationalcreativity.net/iccc2017/ICCC_17_accepted_submissions/ICCC-17_paper_12.pdf				Google Scholar	2017	Stay Awhile And Listen To 3Buddy, A Co-Creative Level Design Support Tool	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109115583&partnerID=40&md5=1e9d81c53be9976a39ee411ae64bf60b	INESC-ID and Instituto Superior Tecnico
Pricing and Upgrade Strategies for Software Vendors in the Software-as-a-Service Market 	"This study examines the competition between an independent software vendor (ISV) who offers a standard fixed-price software product and a cloud software vendor (CSV)
who adopts a usage-based linear pricing scheme for its cloud-based software. Using a game theoretic approach, we set up a duopoly model to derive the optimal pricing and
product feature choice decisions for both the ISV and the CSV in a market characterized with heterogeneous consumer preferences for software features, and different risk
structures. Next, we plan to extend the duopoly model into a two-stage game and examine how the equilibrium outcomes change when we allow both vendors to exercise
their software upgrade strategies. The results of our study will provide important implications to vendors and consumers in software markets where the Software-as-aService (SaaS) business model began to prevail and contribute to the growing literature on economics of cloud computing. "	Cloud computing; Commerce; Competition; Computation theory; Computer games; Costs; Economic analysis; Game theory; Information systems; Information use; Web services; Business modeling; Consumer preferences; Economics of cloud computing; Economics of information; Independent software vendors; Price-analysis; SaaS; Software features; Software as a service (SaaS)	Liu, C. Z.; Au, Y. A.; Ayaburi, E.	Thirty Sixth International Conference on Information Systems	https://web.archive.org/web/20160130133525id_/http://aisel.aisnet.org:80/cgi/viewcontent.cgi?article=1411&context=icis2015		11		Google Scholar	2015	Pricing And Upgrade Strategies For Software Vendors In The Software-As-A-Service Market	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107082355&partnerID=40&md5=66888e78193af9d8426263327ef7ab26	Fort Wort
Kinematic Simulation of a Line Follower Robot for the Creation of the Programming Videogame Rusty Roads in the Unity Framework	This paper presents the results of a research project for developing Rusty Roads, a videogame that simulates line follower robots in tridimensional, competitive environments. During the software development process, it was necessary to define the key physical factors that could affect the performance of the robot. A real robot was considered to define real parameters for the simulation, and to define discrete equations for simulating the robot in the Unity framework, comparing their accuracy with that of continuous, exact equations specifically developed for this study. This paper presents the process for obtaining and testing those equations, which were the foundations for the successful development of the simulation layer of the videogame and allowed the subsequent creation of its 3D visualization component.	video games; simulation; robotics; line followers; mechanics; robot	Oscar F. Gomez; Urbano E. Gomez	Informacion tecnologica	http://dx.doi.org/10.4067/S0718-07642017000500008				Google Scholar	2017	Simulacia3N Cinema!Tica De Un Robot Seguidor De LaNea Para El Desarrollo Del Videojuego De Programacia3N Rusty Roads En El Framework Unity	https://www.scielo.cl/scielo.php?script=sci_arttext&pid=S0718-07642017000500008&lng=en&nrm=iso&tlng=en	SciELO
Network Time Protocol Version 4: Protocol and Algorithms Specification	The Network Time Protocol (NTP) is widely used to synchronize computer clocks in the Internet. This document describes NTP version 4 (NTPv4), which is backwards compatible with NTP version 3 (NTPv3), described in RFC 1305, as well as previous versions of the protocol. NTPv4 includes a modified protocol header to accommodate the Internet Protocol version 6 address family. NTPv4 includes fundamental improvements in the mitigation and discipline algorithms that extend the potential accuracy to the tens of microseconds with modern workstations and fast LANs. It includes a dynamic server discovery scheme, so that in many cases, specific server configuration is not required. It corrects certain errors in the NTPv3 design and implementation and includes an optional extension mechanism. [STANDARDS-TRACK]		Mills, D.; J. Martin, Ed.; Burbank, J.; Kasch, W.		https://doi.org/10.17487/RFC5905				Google Scholar	2010	Network Time Protocol Version 4: Protocol and Algorithms Specification	https://www.rfc-editor.org/info/rfc5905	RFC Editor
The Design, Play, and Experience Framework	This chapter introduces a framework for the design of serious games for learning, called the design, play, and experience framework. The author argues that the great potential of serious games will not be realized without a formal design approach. To that end, the author presents and thoroughly explains the design, play, and experience framework which provides a formal approach to designing the learning, storytelling, game play, user experience, and technology components of a serious game. The author concludes by detailing how the framework provides a common language to discuss serious game design, a methodology to analyze a design, and a process to design a serious game for learning.	Prototyping; Serious Games; Iterative Game Design; Heart of Serious Game Design; Exogenous Educational Games; Balancing; Endogenous Educational Games; Game Design; Game Mechanics; Game Dynamics	Winn, Brian	Proceedings of the ACM on Human-Computer Interaction (PACMHCI), Volume 7, Issue CHI PLAY	https://doi.org/10.4018/978-1-59904-808-6.CH058		15		Google Scholar	2009	The Design, Play, and Experience Framework	https://www.igi-global.com/chapter/design-play-experience-framework/20133	IGI Global Reasearch
"From Knowing It to ""Getting It"": Envisioning Practices in Computer Games Development"	"The development of information systems and software applications increasingly needs to deliver culturally rich and affective experiences for user groups. In this paper, we explore how the collaborative practices across different expert groups can enable this experiential dimension of use to be integrated into the development of a software product. In an empirical study of computer games development an arena in which the novelty and richness of the user experience is central to competitive success, we identify the challenges of conceptualizing and realizing a desired user experience when it cannot be readily specified in an initial design template, nor represented within the expertise of existing groups. Our study develops a theoretical framework to address these challenges. Through this framework, we are able to show how achieving a desired user experience requires developer groups to not only work across the boundaries that arise from specialized expertise, but also across wider fields centred on cultural production and software development, respectively. We find that their ability to do this is supported by distinctive ""envisioning practices"" that sustain an emerging shared ""vision"" for each game. The key research contributions that we then make are (a) grounding envisioning practices as a means of theorizing the collaborative practices centred on conceptualizing the user experience; (b) identifying how these practices are interwoven with the ""producing practices"" of software development, thus enabling collaboration to span expert groups and disparate fields; and (c) theorizing the role of vision as an emerging conceptual boundary object in these practices."	collaborative practice; envizioning; interpretive; computer games development; emergence	Nandhakumar, Joe; Panourgias, Nikiforos S.; Scarbrough, Harry	Information Systems Research	https://doi.org/10.1287/isre.2013.0482		933-955		Google Scholar	2013	"From Knowing It to ""Getting It"": Envisioning Practices in Computer Games Development"	https://pubsonline.informs.org/doi/10.1287/isre.2013.0482	Institute for Operations Research and the Management Sciences
"Controlling the uncontrollable: ""Agile"" teams and illusions of autonomy in creative work"	"The creative industries have recently been hailed as presenting a liberating model for the future of work and a valuable terrain on which to examine purported new regimes of workplace control. This article, based on the empirical examination of a Canadian video game development studio, traces the modes of control which operate on and through project teams in creative settings. The impact of the adoption of an ""emancipatory"", post-bureaucratic project management technology, ""Agile"", is critically examined through interviews and non-participative observation of management, technical and artistic labour within one project team. The potential for autonomy in such ""Agile"" teams is critically assessed within the managerial regime of creative production and the broader power relations implied by the financial, organizational and institutional context."		Hodgson, Damian; Briand, Louise	Work, Employment and Society	https://doi.org/10.1177/0950017012460315				Google Scholar	2013	"Controlling the uncontrollable: ""Agile"" teams and illusions of autonomy in creative work"	https://journals.sagepub.com/doi/full/10.1177/0950017012460315?casa_token=G-C_d0fPy78AAAAA%3Ayy58QtV0Slb683uh32WHxvq99ydiee92W1YtKcnNS66g2zzSjw2H44cE8Y8D9oZ6QJXlDUugspYDymo	Sage Journals
"The politics of ""platforms"""	"Online content providers such as YouTube are carefully positioning themselves to users, clients, advertisers and policymakers, making strategic claims for what they do and do not do, and how their place in the information landscape should be understood. One term in particular, ""platform"", reveals the contours of this discursive work. The term has been deployed in both their populist appeals and their marketing pitches, sometimes as technical ""platforms"", sometimes as ""platforms"" from which to speak, sometimes as ""platforms"" of opportunity. Whatever tensions exist in serving all of these constituencies are carefully elided. The term also fits their efforts to shape information policy, where they seek protection for facilitating user expression, yet also seek limited liability for what those users say. As these providers become the curators of public discourse, we must examine the roles they aim to play, and the terms by which they hope to be judged."		Gillespie, Tarleton	New Media & Society	https://doi.org/10.1177/1461444809342738				Google Scholar	2010	"A Companion to New Media Dynamics - The politics of ""platforms"""	https://journals.sagepub.com/doi/10.1177/1461444809342738	Sage Journals
Danish TV Christmas calendars: Folklore, myth and cultural history	"This article aims at characterizing the Danish Christmas calendar as a TV institution and a meeting place for the traditions of the almanac, folklore and the history of culture. Against the background of a brief outline of the history of Danish Christmas calendars, the article explores ways in which this traditional genre has succeeded in renewing itself. The so-called Pyrus series, TV 2's Christmas calendars during the mid-1990s, exhibited folklore, myth and cultural history in a combination of entertainment and information. They were succeeded by calendars such as Jul i Valhal/""Christmas in Valhalla"" (2005), Absalons hemmelighed/""The Secret of Absalon"" (2006), Mikkel og guldkortet/""Mikkel and the Golden Card"" (2008) and Pagten/""The Covenant"" (2009). Some of these added cultural criticism to the repertoire of the genre."		Agger, Gunhild	Journal of Scandinavian Cinema	https://doi.org/10.1386/jsca.3.3.267_1		267 - 280		Google Scholar	2013	Danish TV Christmas calendars: Folklore, myth and cultural history	https://intellectdiscover.com/content/journals/10.1386/jsca.3.3.267_1	Intellect Discover
Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems	"The prevailing approach to free/open source software and licenses has been that each system is developed, distributed, and used under the terms of a single license. But it is increasingly common for information systems and other software to be composed with components from a variety of sources, and with a diversity of licenses. This may result in possible license conflicts and organizational liability for failure to fulfill license obligations. Research and practice to date have not kept up with this sea-change in software licensing arising from free/open source software development. System consumers and users consequently rely on ad hoc heuristics (or costly legal advice) to determine which license rights and obligations are in effect, often with less than optimal results; consulting services are offered to identify unknowing unauthorized use of licensed software in information systems; and researchers have shown how the choice of a (single) specific license for a product affects project success and system adoption. Legal scholars have examined how pairs of software licenses conflict but only in simple contexts. We present an approach for understanding and modeling software licenses, as well as for analyzing conflicts among groups of licenses in realistic system contexts, and for guiding the acquisition, integration, or development of systems with free/open source components in such an environment. This work is based on an empirical analysis of representative software licenses and of heterogeneously-licensed systems. Our approach provides guidance for achieving a ""best-of-breed"" component strategy while obtaining desired license rights in exchange for acceptable obligations."	Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems	Alspaugh, Thomas A.; Scacchi, Walt; Asuncion, Hazeline U.	SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems	https://aisel.aisnet.org/jais/vol11/iss11/2/				Google Scholar	2010	Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems	https://aisel.aisnet.org/jais/vol11/iss11/2/	Association for Information Systems
Method for Game Development Driven by User-eXperience: A Study of Rework, Productivity and Complexity of Use	The growing capabilities and revenues of video game development are important factors for software companies. However, game development processes could be considered immature, specifically in the design phase. Ambiguous requirements in game design cause rework. User-eXperience (UX) is usually assessed at the end of the development process, causing difficulties to ensure the interactive experience between the game and users. To reduce these problems, this paper proposes a method for Game Development driven by User-eXperience (GameD-UX) that integrates a repository based on requirements engineering, a model for user experience management, and an adjusted agile process. Two experiments were conducted to study rework and productivity of video game development. Results of the first experiment revealed that GameD-UX causes less rework than conventional approaches, but it induces lower productivity. A tool for supporting the GameD-UX method was developed by considering the lessons learned. The second experiment showed that the software tool increases the productivity and reduces the complexity of use of GameD-UX.	Rework; Productivity; Complexity of Use; Video Game Development	Gonzalez-Salazar, Mario; Mitre-Hernandez, Hugo; Lara-Alvarez, Carlos	(IJACSA) International Journal of Advanced Computer Science and Applications	https://dx.doi.org/10.14569/IJACSA.2017.080250		9		Google Scholar	2017	Method For Game Development Driven By User-Experience: A Study Of Rework, Productivity And Complexity Of Use	https://www.researchgate.net/profile/Carlos-Lara-Alvarez/publication/314163800_Method_for_Game_Development_Driven_by_User-eXperience_a_Study_of_Rework_Productivity_and_Complexity_of_Use/links/59540e95a6fdcc1697893e96/Method-for-Game-Development-Driven-by-User-eXperience-a-Study-of-Rework-Productivity-and-Complexity-of-Use.pdf	The Science and Information Organization
E.GRAPHIC (GAMES RESEARCH APPLIED TO PUBLIC HEALTH WITH INNOVATIVE COLLABORATION) - DESIGNING A SERIOUS GAME PILOT FOR DENTAL PUBLIC HEALTH	Educators are looking towards new methods of engaging and motivating students and encouraging self-directed life-long learning in preparing undergraduates to enter the dental profession. This paper's objective is to report the development of an online collaborative serious game as an e-learning Dental Public Health resource for dental undergraduates. GRAPHIC involved the development of a pilot computer game programme. The game was housed within the Moodle environment. In the game students worked collaboratively to consider the views of key stakeholders in the community and individually to explore population oral health, and the evidence base for community initiatives. The pilot programme was used by students in January 2012 with positive feedback. The learning outcomes of the game were achieved with all students successfully passing the game. The development of a pilot game has demonstrated the effective use of technology enhanced learning in dental public health. The pilot programme demonstrates the potential of using gaming as a teaching tool in dental public health. Learning from the pilot is contributing to further developments including an improved user interface.	Dentistry; Dental Public Health; Computer Assisted Learning; Serious gaming	O'Neill, E.; Reynolds, P. A.; Hatzipanagos, S.; Gallagher, J. E.	Bulletin du Groupement international pour la recherche scientifique en stomatologie & odontologie	https://revistes.ub.edu/index.php/bullgirso/article/view/6202		2		Google Scholar	2013	Graphic (Games Research Applied To Public Health With Innovative Collaboration)--Designing A Serious Game Pilot For Dental Public Health.	https://revistes.ub.edu/index.php/bullgirso/article/view/6202	Revistes cientifiques de la Universitat de Barcelona
Defining an Iterative ISO/IEC 29110 Deployment Package for Game Developers	"Software development in a small development team is a challenge, as people have to fulfill several roles, which in larger groups would have dedicated people. To help in this aspect, the ISO/IEC 29110 Lifecycle profiles for Very Small Entities has been developed to help organization and manage the workflow. However, the model presented in the ISO/IEC 29110 is rather abstract, and prominently follows the waterfall approach, even though the documents do amend agile practices as one acceptable approach. In game development this loosely defined approach is problematic, since games industry heavily relies in the agile practices with short cycles of iterations. In this article, the authors present their study of game development organizations, and describe the ISO/IEC 29110 deployment package ""Highly Iterative Software Processes"" which combines the Entry level model with the industry-specific requirements. In general, the definition of support for the iterative development makes the model feasible for the industry."		Kasurinen, Jussi; Smolander, Kari	International Journal of Information Technologies and Systems Approach	https://www.igi-global.com/article/defining-an-iterative-isoiec-29110-deployment-package-for-game-developers/169770		19		Google Scholar	2017	Defining An Iterative Iso/Iec 29110 Deployment Package For Game Developers	https://www.igi-global.com/article/defining-an-iterative-isoiec-29110-deployment-package-for-game-developers/169770	IGI Global
Introduction of a process maturity model for market-driven product management and requirements engineering	The area of software product development of software intensive products has received much attention, especially in the area of requirements engineering and product management. Many companies are faced with new challenges when operating in an environment where potential requirements number in thousands or even tens of thousands, and where a product does not have a customer, but any number of customers or markets. The development organization carries not only all the costs of development, but also takes all the risks. In this environment traditional bespoke requirements engineering, together with traditional process assessment and improvement models fall short as they do not address the unique challenges of a market-driven environment. This paper introduces the Market-driven Requirements Engineering Process Model, aimed at enabling process improvement and process assurance for organizations faced with these new challenges. The model is also validated in the industry through three case studies where the model is used for process assessment and improvement suggestion. Initial results show that the model is appropriate for process improvement for organizations operating in a market-driven environment. In addition, the model was designed to be light weight in terms of low cost and thus adapted not only for large organizations but suitable for small and medium enterprises as well.		Gorschek, Tony; Gomes, Andrigo; Pettersson, Andreas; Torkar, Richard	TT '06: Proceedings of the 2006 international workshop on Software technology transfer in software engineering	https://onlinelibrary.wiley.com/doi/10.1002/smr.535				Google Scholar	2011	Introduction of a process maturity model for market-driven product management and requirements engineering	https://onlinelibrary.wiley.com/doi/10.1002/smr.535	Wiley Online Library
How BioWare's Anthem Went Wrong			Runcieman, Michael		https://kotaku.com/how-biowares-anthem-went-wrong-1833731964				Google	2019	How BioWare's Anthem Went Wrong	https://kotaku.com/how-biowares-anthem-went-wrong-1833731964	Kotaku
Doors and Perception: Fiction vs. Simulation in Games	In this paper, the author outlines a theory of the relationship of fictional, virtual and real elements in games. Not much critical attention has been paid to the concept of fiction when applied to games and game worlds, despite many books, articles and papers using the term, often in the title. Here, it is argued that game worlds and their objects are ontologically different from fictional worlds; they are empirically upheld by the game engine, rather than by our mind stimulated by verbal information. Game phenomena such as labyrinths, moreover, are evidence that games contain elements that are just as real as their equivalents outside the game, and far from equal to the fictional counterparts.		Aarseth, Espen	Intermediality	https://www.erudit.org/en/journals/im/2007-n9-im1814828/1005528ar/abstract/				Google Scholar	2011	Doors and Perception: Fiction vs. Simulation in Games	https://www.erudit.org/en/journals/im/2007-n9-im1814828/1005528ar/abstract/	Erudit
Game Engines and Game History	I have a problem: it's called preaching to the choir.  Nobody here lacks faith in the history of games, and I can only hope that my talk will not reduce your enthusiasm.  That realization eliminates the need for a homily on the value of historical studies.  Instead, I will talk about the possibilities and promises facing the history of games as we plunge forward. What does history offer game studies and what might a history of games give back? These questions guide my thoughts at the end of this conference, which has given so much food for these thoughts.		Lowood, Henry	History of Games International Conference Proceedings	https://www.kinephanos.ca/2014/game-engines-and-game-history/		20		Google Scholar	2014	Game Engines and Game History	https://www.kinephanos.ca/2014/game-engines-and-game-history/	Kinephanos
Beyond the HUD - User Interfaces for Increased Player Immersion in FPS Games	The concept of immersion has been adapted by game developers and game critics to describe a deep and positive game experience. While the definition of this concept varies, the user interface of the game is often said to affect the degree to which players can immerse themselves in a game experience. In cooperation with game developer EA DICE, this master thesis aims to investigate how the notion of immersion affects, and is affected by, the user interface (UI) of first-person shooter games, with the ultimate purpose of delivering user interface guidelines for increased immersion. By conducting a study of contemporary first-person shooter (FPS) games, the current state of user interfaces in FPS games is documented. With the addition of a subjective study of FPS games as well as games of other genres, a design space for UI designers is mapped out in order to provide a structure upon which the guidelines can be built. A literature study of various resources within the fields of ludology, cognitive science and media studies is conducted in order to gain increased understanding of what immersion is and its relation to the game experience. The knowledge acquired is used to formulate various hypotheses of how player immersion is connected to the user interfaces of FPS games. These hypotheses are evaluated by user studies and user tests. Looking at the results of the user tests and the literature study, a final definition of immersion is proposed, upon which the guidelines are based. The first guideline, Know Your Design Space, explains the user interface design space of FPS games and encourages UI designers to look at it as a set of tools. Know Your Game discusses how the competitive focus of the game and the game fiction affects the user interface from an immersion point of view. The guideline Establish Player Agency focuses on how the player can be transferred into the game world by acting within it as an agent rather than simply a player of the game. Finally, Strengthen the Player-Avatar Perceptual Link suggests how the user interface can link the player closer to his in-game character on a perceptual level.	Datalogi; Bildanalys; Computer science; Image analysis	Fagerholt, Erik; Lorentzon, Magnus	CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems	https://odr.chalmers.se/items/d5fe6889-4cc6-49c2-ba56-0d759e2f37eb				Google Scholar	2009	Beyond the HUD - User Interfaces for Increased Player Immersion in FPS Games	https://odr.chalmers.se/items/d5fe6889-4cc6-49c2-ba56-0d759e2f37eb	CHALMERS UNIVERSITY OF TECHNOLOGY
PROV Model Primer	"This document provides an intuitive introduction and guide to the PROV Data Model for provenance interchange on the web. PROV defines a core data model for provenance for building representations of the entities, people and processes involved in producing a piece of data or thing in the world. This primer explains the fundamental PROV concepts and provides examples of its use. The primer is intended as a starting point for those wishing to create or use PROV data.

The PROV Document Overview describes the overall state of PROV, and should be read before other PROV documents."		Gil, Yolanda; Miles, Simon; Belhajjame, Khalid; Deus, Helena F.; Garijo, Daniel; Klyne, Graham; Missier, Paolo; Soiland-Reyes, Stian; Zednik, Stephan	AISec'22: Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security	https://www.w3.org/TR/prov-primer/				Google Scholar	2012	PROV Model Primer	https://www.w3.org/TR/prov-primer/	W3C
Understanding Reliability and Validity in Qualitative Research	The use of reliability and validity are common in quantitative research and now it is reconsidered in the qualitative research paradigm. Since reliability and validity are rooted in positivist perspective then they should be redefined for their use in a naturalistic approach. Like reliability and validity as used in quantitative research are providing springboard to examine what these two terms mean in the qualitative research paradigm, triangulation as used in quantitative research to test the reliability and validity can also illuminate some ways to test or maximize the validity and reliability of a qualitative study. Therefore, reliability, validity and triangulation, if they are relevant research concepts, particularly from a qualitative point of view, have to be redefined in order to reflect the multiple ways of establishing truth.	Reliability; Validity; Triangulation; Construct; Qualitative; Quantitative	Golafshani, Nahid	The Qualitative Report	https://doi.org/10.46743/2160-3715/2003.1870		597-606		Google Scholar	2015	Understanding Reliability and Validity in Qualitative Research	https://doi.org/10.46743/2160-3715/2003.1870	The Qualitative Report
Keeping the Game Alive: Evaluating Strategies for the Preservation of Console Video Games	"Interactive fiction and video games are part of our cultural heritage. As original systems cease to work because of hardware and media failures, methods to preserve obsolete video games for future generations have to be developed. The public interest in early video games is high, as exhibitions, regular magazines on the topic and newspaper articles demonstrate. Moreover, games considered to be classic are rereleased for new generations of gaming hardware. However, with the rapid development of new computer systems, the way games look and are played changes constantly. When trying to preserve console video games one faces problems of classified development documentation, legal aspects and extracting the contents from original media like cartridges with special hardware. Furthermore, special controllers and non-digital items are used to extend the gaming experience making it difficult to preserve the look and feel of console video games.

This paper discusses strategies for the digital preservation of console video games. After a short overview of console video game systems, there follows an introduction to digital preservation and related work in common strategies for digital preservation and preserving interactive art. Then different preservation strategies are described with a specific focus on emulation. Finally a case study on console video game preservation is shown which uses the Planets preservation planning approach for evaluating preservation strategies in a documented decision-making process. Experiments are carried out to compare different emulators as well as other approaches, first for a single console video game system, then for different console systems of the same era and finally for systems of all eras. Comparison and discussion of results show that, while emulation works very well in principle for early console video games, various problems exist for the general use as a digital preservation alternative. We show what future work has to be done to tackle these problems."		Guttenbrunner, Mark; Becker, Christoph; Rauber, Andreas	International Journal of Digital Curation	https://doi.org/10.2218/ijdc.v5i1.144				Google Scholar	2010	Keeping the Game Alive: Evaluating Strategies for the Preservation of Console Video Games	https://doi.org/10.2218/ijdc.v5i1.144	International Journal of Digital Curation
Onward Through the Fog: Computer Game Collection and the Play of Obsolescence	"In Mardi and a Voyage Thither, novelist Herman Melville writes of the peculiar and startling confluence of memory, objects, valuation, and disfigurement that mark the collector of obsoletia. The story's antiquary is the picture of perverse depletion, with a body ""crooked, and dwarfed, and surmounted by a hump, that sat on his back like a burden"" (328), his hut in shambles, and ""the precious antiques, and curios, and obsoletes""-the objects of his collection-""strewn about, all dusty and disordered"" (329). This unkempt display cum impromptu museum turns out to present a mere fraction of the curator's collection, the rest of which is host to countless subtle molds and ravenous worms in a vast catacomb below ground. Traversing this darkened vault, one visitor says, is ""like going down to posterity"" (332).

As inveterate accumulators ourselves, we can certainly relate to Mardi's ""extraordinary antiquarian"": pursuing obsolete things has transformed us too (though hopefully not quite so hideously), as well as the work we do and the spaces we do it in. Since 1999, we have been collecting-and subsequently lending out to scholars the world over-computer games, systems, and game-related paraphernalia. By recent estimates, our Learning Games Initiative Archive contains more than 20,000 artifacts, from Venezuelan Pong clones to Mario-themed lollipops. Archival work at this scale and with this diversity is not easy, and it constantly butts up against a host of intractable questions. For example, what does it mean to isolate a thing that no longer has its original value but has taken on a new one? When researchers hold such transmuted artifacts up for inspection, what are they looking for and how might archivists help them to find it? Is the primary work of computer game archivists (and indeed archivists of all types) to protect artifacts from the elements, to enjoin them upon their kin, and to guard over the collection for the sake of some abstract posterity, or is it something more collaborative and communal? Finally, is it possible for research-oriented collectors to engage the process of collection without suffering the deformations of skin and soul (not to mention pocketbook) that often plague the more solipsistic acquirer?

We offer this article as an entree to these questions, as a way to begin to attend to some of the theoretical and practical complexities of obsolescence and its negotiation. We do so primarily by focusing on where those complexities intersect with computer games, the new media we collect and study."	obsolete; obsoletia; obsolescence; computer games; play	Thompson, Jason; McAllister, Ken S.; Ruggill, Judd Ethan	M/C Journal	https://doi.org/10.5204/mcj.155				Google Scholar	2009	Onward Through the Fog: Computer Game Collection and the Play of Obsolescence	https://doi.org/10.5204/mcj.155	M/C Journal
Twisty Little Passages Almost All Alike: Applying the FRBR Model to a Classic Computer Game.	Humanities scholars and librarians both confront questions regarding the boundaries of texts and the relationships between various editions, translations and adaptations. The Functional Requirements for Bibliographic Records (FRBR) Final Report from the International Federation of Library Associations has provided the library community with a model for addressing these questions in the bibliographic systems they create. The Preserving Virtual Worlds project has been investigating FRBR's potential as a model for the description of computer games and interactive fiction. While FRBR provides an attractive theoretical model, the complexity of computer games as works makes its application to such software creations problematic in practice.		McDonough, Jerome P.; Kirschenbaum, Matthew G.; Reside, Doug; Fraistat, Neil; Jerz, Dennis	Digital Humanities Quaterly	https://www.proquest.com/docview/2555208458?pq-origsite=gscholar&fromopenview=true&sourcetype=Scholarly%20Journals				Google Scholar	2010	Twisty Little Passages Almost All Alike: Applying the FRBR Model to a Classic Computer Game.	https://www.proquest.com/docview/2555208458?pq-origsite=gscholar&fromopenview=true&sourcetype=Scholarly%20Journals	Alliance of Digital Humanities Organizations
Developing Digital Games through Software Reuse	"Gaming is an old humans' habit. Games help in logical development and encourage learning of theoretical and practical concepts. Besides they offer entertainment
and challenge. The advent of the personal computer changed this tradition. Every year new challenges arise in a digital format, which lead the young and adults to spend hours
in front of a computer or TV screen in an attempt to overcome hurdles and reach an objective. Quality, sophistication, and constant innovation are attained through complex
computer software that almost has an obligation to improve as each new title is released, due to this game development becomes a challenge. Considering that a game title is
software and thus faces the same restrictions of business applications, this article intends to analyze, under the optics of reuse, if game development resorts to reuse, and where and how this happens. "	Games; Product Line; Software Reuse; Software Development	Neto, Beatriz Helena; Fernandes, Lucia Abrunhosa; Werner, Claudia; Souza, Jano M.	Journal of Information Processing Systems	http://dx.doi.org/10.3745/JIPS.2010.6.2.219		16		Google Scholar	2010	Developing Digital Games through Software Reuse	https://koreascience.kr/article/JAKO201020163390753.pdf	KIPS (Korea Information Processing Society)
Work for Play: Careers in Video Game Development	Video games are not only for play; they also provide work. Making video games is a serious--and big--business. Creating these games is complex and requires the collaboration of many developers, who perform a variety of tasks, from production to programming. They work for both small and large game studios to create games that can be played on many different devices, including console systems, computers, and cell phones. This article covers career options in video game development. The first section provides an overview of the development process. The second section describes four groups of video game occupations: designers, programmers, artists, and others. The third section covers the skills and training workers need for these jobs. The fourth section discusses the benefits and challenges of working in the video game industry. And the fifth section provides job-seeking tips for a career in video game development. Suggested resources for additional information are presented.	Video Games; Development; Occupational Information; Employment Opportunities; Employment Qualifications; Educational Attainment	Liming, Drew; Vilorio, Dennis	Occupational Outlook Quarterly	https://eric.ed.gov/?id=EJ945968		02-11		Google Scholar	2011	Work for Play: Careers in Video Game Development.	https://eric.ed.gov/?id=EJ945968	Association for Computing Machinery
An MDA-based framework for collaborative business process modelling	"Purpose
This paper aims to present a new approach for developing a framework based on a model driven architecture (MDA) for the modelling of technology?independent collaborative processes.

Design/methodology/approach
The paper suggests a new collaborative process modelling approach based on an MDA and a metamodelling technique. The research method, based on the design science approach, was started by identifying the characteristics of the collaborative processes, which distinguish them from the classical intraorganizational ones. Then, the generic collaborative business process (CBP) modelling framework is developed based on MDA approach and definition of a set of transformation rules through three layers: business, process, and technical. After that, the core component of the framework was the proposition of a generic CBP metamodel at PIM/MDA level. The specific collaboration participant's business processes (expressed as BPMN model) are generated from the generic CBP model represented as an UML2 Profile activity diagram, which is compliant to CBP metamodel. Finally, as proof?of?concept, the architecture of an Eclipse?based open development platform is developed implementing an e?Procurement collaborative process.

Findings
The proposed framework for CBP modelling and the generic CBP metamodel contribute towards a more efficient methodology and have consequences for BPM?related collaboration, facilitating the B2B processes modelling and implementation. In order to demonstrate and evaluate the practical applicability of the framework, the architecture of an Eclipse?based open development platform is developed implementing a collaborative business application on the basis of an e?Procurement use case.

Research limitations/implications
There is a need to focus future research efforts on the improvement of the semi?automatic transformation phase from public to private processes which needs human intervention by adding a suitable interfaces at both sides of the B2B interaction. In addition, the problem of semantic heterogeneities regarding the partner's business process elements (business documents, activity/task names) should be tackled by developing an approach that uses ontology.

Practical implications
Business processes developers find a B2B technology?independent solution for implementing and using interorganizational information systems.

Originality/value
The paper provides a framework that enables the CBP modelling and integrates a generic CBP metamodel. Currently, to the best of the authors' knowledge, such a generic metamodel and his instantiation have not so far been developed."	Business process; Collaborative business process; Collaborative business process metamodel; MDA-based framework; E-procurement; UML2 profile; Process management; Team working	Bouchbout, Khoutir; Akoka, Jacky; Alimazighi, Zaia	Business Process Management Journal	https://www.emerald.com/insight/content/doi/10.1108/14637151211283357/full/html?casa_token=mpPxdAoPSnQAAAAA:TadUnDgj7LnKG7kXx7SeBjOLPyTCUMWeAl9efrrDz4b-T4P5hD6H5anWd4q3eZ-zln_1ER4uuIYL5fthV_hpBHGldsU-nCFmOWZ127H6UxvmwRMvgDTL6w				Google Scholar	2012	An MDA?based framework for collaborative business process modelling	https://www.emerald.com/insight/content/doi/10.1108/14637151211283357/full/html?casa_token=mpPxdAoPSnQAAAAA:TadUnDgj7LnKG7kXx7SeBjOLPyTCUMWeAl9efrrDz4b-T4P5hD6H5anWd4q3eZ-zln_1ER4uuIYL5fthV_hpBHGldsU-nCFmOWZ127H6UxvmwRMvgDTL6w	Emerald Insight
A Comparison Between Three SDLC Models Waterfall Model, Spiral Model, and Incremental/Iterative Model	"The computer has become indispensable in today's life, and it is
Widely used in many fields of life such as commerce, education,
industry, etc. The computer saves time in regarding to help
solving complex, long, repeated processes in a short time and
high speed. As the software programs need to handle these
features, many companies produce software programs to
facilitate the works for administrations, banks, offices, etc.
Moreover, software has been in used for analyzing information or solving problems for more than four decades. Creating a
suitable work to develop programs of high quality is the main goal of the software engineering. Usually, clients seek the
assistance from computer and software engineers to solve and handle their problems. There are various models have been
widely in used to develop software products. Common models will be described in this paper. "	SDLC Models; Software Engineering; Waterfall model; Spiral model; Iterative model;	Alshamrani, Adel; Bahattab, Abdullah; Fulton, Ira A.	IJCSI International Journal of Computer Science Issues	https://d1wqtxts1xzle7.cloudfront.net/36637147/SDLC-libre.pdf?1423940228=&response-content-disposition=inline%3B+filename%3DA_Comparison_Between_Three_SDLC_Models_W.pdf&Expires=1726161820&Signature=XHYZ~fSliY2iGbyADM8YvfpjPPO2-V5aoJz2PmsWeR6uL3xwPpflzV1tpCD7xdYqFbu76L6HIti5db0D9ZDMcqHIv9LMiTgM4kWkUhp1MePWsj3~2Hg~mi3fMmv~yiYEbskKsqnwazDIDpJdhEDctVeZ5fG8Ecz-2U~LnTBTmvvpGk~cW5ua8GpKuyWdKTcEEKnI~jfqCE-DiPsW2D4I0i19JGmQbRgFe-cURjeoYJO57XzY7J-jHy2ylKQ0LxSkGvHyDECOVKagc7ZWfTKEIsYWgGb7pLpT1XNwkFk~RVE8egQ0VMxl3JexrTjyMTAuCSxZPdOApes7KZGJGz8iaA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA				Google Scholar	2015	A Comparison Between Three SDLC Models Waterfall Model, Spiral Model, and Incremental/Iterative Model	https://d1wqtxts1xzle7.cloudfront.net/36637147/SDLC-libre.pdf?1423940228=&response-content-disposition=inline%3B+filename%3DA_Comparison_Between_Three_SDLC_Models_W.pdf&Expires=1726161820&Signature=XHYZ~fSliY2iGbyADM8YvfpjPPO2-V5aoJz2PmsWeR6uL3xwPpflzV1tpCD7xdYqFbu76L6HIti5db0D9ZDMcqHIv9LMiTgM4kWkUhp1MePWsj3~2Hg~mi3fMmv~yiYEbskKsqnwazDIDpJdhEDctVeZ5fG8Ecz-2U~LnTBTmvvpGk~cW5ua8GpKuyWdKTcEEKnI~jfqCE-DiPsW2D4I0i19JGmQbRgFe-cURjeoYJO57XzY7J-jHy2ylKQ0LxSkGvHyDECOVKagc7ZWfTKEIsYWgGb7pLpT1XNwkFk~RVE8egQ0VMxl3JexrTjyMTAuCSxZPdOApes7KZGJGz8iaA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA	IJCSI International Journal of Computer Science Issues
Comparison of a Traditional and a Video Game Based Balance Training Program	"The aim of the present study is to compare the efficiency of traditional and video game based balance training programs. 22 customers of a health care
centre (age: M = 47.6 yrs; SD = 13.1) volunteered to participate in the experiment. They were randomly assigned to two experimental groups. One group underwent a traditional training program, while the other group trained using the Nintendo Wii Fit TM Balance Board. Between pre and post test procedures, training sessions were performed three times a week for three weeks. In addition to five balance tests (SEBT, ball-handling, two video games, dynamic balance), a questionnaire was applied concerning mood state, self-efficacy, physical activity enjoyment, flow and subjective experience in order to evaluate psychological effects of the interventions. Two-factor analyses of variance showed both general and differential improvements of the two groups for pre and post test measures. Both groups improved their balance performance in 4 of five tests. The traditional group showed a significantly greater improvement in two tests (SEBT: F(1,20) = 8.907, p = .007, ?2 = .308; ball-handling: F(1,20) = 13.578, p = .001, ?2 = .404),
whereas the Wii group showed a significantly greater improvement in one test (Ski Slalom (F(1,20) = 5.101, p = .035, ?2 = .203). Psychological questionnaires revealed neither significant pre-post effects nor differences between the groups for pre and post test measurements. The results confirm that the Nintendo Wii Fit TM may be a suitable medium
of training balance in prevention and rehabilitation of adults. Specific effects of training are more pronounced than transfer between virtual and real performance. "	BALANCE; VIDEO GAMES; NINTENDO Wii Fit TM; REHABILITATION; PREVENTION	Kliem, Annika; Wiemeyer, Josef	International Journal of Computer Science in Sport	https://www.researchgate.net/profile/Josef-Wiemeyer/publication/220244895_Comparison_of_a_traditional_and_a_Video_Game_Based_Balance_Training_Program/links/54806fad0cf25b80dd723663/Comparison-of-a-traditional-and-a-Video-Game-Based-Balance-Training-Program.pdf		12		Google Scholar	2010	Comparison of a traditional and a Video Game Based Balance Training Program.	https://www.researchgate.net/profile/Josef-Wiemeyer/publication/220244895_Comparison_of_a_traditional_and_a_Video_Game_Based_Balance_Training_Program/links/54806fad0cf25b80dd723663/Comparison-of-a-traditional-and-a-Video-Game-Based-Balance-Training-Program.pdf	IACSS
From Game Mod to Low-Budget Film : The Evolution of Machinima	"Machinima was, in its fi rst few years, an art form that sprang out of gaming, and was initially regarded as an extreme form of gaming. However, in the last few years,
machinima has begun to broaden its appeal from its early base of hardcore game fans to a wider group of both creators and viewers. "		Kelland, Matt	NordiCHI '06: Proceedings of the 4th Nordic conference on Human-computer interaction: changing roles	https://direct.mit.edu/books/edited-volume/chapter-pdf/2281403/9780262295369_cab.pdf				Google Scholar	2011	The Machinima Reader - From Game Mod to Low-Budget Film: The Evolution of Machinima	https://direct.mit.edu/books/edited-volume/chapter-pdf/2281403/9780262295369_cab.pdf	The MIT Press
Advanced reduction techniques for model checking			Keiren, Jja Jeroen		https://doi.org/10.6100/IR757862		212		Google Scholar	2013	Advanced reduction techniques for model checking	https://pure.tue.nl/ws/portalfiles/portal/3664911/757862.pdf	Technische Universiteit Eindhoven
Game design tools: Time to evaluate	The art form of the video game has a very idiosyncratic reliance on the process and practice of its designers. We work with creative and computational problems that form a web of deep complexity. And yet, as I have noticed in my professional practice as a game designer, we do not use tools to support our design process. For more than a decade, designers and researchers have argued for the development and use of both conceptual and concrete tools. To this end, formal and semi-formal game design models have been proposed and, more recently, experimental software-based tools have been developed by the research community. To date, however, none of these tools or models have been adopted into mainstream practice within the game design community. In this paper I argue that it is difficult, if not methodologically flawed, to assess the work in the field of game design support without more qualitative data on how such tools fare in actual game design practice. Evaluation research would be an essential contribution towards answering the question of whether - and if so, how - these experimental formal models and tools can support and improve the game design process.	game design; design tools; ludocore; machinations; game atoms; game diagramming	Katharine, Neil	Proceedings of Nordic DiGRA 2012 Conference	https://dl.digra.org/index.php/dl/article/view/606			"@Conference{digra606, title =""Game design tools: Time to evaluate"", year = ""2012"", author = ""Neil, Katharine"", publisher = ""DiGRA"", address = ""Tampere"", howpublished = ""\url{https://dl.digra.org/index.php/dl/article/view/606}"", booktitle = ""Proceedings of Nordic DiGRA 2012 Conference""}"	Google Scholar	2012	Game Design Tools: Time to Evaluate	https://dl.digra.org/index.php/dl/article/view/606	Digital Games Research Association
Modeling and formal verification of gaming storylines	Video games are becoming more and more interactive with increasingly complex plots. These plots typically involve multiple parallel storylines that may converge and diverge based on player actions. This may lead to situations that are inconsistent or impassable. Current techniques for planning and testing game plots involve naive means such as text documents, spreadsheets, and critical path testing. Recent academic research [1] [2] [3] examines the design planning problems, but neglect testing and veri?cation of the possible plot lines. These complex plots have thus until now been handled inadequately due to a lack of a formal methodology and tools to support them. In this dissertation, we describe how we develop methods to 1) characterize storylines (SChar), 2) de?ne a storyline description language (SDL), and 3) create a storyline veri?cation tool based in formal veri?cation techniques (StoCk) that use our SDL as input. SChar (Storyline Characterization) help game developers characterize the category of story line they are working on (e.g. linear, branching and plot) through a tool that give a set of guided questions. Our SDL allows its users to describe storylines in a consistent format similar to how they reason about storylines, but in such a way that it can be used for formal veri?cation. StoCk accepts storylines, described in SDL, to be formally veri?ed using SPIN for errors. StoCk is also examined in three common use cases found in the gaming industry used as a tool 1) during storyline creation 2) during quality assurance and 3) during storyline implementation. The combination of SChar, SDL, and StoCk provides designers, writers, and developers a novel methodology and tools to verify consistency in large and complex game plots.		Holloway, Lane Thomas	UT Electronics Theses and Dissertations	http://hdl.handle.net/2152/41455				Google Scholar	2016	Modeling and formal veri?cation of gaming storylines	http://hdl.handle.net/2152/41455	The University of Texas at Austin
Game Design by Numbers: Instrumental Play and the Quantitative Shift in the Digital Game Industry	"This dissertation chronicles ideological, technological and economic changes in the digital game industry, focusing on how games are transforming as play becomes
instrumentalized. It pays particular attention to the struggles of developers as they search for creative freedom and autonomy in a risk-averse industry. It makes original
contributions to the literature on games by situating and explaining industiy-wide shifts in terms of the socio-economics of game development and the rationalities that drive
individual developers. It contributes to social theory more generally by explaining how transformations in play, games, and creativity are linked to much wider adaptations in the
operation of capitalism and how it is justified to both workers and consumers. I use ground-level accounts from those within the game industry to describe how new media technologies interact with socio-economic forces, detailing the adaptability of capitalist modes of production in the face of critique. I show how definitions of 'games'
and 'play' are changing as they come into contact with technology, allowing games to be reformulated in powerful new ways, so games are not only tools of entertainment but also tools of governance. I argue that the collective valuation of objective quantitative data and the belief in the fallibility of individual creative autonomy has turned game design into ""design by numbers"". The complementary themes of this thesis are bound together by references to the ""New Spirit of Capitalism"" (Boltanski and Chiapello 2007, 2005), which explains how capitalism is continually reorganizing itself, adapting the language and spirit of 1960s counterculture and emphasizing freedom in order to drive though new, more efficient, work practices and more subde forms of exploitation. This ""New Spirit"" accounts for the current upheavals in the game industry. Changes to the Spirit of Capitalism have initiated tectonic shifts, reforming the geography of the game industry and creating fissures in the landscape that allow new game sectors to emerge, while others struggle to avoid being buried. In turn, innovations from the game industry, particularly the emphasis on datadriven design, shore up the weaknesses in the New Spirit of Capitalism, allowing it to operate more successfully."	digital games; game development; game industry; Instrumentalization of Play; New Spirit of Capitalism; governance; surveillance	Whitson, Jennifer R.	Carleton University	https://repository.library.carleton.ca/downloads/bc386j71t				Google Scholar	2013	Game design by numbers : instrumental play and the quantitative shift in the digital game industry	https://repository.library.carleton.ca/downloads/bc386j71t	Carleton University
Usability of Web sites, methods and evaluation techniques	In the present society, the relevance of the Web is unquestionable and there is a great variety of Web sites that offer services to users. In this context, usability plays a fundamental role in the process of developing successful Web sites. In this article we review different definitions about the discipline of usability, its incorporation in the engineering process (usability engineering) and its relation to software engineer, as well as its attributes and evaluation methods. The considerations about the evaluation system of the Web usability are also presented, advising the user and basing it in the establishment of critical tasks (SIRIUS, reviewed by the authors).	Web site; usability; usability evaluation; heuristic; expert; classification of Web sites; Web tools	Jaimes, Oscar Mauricio Serrano; Gonzalez, Carlos Alberto Rodriguez; de Sistemas, Ingeniero	Revista Cubana de Informacion en Ciencias de la Salud	http://scielo.sld.cu/scielo.php?script=sci_arttext&pid=S2307-21132013000200007				Google Scholar	2012	EVALUACION DE LA USABILIDAD EN SITIOS WEB, BASADA EN EL ESTANDAR ISO 9241-11 (International Standard (1998) Ergonomic requirements For office work with visual display terminals (VDTs)-Parts II: Guidance on usability	http://scielo.sld.cu/scielo.php?script=sci_arttext&pid=S2307-21132013000200007	SciELO
Jumping the Gap: Indie Labour and the Imagined Indie Community	"Due to the recent proliferation of free-to-use, professional-quality development tools, crowdsourced fundraising, and the ascendency of digital distribution platforms, ""indie"" digital game developers are emerging on an unprecedented scale. Based on ethnographic research conducted at indie accelerator Execution Labs from January to June of 2015, this thesis explores how indie developers frame risk, creativity, success, and failure in relation to the communities they are a part of. The first three chapters highlight useful concepts from Gina Neff's ""Venture Labour"" (2012), describe Execution Labs from the researcher's perspective, and detail the researcher's approach to collaborative embedded ethnography, respectively. The fourth chapter is dedicated to two related purposes: the first part will posit that communities of indie developers and indie fans share a common creative discourse, and thus constitute what Benedict Anderson has termed an ""Imagined Community"" (2006). The latter portion of the chapter will explore how Newgrounds.com facilitated discourses about digital games and Macromedia Flash, and how the resultant imagined communities have influenced contemporary indie identity and development practice. The final chapter opens by proposing a definition of ""indie labour"": a creative, communitarian strategy with which indie developers manage the risks they face. It will then consider the stories of those who performed indie labour as part of Execution Labs. This thesis will conclude by ruminating on potential future avenues of study and ongoing issues that hinder indie development's potential as an open and accessible praxis."		Browne, Pierson	Communication Studies	https://spectrum.library.concordia.ca/id/eprint/980737/				Google Scholar	2015	Jumping the Gap: Indie Labour and the Imagined Indie Community	https://spectrum.library.concordia.ca/id/eprint/980737/	Concordia University
Bomberman as an Artificial Intelligence Platform	"Computer games have a long tradition in the Artificial Intelligence (AI) field. They serve the purpose of establishing challenging goals with long term positive repercussions, with researchers trying to develop AIs capable of beating the world human experts. They also provide a good amount of exposition in mainstream media, whilst also giving an engaging environment for teaching computer science concepts in general and AI methods in particular. This thesis aims precisely to contribute to the educational gaming landscape, by providing a novel Bomberman themed platform and several associated intelligent agents. Our first major contribution is precisely the platform itself, Bomberman
as an Artificial Intelligence Platform (BAIP). We provide a fully functional opensource, graphical, language agnostic platform, aimed towards facilitating the study and development of AI methods. We showcase the potential of BAIP and the richness of the selected game environment by developing a plethora of AI agents using different methodologies. At an initial level we introduce a set of basic heuristic methods capable of providing agents with the most essential behavior primitives, equipping them for basic survival in the game environment. At a more advanced level we introduce search-based methods capable of some form of planning. From a machine learning perspective we show how BAIP can be integrated and used with Reinforcement Learning (RL). We use a simple RL problem within the platform and implement both Q-learning and Sarsa algorithms in order to tackle it. This thesis also provides detailed and thorough experimental results about all the developed agents and methods, using the developed platform capabilities. To sum up, this work introduces a new AI platform, gives a strong baseline agent and demonstrates the feasibility of using it for machine learning tasks."		Manuel Antonio da Cruz Lopes		https://repositorio-aberto.up.pt/bitstream/10216/91011/2/176444.pdf		96		Google Scholar	2016	Bomberman as an Artificial Intelligence Platform	https://repositorio-aberto.up.pt/bitstream/10216/91011/2/176444.pdf	Faculdade de Ciencias da Universidade do Porto
Requirements Engineering in Scrum Framework	Requirement Engineering (RE) plays an important role in the success of software development life cycle. As RE is the starting point of the life cycle, any changes in requirements will be costly and time consuming. Failure in determining accurate requirements leads to errors in specifications and therefore to a mal system architecture. In addition, most of software development environments are characterized by user requests to change some requirements.Scrum as one of agile development methods that gained a great attention because of its ability to deal with the changing environments. This paper presents and discusses the current situation of RE activities in Scrum, how Scrum benefits from RE techniques and future challenges in this respect.	Agile; Requirement Engineering; Software Development; Scrums	Darwish, Nagy Ramadan; Megahed, Salwa	International Journal of Computer Applications	https://www.researchgate.net/profile/Nagy-Ramadan/publication/308186449_Requirements_Engineering_in_Scrum_Framework/links/57de46b008aeea195938d140/Requirements-Engineering-in-Scrum-Framework.pdf		6		Google Scholar	2016	Requirements Engineering in Scrum Framework	https://www.researchgate.net/profile/Nagy-Ramadan/publication/308186449_Requirements_Engineering_in_Scrum_Framework/links/57de46b008aeea195938d140/Requirements-Engineering-in-Scrum-Framework.pdf	Foundation of Computer Science
Software in 30 days: how agile managers beat the odds, delight their customers, and leave competitors in the dust	We, Jeff and Ken, have been in the software industry, collectively, for 70 years. We have been software developers, managers in IT organizations and software product companies, and owners of both product companies and service organizations. More than 20 years ago, we created a process that lets organizations deliver software better. Since then, we have helped hundreds of organizations do the same. Our work has spread further than we have ever imagined possible, being put to use by millions of people. We are humbled by the extent of its adoption, and we are awed by the feats people have accomplished using it. This is not the first book we have written on the topic of building software. It is, however, the first book we have written for people who do not themselves build software. This book is instead for leaders within organizations that depend on software for their survival and competitiveness. It is for leaders within organizations that can benefit from developing software rapidly, inerementally. and with the best return on investment possible. It is for leaders who face business and technological complexity that has made the delivery of software difficult. We have written this book so that these leaders can help their organizations achieve these goals, enhance their internal capabilities, improve their product offerings, and more. This book is for chief executive officers (CEOs), executives, and senior managers who need their organizations to deliver better software in less time, with lower cost, greater predictability, and lower risk. For this audience, we have a message: You may have had negative experiences with software devel- opment in the past, but the industry has turned a corner. The software profes- sion has radically improved its methods and its results. The uncertainty, risk, and waste to which you are accustomed are no longer par for the course. We have worked with many software organizations that have already turned the corner; we want to help you do so, too. In this book, we show you how to create business value using a process that delivers complete pieces of software functionality at least every 30 days. This book will show you how you can prioritize the functionality you want and have it delivered - la carte. It will show you how to gain transparency not only into business value, by tracking functionality delivered against functionality desired, but also into the health of the software development process and your organization as a whole. The tools in this book will help you work with your software organization to get up to speed with modern practices and begin to deliver the results you've been expecting all along. This is software in 30 days.		Schwaber, Ken; Sutherland, Jeff	Software in 30 Days	https://books.google.ca/books?hl=en&lr=&id=FSQbYl5HMo8C&oi=fnd&pg=PP11&dq=Software+in+30+Days:+How+Agile+Managers+Beat+the+Odds,+Delight+Their+Customers,+And+Leave+Competitors+In+the+Dust+-+The+Scrum+Guide&ots=V6fElsfP-i&sig=1MZtgdg3yY7QYqedHD2x_gdHmTU#v=onepage&q=Software%20in%2030%20Days%3A%20How%20Agile%20Managers%20Beat%20the%20Odds%2C%20Delight%20Their%20Customers%2C%20And%20Leave%20Competitors%20In%20the%20Dust%20-%20The%20Scrum%20Guide&f=false		185		Google Scholar	2015	Software in 30 Days: How Agile Managers Beat the Odds, Delight Their Customers, And Leave Competitors In the Dust - The Scrum Guide	https://books.google.ca/books?hl=en&lr=&id=FSQbYl5HMo8C&oi=fnd&pg=PP11&dq=Software+in+30+Days:+How+Agile+Managers+Beat+the+Odds,+Delight+Their+Customers,+And+Leave+Competitors+In+the+Dust+-+The+Scrum+Guide&ots=V6fElsfP-i&sig=1MZtgdg3yY7QYqedHD2x_gdHmTU#v=onepage&q=Software%20in%2030%20Days%3A%20How%20Agile%20Managers%20Beat%20the%20Odds%2C%20Delight%20Their%20Customers%2C%20And%20Leave%20Competitors%20In%20the%20Dust%20-%20The%20Scrum%20Guide&f=false	John Wiley & Sons
Gaming Culture at the Boundaries of Play	As the field of game studies and research has matured, multiple valid, yet somewhat differently oriented approaches to games, play and players have emerged. One of them is the cultural study of games and play related phenomena. Both the British and American traditions of cultural studies have been particularly interested in advancing our understanding of popular culture by tracing its position and significance from perspectives opened up by politics, economics, sociology, as well as by literary or textual studies, to mention just a few elements from this highly interdisciplinary field. Where the hallmark of British cultural studies used to be reliance on some version of Marxism or left-wing critique of capitalist society and its cultural industries, and the American approach to cultural studies was more likely to develop liberalist analyses of empowered audiences, or fandom activities, this divide does not stand out so clearly these days. While approaching Cheating by Mia Consalvo, it is nevertheless useful to be keep in mind such intellectual traditions and debates.		Frans Mayra	Game Studies	https://gamestudies.org/1001/articles/mayra				Google Scholar	2010	"Gaming Culture at the Boundaries of Play. Review of ""Cheating: Gaining Advantage in Videogames"" by Mia Consalvo, (MIT Press 2007)"	https://gamestudies.org/1001/articles/mayra	Game Studies
Engineering emergence: applied theory for game design	"Dit proefschrift presenteert twee theoretische kaders voor het ontwerpen van games en beschrijft hoe game designers deze kunnen inzetten om het game ontwerpproces te stroomlijnen. Er bestaan op dit moment meerdere ontwerptheorie""en voor games, maar geen enkele kan rekenen op een breed draagvlak binnen de game industrie. Vooral academische ontwerptheorie""en hebben regelmatig een slechte reputatie. Het eerste kader dat game designers inzicht biedt in spelregels en hun werking heet Machinations en maakt gebruik van dynamische, interactieve diagrammen. Het tweede theoretische kader van dit proefschrift, Mission/Space, richt zich op level-ontwerp en spelmechanismen die de voortgang van een speler bepalen. In tegenstelling tot bestaande modellen voor level-ontwerp, bouwt Mission/Space voort op het idee dat er in een level twee verschillende structuren bestaan. Mission-diagrammen worden gebruikt om de structuur van taken en uitdagingen voor de speler te formaliseren, terwijl space-diagrammen de ruimtelijke constructie formaliseren. Beide constructies zijn aan elkaar gerelateerd, maar zijn niet hetzelfde. De verschillende wijzen waarop missies geprojecteerd kunnen worden op een bepaalde ruimte speelt uiteindelijk een belangrijke rol in de totstandkoming van de spelervaring."		Dormans, Joris		https://research.hva.nl/en/publications/engineering-emergence-applied-theory-for-game-design				Google Scholar	2012	Engineering emergence: applied theory for game design	https://research.hva.nl/en/publications/engineering-emergence-applied-theory-for-game-design	Universiteit van Amsterdam
The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software	Your free lunch will soon be over. What can you do about it? What are you doing about it? The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hyperthreading and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it's not much of a stretch to call 2004 the year of multicore. And that puts us at a fundamental turning point in software development, at least for the next few years and for applications targeting general-purpose desktop computers and low-end servers (which happens to account for the vast bulk of the dollar value of software sold today). In this article, I'll describe the changing face of hardware, why it suddenly does matter to software, and how specifically the concurrency revolution matters to you and is going to change the way you will likely be writing software in the future. Arguably, the free lunch has already been over for a year or two, only we're just now noticing.		Sutter, Herb	Dr. Dobb's Journal	http://www.mscs.mu.edu/~rge/cosc2200/homework-fall2013/Readings/FreeLunchIsOver.pdf		8		Google Scholar	2013	The Free Lunch Is Over A Fundamental Turn Toward Concurrency in Software	http://www.mscs.mu.edu/~rge/cosc2200/homework-fall2013/Readings/FreeLunchIsOver.pdf	Guru of the Week
Underrepresented Youth Creating Culturally Relevant Games	By incorporating culture into working with technology, SDK Bridge attempts to inspire underrepresented youth to consider pursuing careers in technology fields.	entertainment computing; STEM education; SDK Bridge; speech recognition; ethnobotany	Gruenbaum, Peter	COMPUTER	https://doi.org/10.1109/MC.2012.357		91-93		Web of Science	2012	Underrepresented Youth Creating Culturally Relevant Games	https://doi.org/10.1109/MC.2012.357	IEEE COMPUTER SOC
Methods for Game User Research Studying Player Behavior to Enhance Game Design	The emerging field of game user research (GUR) investigates interaction between players and games and the surrounding context of play. Game user researchers have explored methods from, for example, human-computer interaction, psychology, interaction design, media studies, and the social sciences. They've extended and modified these methods for different types of digital games, such as social games, casual games, and serious games. This article describes several current GUR methods. A case study illustrates two specific methods: think-aloud and heuristics.	game user research; computer games; human-computer interaction; software development; computer graphics; think-aloud; Rapid Iterative Testing and Evaluation; RITE; heuristics; playtesting; game usability; game playability; A/B testing	Desurvire, Heather; El-Nasr, Magy Self	IEEE COMPUTER GRAPHICS AND APPLICATIONS	https://doi.org/10.1109/MCG.2013.61		82-87		Web of Science	2013	Methods For Game User Research Studying Player Behavior To Enhance Game Design	https://doi.org/10.1109/MCG.2013.61	IEEE COMPUTER SOC
Hidden Dragon: Research Findings from Asia in Games for Health			Mellecker, Robin R.	GAMES FOR HEALTH JOURNAL	https://doi.org/10.1089/g4h.2015.0016		159-160		Web of Science	2015	Hidden Dragon: Research Findings From Asia In Games For Health	https://doi.org/10.1089/g4h.2015.0016	MARY ANN LIEBERT, INC
Write-once,transpile-everywhere:Re-usingmotioncontrollersofvirtualhumansacrossmultiplegameengines|Signedin	Transpilation allows to write code once and re-use it across multiple runtime environments. In this paper, we propose a software development practice to implement once the motion controllers of virtual humans and re-use the implementation in multiple game engines. In a case study, three common human behaviors - blinking, text-to-speech, and eye-gaze - were developed in the Haxe programming language and deployed in the free, open-source Blender Game Engine and the commercial Unity engine. Performance tests show that transpiled code executes within 67% faster to 127% slower with respect to an implementation manually written in the game engine target languages.	Haxe; Motion controller; Software architecture; Transpilation; Virtual humans	Nunnari, Fabrizio; Heloir, Alexis	Augmented Reality, Virtual Reality, and Computer Graphics, Avr 2018, Pt I	https://doi.org/10.1007/978-3-319-95270-3_37				Scopus	2018	Write-Once, Transpile-Everywhere: Re-Using Motion Controllers Of Virtual Humans Across Multiple Game Engines	https://doi.org/10.1007/978-3-319-95270-3_37	
Scott and the logs: design and data capture in a preparatory online package for children undergoing GA for dental procedures	"This National Institute of Health Research project aims to test if children scheduled for anaesthesia benefit from an interactive online package. The project deals with the design and data capture (logs) of a prototype (alpha) online interactive cartoon created to answer the research question: ""Will internet delivered information help children cope better with anaesthesia?"" Following modification of the alpha package the resultant beta package will be compared to two control groups: standard care procedures and a non-medical computer game. An international academic audience provided positive feedback on the package design and data capture. The animation package sets an example of good practice in design for other similar healthcare scenarios. "	Adaptation;  Psychological; Anesthesia;  Dental; Anesthesia;  General; Cartoons as Topic; Child; Child Behavior; Child;  Preschool; Dental Care for Children; Humans; Internet; Online Systems; Patient Education as Topic; Software Design; Standard of Care; Video Games; adaptive behavior; art; child; child behavior; comparative study; dental anesthesia; dental procedure; general anesthesia; health care quality; human; Internet; online system; patient education; preschool child; software design; video game	Reynolds, P.A.; Donaldson, N.; Huntington, C.; Liossi, C.; Newton, T.C.; Hosey, M.T.	Bulletin du Groupement international pour la recherche scientifique en stomatologie & odontologie	https://www.scopus.com/record/display.uri?eid=2-s2.0-84994166848&origin=resultslist&sort=plf-f&src=s&sid=2cb865c8f035aa76392f84f331b5f332&sot=b&sdt=b&s=TITLE-ABS-KEY%28scott+and+the+logs+design+and+data+capture+in+a+preparatory+online+package+for+children+undergoing+ga+for+dental+procedures%29&sl=138&sessionSearchId=2cb865c8f035aa76392f84f331b5f332&relpos=0		e23 - e24	"@ARTICLE{Reynolds2013e23,
    author = ""Reynolds, P.A. and Donaldson, N. and Huntington, C. and Liossi, C. and Newton, T.C. and Hosey, M.T."",
    title = ""Scott and the logs: design and data capture in a preparatory online package for children undergoing GA for dental procedures"",
    year = ""2013"",
    journal = ""Bulletin du Groupement international pour la recherche scientifique en stomatologie \& odontologie"",
    volume = ""51"",
    number = ""3"",
    pages = ""e23 - e24"",
    url = ""https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994166848\&partnerID=40\&md5=593be78d74ab8e0dd818f8c60661e07b"",
    affiliations = ""Dental Institute, King's College London, London WC2R 2LS, United Kingdom; Dental Institute, King's College London, London WC2R 2LS, United Kingdom; Dental Institute, King's College London, London WC2R 2LS, United Kingdom; Dental Institute, King's College London, London WC2R 2LS, United Kingdom; Dental Institute, King's College London, London WC2R 2LS, United Kingdom; Dental Institute, King's College London, London WC2R 2LS, United Kingdom"",
    keywords = ""Adaptation, Psychological; Anesthesia, Dental; Anesthesia, General; Cartoons as Topic; Child; Child Behavior; Child, Preschool; Dental Care for Children; Humans; Internet; Online Systems; Patient Education as Topic; Software Design; Standard of Care; Video Games; adaptive behavior; art; child; child behavior; comparative study; dental anesthesia; dental procedure; general anesthesia; health care quality; human; Internet; online system; patient education; preschool child; software design; video game"",
    issn = ""16471377"",
    pmid = ""25461125"",
    language = ""English"",
    abbrev_source_title = ""Bull Group Int Rech Sci Stomatol Odontol"",
    type = ""Article"",
    publication_stage = ""Final"",
    source = ""Scopus"",
    note = ""Cited by: 1""
}"	ScopusSignedIn	2013	Scott And The Logs: Design And Data Capture In A Preparatory Online Package For Children Undergoing Ga For Dental Procedures.	https://www.scopus.com/record/display.uri?eid=2-s2.0-84994166848&origin=resultslist&sort=plf-f&src=s&sid=2cb865c8f035aa76392f84f331b5f332&sot=b&sdt=b&s=TITLE-ABS-KEY%28scott+and+the+logs+design+and+data+capture+in+a+preparatory+online+package+for+children+undergoing+ga+for+dental+procedures%29&sl=138&sessionSearchId=2cb865c8f035aa76392f84f331b5f332&relpos=0	
Encore: Atari's Second System A design story	In 1975, Fredrick Brooks wrote an amazing book, The Mythical Man-Month: Essays on Software Engineering [1]. The book applies to managing engineering, not just software. Brooks got his experience the hard way: he managed the development of IBM?s OS/360. At that time in history, in the mid-1960s, it was the largest software project ever attempted. In the 1990s, I worked on several versions of Microsoft Windows, which were even larger. Brooks? writing was directly applicable.		Decuir, Joseph	IEEE CONSUMER ELECTRONICS MAGAZINE	https://doi.org/10.1109/MCE.2015.2484638		40-50		Web of Science	2016	Encore: Atari'S Second System A Design Story	https://doi.org/10.1109/MCE.2015.2484638	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
Rubber Ball to Cloud Rehabilitation(c) Musing on the Future of Therapy	Summary form only. We can trace the origins of virtual rehabilitation to the late 80s when sensing gloves were used to determine the degree of hand tremor in patients with Parkinson, and virtual environments were investigated as a medium to train wheelchair navigation. At the first Medicine Meets Virtual Reality conference in San Diego in 1992, we proposed a unified system where sensing gloves were used to diagnose and train patients post hand surgery. Other researchers pioneered the use of virtual environments in phobias, attention deficit, post-traumatic stress and other conditions. In 1996 researchers interested primarily in VR phobia treatment started the CyberTherapy conference series, and VR-based physical therapy, occupational therapy, therapy for learning deficits, and amnesia were reported at the first International Conference on Disability, Virtual Reality and Associated Technologies. By 1997 the National Science Foundation funded a study of rehabilitation at a distance between Rutgers and Stanford universities, located on either side of the United States. These beginnings used color-coded virtual rubber balls and haptic gloves to program the mechanical work done by the patient's affected hand. An artificial separation existed in the clinical practice between physical or occupational rehabilitation and cognitive therapy, due in part to separate education tracks. Nonetheless virtual reality researchers realized that the same hardware could be used in either physical or cognitive rehabilitation, and all that needed changing was the simulation software used. We thus coined the term ldquovirtual rehabilitationrdquo to encompass the continuum of therapy. In 2002 the associated conference started in Switzerland as the International Workshop on Virtual Rehabilitation. This later became the Virtual Rehabilitation International Conference series which you are attending today. While ldquovirtual rehabilitationrdquo was initially met with some skepticism by therapists who were concerned patients will misunderstand it, nowadays the term is better understood. To help further recognition for this emerging field, a new society was formed in 2008, the International Society for Virtual Rehabilitation (www.isvr.org), which is a co-sponsor of this conference. The merging of physical/occupational therapy and cognitive therapy is not due solely to the modularity offered by the hardware and software used in virtual environments. Another cause is the fact that patients affected by certain neurologic and motor deficits often have psychological and other cognitive co-morbidities. A well known example is depression associated with some types of stroke or with societal isolation that often follows the inability to have regular employment. The same tele-rehabilitation systems that are projected for large scale use to train patients in their home, may also be used to reduce the sense of isolation. Video games that are now being investigated as a way to reinvigorate therapeutic interventions could also be used in future game ldquotournamentsrdquo among teams of people with disabilities, or among people with disabilities and their families and friends. Virtual environments could then be used to customize the games and allow a patient to succeed, greatly boosting morale. An extreme example is the use of virtual hand avatars controlled by people with amputated arms, an application which we pioneered back in 2003. Popular awareness of and demand for virtual rehabilitation is expected to grow, which in turn will trigger changes in the way therapists are educated and accredited. A new field of study will emerge, as will the way therapists and psychologists will be recertified. Certainly the way licensing, insurance, even liability clauses follow local geography is archaic, and a more global certification program is expected to emerge. The one-to-one paradigm of therapy will also change, with one therapist performing ldquomultiplexedrdquo tele-rehabilitation. This is expected to reduce treatment costs while also increasing access to therapeutic care worldwide. Certain technologies will need to advance to act as force multipliers and to help therapists handle the expected workload increase. One supporting technology will be home-based robots which will not only clean, cook and guard, but extend their use to provision of therapy, especially physical therapy. Advances in technology will provide the ability to take therapy anywhere, anytime, addressing current limitation due to geographical location, lack of transportation, limited therapist availability or endurance. This will be facilitated by the proliferation of portable computing/communication terminals coupled to powerful mega-servers, in what is called today ldquocloud computing.rdquo We predict that cloud computing will be extended to ldquocloud rehabilitationrdquo by transforming these portable devices into rehabilitation systems. In cloud rehabilitation the library of disability-specific software simulations or games will reside on a third-party ldquocloudrdquo of web servers. This is where clinicians will log on to set up rehabilitation regimens, follow up patient progress, insure compliance and monitor safety. By concentrating software maintenance and licensing to a unified web structure, the current information technology problems that plague healthcare institutions will be alleviated, the portability of medical data improved and the defense against unauthorized access to medical data boosted. The way to cloud rehabilitation seems straightforward - new types of input devices to measure the patient's input, games that allow clinically meaningful variables to be stored and therapeutic regimens set and monitored, distributed databases storing medical data securely, reliable and encoded communication, and of course, more computer savvy patient and therapist populations.		Burdea, Grigore C.	2009 Virtual Rehabilitation International Conference	https://doi.org/10.1109/ICVR.2009.5174204		50-50		Web of Science	2009	Rubber Ball To Cloud Rehabilitation (C) Musing On The Future Of Therapy	https://doi.org/10.1109/ICVR.2009.5174204	
Whack-a-Meltdown: Microarchitectural Security Games	In early 2018, the disclosure of Spectre and Meltdown exposed the security risks inherent in speculative and out-of-order execution, which were hitherto considered harmless and valuable performance optimizations. In a nutshell, these attacks demonstrated that transient execution, where the computer executes code speculatively before reverting execution, leaves side effects on the microarchitecture, allowing adversaries to retrieve data across software- and hardware-enforced security boundaries.		Genkin, Daniel; Yarom, Yuval	IEEE SECURITY & PRIVACY	https://doi.org/10.1109/MSEC.2020.3036146		95-98		Web of Science	2021	Whack-A-Meltdown: Microarchitectural Security Games	https://doi.org/10.1109/MSEC.2020.3036146	IEEE COMPUTER SOC
Do Improve Typing Skill But No Significant Difference between Drill-Based and Game-Based Typing Software	"Improving typing skill can help students use computers to deal with different kinds of tasks more efficiently. In the past, typing software was commonly used to train students' typing skill, and they usually didn't have much attractive multimedia effect. In recent years, games have become more and more popular, and one of the reasons is that games have lots of attractive elements, like lots of multimedia effect and interesting scripts. About the educational effect of typing games, game is more interesting than traditional instruction, and that's the reason why people use typing games to train their typing skill. This study tried to realize the effect on typing skill of drill-based and game-based typing software, and proposed possible design direction for typing software.

There were 160 second year junior high school students participating in this study. The students studied in a junior high school in middle Taiwan, and they came from five different classes. After excluding six invalid samples, 82 students were males (53.2%) and 72 students were females (46.8%). Pretest-posttest control group design was used in this study. The independent variable was different types of typing software. There were 92 students in the control group, and they used drill-based typing software to learn typing. There were 62 students in the experimental group, and they used game-based typing software to learn typing. The dependent variable was typing speed. The difference of typing speed between pre-test and post-test and the effect on typing speed of different software would be compared.

The result showed that both kinds of typing software could improve students' typing speed, but the effect on typing speed between the two kinds of software didn't reach significant difference. The result indicated that both entertainment and learning design direction were beneficial for learning typing skill, but different kinds of design also showed different disadvantages and limitations. Therefore, it is better to consider the two aspects at the same time when design edutainment software, and try to keep the balance between the two elements to produce the optimal learning effect. The future studies may confirm if the learning effect from both learning element and entertainment element is better than the learning effect only from learning element or entertainment element."	Computer-assisted instruction; Typing speed; Drill-based typing software; Game-based typing software	Lin, Chun-Hung; Liu, Eric Zhi-Feng	LEARNING BY PLAYING	https://doi.org/10.1007/978-3-642-03364-3_19		149-149		Web of Science	2009	Do Improve Typing Skill But No Significant Difference Between Drill-Based And Game-Based Typing Software	https://doi.org/10.1007/978-3-642-03364-3_19	SPRINGER-VERLAG BERLIN
CLASSROOM VIGNETTES: Adjusting courses to address varying student motivations			Henry M. Walker	ACM Inroads	https://doi.org/10.1145/3494573		14-oct		ACM	2021	CLASSROOM VIGNETTES: Adjusting Courses to Address Varying Student Motivations	https://dl.acm.org/doi/10.1145/3494573	Association for Computing Machinery
2 - World of Warcraft			Wyman, Michael Thornton	Making Great Games	https://www.sciencedirect.com/science/article/abs/pii/B9780240812854100029?via%3Dihub		23-38	"@incollection{WYMAN201123,
    editor = ""Wyman, Michael Thornton"",
    author = ""Wyman, Michael Thornton"",
    title = ""2 - World of Warcraft"",
    booktitle = ""Making Great Games"",
    publisher = ""Focal Press"",
    address = ""Boston"",
    pages = ""23-38"",
    year = ""2011"",
    isbn = ""978-0-240-81285-4"",
    doi = ""https://doi.org/10.1016/B978-0-240-81285-4.10002-9"",
    url = ""https://www.sciencedirect.com/science/article/pii/B9780240812854100029""
}"	ScopusSignedIn	2011	2 - World of Warcraft	https://www.sciencedirect.com/science/article/abs/pii/B9780240812854100029?via%3Dihub	Focal Press
Implementing Virtual Agents: a HABA-Based Approach	In recent years developments in gaming technology have focused on Massively Multi-User Persistent Worlds. This virtual communities host thousands of users interacting with each other through the Internet in real time. Such communities require new distributed programming paradigms for implementing the virtual life management. In this work we present a practical methodology to add Virtual Agents to Massively Multi-User Persistent Worlds. To demonstrate the use of our methodology we present a realworld application. It adds a set of virtual agents to a 3D chat. The agents survey the users to obtain data to evaluate the processes and components of the virtual world. 	Software Agents; Multi-agent systems; Agent-Oriented Software Engineering; Virtual Environments	Calvelo, Alejandro Garces; Bauset, Ricardo Quiros; Chover, Miguel; Camahort, Emilio	The International journal of Multimedia & Its Applications (IJMA) 	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3873748		15		Google Scholar	2021	IMPLEMENTING VIRTUAL AGENTS : A HABA- BASED APPROACH	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3873748	Social Science Research Network (SSRN)
Industry life-cycle theory in the cultural domain: Dynamics of the games industry	"Industry life-cycle theory is interested in changes in key industry variables, including entry and exit rates, firm numbers, price, performance and concentration, and in innovative activity as the industry ages. To date, industry life-cycle studies have concentrated on traditional manufacturing industries that produce utilitarian goods. The present study investigates the dynamics of the games industry which is a prime example of a non-manufacturing and non-utilitarian industry. As prior research on the games industry is scarce, the cultural industries literature is reviewed to shed light on the conformities and deviations that can be expected to be found between the propositions of the industry life-cycle theory and the empirical analysis on the games industry. The specificities of cultural industries are discussed in terms of economic characteristics, management challenges and industry dynamics.
The empirical study comprises two analytical steps. The first one investigates the evolution of the games industry with standard industry life-cycle tools. In the second one a qualitative systems dynamics model on the micromechanisms of the game development sector is built to explain the observations made in the first step.
The key finding of the study is that the dynamics of the games industry differ from the propositions of the industry life-cycle theory in two respects. Firstly, innovative activity has not levelled off in either hardware or software. Secondly, game development has remained an unconcentrated industry. The innovative activity is explained by the constant need for novelty common to all cultural industries and it manifests in the key role that the production of original ideas has in game development firms. The low level of concentration is explained by lesser economies of scale caused by increasing management challenges and the increasing risk of bankruptcy with increase in firm size.
The study contributes (1) by applying the industry life-cycle theory to the cultural domain, (2) by combining the phenomena listed in the cultural industries literature and analysing their effect on industry dynamics, (3) by examining the evolution of the games industry in both hardware and software sectors and (4) by highlighting the micromechanisms that produce industry dynamics. The study also has implications for policy-making and management practice. These relate to different forms of public funding, education, outsourcing and the key role of original IP."	Games industry; industry life-cycle; cultural industries; creative industries	Peltoniemi, M.		https://trepo.tuni.fi/handle/10024/114539		281		Google Scholar	2009	Industry life-cycle theory in the cultural domain: Dynamics of the games industry	https://trepo.tuni.fi/handle/10024/114539	Tampere University of Technology
A formal support for incremental behavior specification in agile development	Incremental development is now state of the practice. Indeed, it is promoted from the rational unified process to agile development methods. Few methods however guide software developers and architects in doing so. For instance, no tool is proposed to verify the non-regression of functionalities, modeled as behavior specifications, between increments. This work helps to incrementally specify software functionalities using UML state machines. It provides an on-the-fly evaluation of a specified behavior as compared to that of previous increments. The proposed contribution is based on two formally specified relations that are proved to preserve refinement when composed. Architects and developers are free to choose their preferred behavior specification strategy by iteratively applying them, so as to develop the required functionalities, having at each step the benefit of a formal non-regression checking to guide the global specification process. Our proposal is implemented in a proof-of-concept tool and illustrated by a didactic casestudy. 	UML state machines; incremental development; agile methods; state machine verification; conformance relations; refinement	 Anne-Lise Courbis; Thomas Lambolais; Hong-Viet Luong; Thanh-Liem Phan; Christelle Urtado; Sylvain Vauttier	Software Engineering and Knowledge Engineering (SEKE)	https://hal.science/hal-00800998		7		Google Scholar	2013	SEKE - A formal support for incremental behavior specification in agile development	https://hal.science/hal-00800998	HAL open science
Design Patterns and General Video Game Level Generation	Design patterns have become a vital solution for a number of problems in software engineering. In this paper, we have performed rhythmic analysis of General Video Game Level Generation (GVG-LG) framework and have discerned 23 common design patterns. In addition, we have segregated the identified patterns into four unique classes. The categorization is based on the usage of identified patterns in game levels. Our future aim is to employ these patterns as an input for a search based level generator.	General video game level generation; rhythmic analysis; procedural content generation; design pattern; search based level generator	Sharif, Mudassar; Zafar, Adeel; Muhammad, Uzair	International Journal of Advanced Computer Science and Applications	https://dx.doi.org/10.14569/IJACSA.2017.080952				Google Scholar	2017	Design Patterns And General Video Game Level Generation	https://thesai.org/Publications/ViewPaper?Volume=8&Issue=9&Code=IJACSA&SerialNo=52	The Science and Information Organization
Model-Based Engineering for Embedded Systems in Practice	Model-Based Engineering (MBE) aims at increasing the e?ectiveness of engineering by using models as key artifacts in the development process. While empirical studies on the use and the e?ects of MBE in industry generally exist, there is only little work targeting the embedded systems domain. We contribute to the body of knowledge with a study on the use and the assessment of MBE in that particular domain. Therefore, we collected quantitative data from 112 subjects, mostly professionals working with MBE, with the goal to assess the current State of Practice and the challenges the embedded systems domain is facing. Of the 112 subjects, the majority are experienced with MBE, working at large companies in the automotive, avionics, or healthcare domains. Additionally, mainly OEMs and First-tier suppliers are represented in the study. Our main findings are that MBE is used by a majority of all participants in the embedded systems domain, mainly for simulation, code generation, and documentation. Reported positive e?ects of MBE are higher quality and improved reusability. Main shortcomings are interoperability difficulties between MBE tools, high training e?ort for developers and usability issues. The data also shows that there are no large di?erences between subgroups with respect to domains, position in the value chain, company size and product size.		Marko, Nadja; Liebel, Grischa; Sauter, Daniel; Lodwich, Aleksander; Tichy, Matthias; Leitner, Andrea; Hansson, Jorgen	Software Engineering & Management	http://hdl.handle.net/2077/37424				Google Scholar	2014	Model-based engineering for embedded systems in practice	https://gupea.ub.gu.se/handle/2077/37424	University of Gothenburg
Play-Graph: A Methodology and Visualization Approach for the Analysis of Gameplay Data	Instrumentation to automatically log information - so called gameplay metrics - about the player-game interaction has become an important tool for analyzing player behavior in games. However, due to the usually large amount of gathered data, analysis of the collected data can be challenging. Visualizations have become a promising addition to statistical techniques to explore and better understand the data. In this paper we build upon our previous work on gameplay analysis and introduce the concept of a play-graph as a way to formally describe and visualize gameplay data. Difference graphs are used to depict the changes between two different datasets - a relevant but currently neglected task in gameplay analytics. Furthermore, we address issues in regard to the visual representation of our former prototype. Data obtained from a puzzle game, a team-based shooter, and a MMORPG are used to illustrate the concepts.	Gameplay Visualization; Playtesting; Play-Graph; Difference Graph; Clustering	Wallner, Gunter		http://www.fdg2013.org/program/papers/paper33_wallner.pdf				Google Scholar	2013	FDG - Play-Graph: A methodology and visualization approach for the analysis of gameplay data.	http://www.fdg2013.org/program/papers/paper33_wallner.pdf	University of Applied Arts, Institute of Art and Technology, Vienna, Australia
Implementation Analytical Hierarchy Process On Airplane Ticket Booking Application Selection With Software Quality Requirements and Evaluation ISO/IEC 25010:2011 	Decision-making is the process of selecting alternative actions to achieve a particular goal. Increased movement of the number of passengers using air transportation mode, making the growth of ticket booking application also increase. A system judgment is required to determine which airplane ticket booking application to use. This study discusses the process of choosing an airplane ticket booking application using Analytical Hierarchy Process (AHP) method by using Systems and Software Quality Requirements and Evaluation ISO / IEC 25010:2011 criteria measurement of quality in use. The processed data is obtained from the geometric average of three respondents ie representatives from information technology experts, the public and travel agent. Analysis and data processing using expert choice tools. There are 3 applications airplane ticket booking online in this research deliberately disguised his name. This is because it is feared to have a negative effect on the existing business competition. The results obtained from this study that the application of ticket booking that have the highest value recommended for use is Application A, followed by Application B and Application C		Andalia, Fanny	International Journal of New Media Technology	https://doi.org/10.31937/ijnmt.v4i2.660				Google Scholar	2017	Implementation Analytical Hierarchy Process On Airplane Ticket Booking Application Selection With Software Quality Requirements and Evaluation ISO/IEC 25010:2011		
Human-Computer Interaction - INTERACT 2009	INTERACT 2009 was the 12th of a series of INTERACT international c- ferences supported by the IFIP Technical Committee 13 on Human-Computer Interaction. This year,INTERACT washeld in Uppsala (Sweden), organizedby the Swedish Interdisciplinary Interest Group for Human-Computer Interaction (STIMDI) in cooperation with the Department of Information Technology at Uppsala University. Like its predecessors, INTERACT 2009 highlighted, both to the academic and to the industrial world, the importance of the human-computer interaction (HCI) area and its most recent breakthroughs on current applications. Both - perienced HCI researchers and professionals, as well as newcomers to the HCI ?eld, interested in designing or evaluating interactive software, developing new interaction technologies, or investigating overarching theories of HCI, found in INTERACT 2009 a great forum for communication with people of similar int- ests, to encourage collaboration and to learn. INTERACT 2009 had Research and Practice as its special theme. The r- son we selected this theme is that the research within the ?eld has drifted away from the practicalapplicability of its results and that the HCI practice has come to disregard the knowledge and development within the academic community. 		Gross, Tom; Gulliksen, Jan; Kotze, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco	Lecture Notes in Computer Science	https://doi.org/10.1007/978-3-642-03655-2		928		Google Scholar	2009	Human-Computer Interaction - INTERACT 2009 - Human-Computer Interaction - INTERACT 2009	https://link.springer.com/book/10.1007/978-3-642-03655-2	Springer Berlin, Heidelberg
A New Villain: Investigating Steganography in Source Engine Based Video Games	In an ever expanding field such as computer and digital forensics, new threats to data privacy and legality are presented daily. As such, new methods for hiding and securing data need to be created. Using steganography to hide data within video game files presents a solution to this problem. In response to this new method of data obfuscation, investigators need methods to recover specific data as it may be used to perform illegal activities. This paper demonstrates the widespread impact of this activity and shows how this problem is present in the real world. Our research also details methods to perform both of these tasks: hiding and recovery data from video game files that utilize the Source gaming engine. 	Digital forensics; Hammer; Investigation; Source; Steam; Steganography; Video games	Christopher Hale; Lei Chen; Qingzhong Liu	Proceedings of the Hong Kong International Conference on Engineering and Applied Science	https://scholars.georgiasouthern.edu/en/publications/a-new-villain-investigating-steganography-in-source-engine-based--5				Google Scholar	2012	A New Villain: Investigating Steganography in Source Engine Based Video Games	https://scholars.georgiasouthern.edu/en/publications/a-new-villain-investigating-steganography-in-source-engine-based--5	Information Technology
GameSpace: Methods and Evaluation for Casual Mobile Multiplayer Games	"GameSpace was a new kind of a research project: it involved close collaboration between academic researchers and the game industry, and it looked at the methodological issues involved in game creation rather than focusing on a single game product or technology. The project was funded by the Finnish Funding Agency for Technology and Innovation (Tekes) and five industry partners. The GameSpace industry partners were Nokia Research Center, Veikkaus, TeliaSonera Finland, Sulake Corporation and Digital Chocolate. The project ran for over two years, from August 1st 2006 to September 30th 2008.
GameSpace looked at the design and evaluation of games that are characterised by three main features: they are casual multiplayer games in a mobile use context. The goals for the project were threefold:

- To analyse the playability criteria of a successful mobile multiplayer game - especially in terms of casual gameplay.
- To develop and evaluate game design methodologies suited for the aforementioned games.
- To develop and test gameplay evaluation methods suited for mobile game development and research.

On a broader level, the project could be divided into two major themes. The first theme was design space, involving the study of casuality, mobility and mobile use context. The first phase of the project mostly consisted of conceptual analysis where a deeper and clearer understanding of the design space was sought. This phase mostly focused on the phenomenon of casual games. The aim was to understand the specific features that make a game casual. Casuality in games has not been studied rigorously before, which made it an interesting and important research topic. The second theme was methodological study on exploring and researching new approaches and methods for designing and evaluating casual mobile multiplayer games. The second theme was called design research.
The research project was executed by a series of workshops in which the industry partners and the research team worked in collaboration. The workshop themes followed the design space and the design research topics. The first three workshops focused on ideation methods for casual mobile multiplayer games. In these workshops, the GameSpace research team iteratively designed different ideation methods which would be used in the process of game ideation. Later the ideation methods formed a complete package, which was tested by the industry partners during a three-month actual use period. The next two workshops focused on low and medium fidelity prototyping. The workshops focused on practical prototyping work with various casual mobile multiplayer concepts. The sixth workshop focused on using expert evaluation methods in mobile game evaluation. In addition to the evaluation workshop, the GameSpace research team was involved in various case evaluations for different games which we!
re evaluated with both expert evaluation and user evaluation methods. The case evaluation work was done in close co-operation with the IPerG -project (Integrated Project on Pervasive Gaming). The last workshop expanded the scope of GameSpace by focusing on the user experiences with different games and game related services. This last workshop acted as a stepping stone for upcoming research projects and it will be reported on elsewhere.
The project produced interesting new knowledge on all of the major research areas of the project. Especially the findings on the phenomenon of casual games and the exploratory work on methodological idea generation are leading edge in game research. The prototyping and the evaluation phases also produced new knowledge and the research team produced several conference articles from all these research areas. In addition to the conference articles, the project produced two master's theses. The project received good feedback from the industry partners and led to two continuation projects, SoPlay and GaIn .
This final report contains the knowledge acquired from the GameSpace project. Like the project itself, this final report is divided into two main sections: design space and design research. The design space covers the special characteristics of casuality, mobility and the multiplayer aspects. The design research focuses on game ideation, game prototyping and game evaluation methods. In addition to this final report, the research team will produce a Flash-based application which reflects the content of this report on a broader level."		Paavilainen, Janne; Kultima, Annakaisa; Kuittinen, Jussi; Mayra, Frans; Saarenpaa, Hannamari; Niemela, Johannes		https://urn.fi/urn:isbn:978-951-44-7730-0				Google Scholar	2009	GameSpace: Methods and Evaluation for Casual Mobile Multiplayer Games	https://trepo.tuni.fi/handle/10024/65773	Tampere University of Technology
Human Centered Design 	This volume constitutes the refereed proceedings of the Second International Conference on Human Centered Design, HCD 2011, held as Part of HCI International 2011, in Orlando, FL, USA, in July 2011, jointly with 9 other thematically similar conferences. The 66 revised papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in topical parts on human centered design methods and tools, mobile and ubiquitous interaction, human centered design in health and rehabilitation, human centered design in work, business and education, and applications of human centered design.	"human factor; mobile HCI; pervasive computing; semantic Web; social networking
"	Kurosu, Masaaki	Lecture Notes in Computer Science	https://doi.org/10.1007/978-3-642-21753-1		609		Google Scholar	2009	Human Centered Design - Human Centered Design	https://link.springer.com/book/10.1007/978-3-642-21753-1	Springer Berlin, Heidelberg
Biometric storyboards: a games user research approach for improving qualitative evaluations of player experience	Developing video games is an iterative and demanding process. It is difficult to achieve the goal of most video games - to be enjoyable, engaging and to create revenue for game developers - because of many hard-to-evaluate factors, such as the different ways players can interact with the game. Understanding how players behave during gameplay is of vital importance to developers and can be uncovered in user tests as part of game development. This can help developers to identify and resolve any potential problem areas before release, leading to a better player experience and possibly higher game review scores and sales. However, traditional user testing methods were developed for function and efficiency oriented applications. Hence, many traditional user testing methods cannot be applied in the same way for video game evaluation. This thesis presents an investigation into the contributions of physiological measurements in user testing within games user research (GUR). GUR specifically studies the interaction between a game and users (players) with the aim to provide feedback for developers to help them to optimise the game design of their title. An evaluation technique called Biometric Storyboards is developed, which visualises the relationships between game events, player feedback and changes in a player's physiological state. Biometric Storyboards contributes to the field of human-computer interaction and GUR in three important areas: (1) visualising mixedmeasures of player experience, (2) deconstructing game design by analysing game events and pace, (3) incremental improvement of classic user research techniques (such as interviews and physiological measurements). These contributions are described in practical case studies, interviews with game developers and laboratory experiments. The results show this evaluation approach can enable games user researchers to increase the plausibility and persuasiveness of their reports and facilitate developers to better deliver their design goals. Biometric Storyboards is not aimed at replacing existing methods, but to extend them with mixed methods visualisations, to provide powerful tools for games user researchers and developers to better understand and communicate player needs, interactions and experiences. The contributions of this thesis are directly applicable for user researchers and game developers, as well as for researchers in user experience evaluation in entertainment systems.		Mirza-Babaei, Pejman		https://sussex.figshare.com/articles/thesis/Biometric_storyboards_a_games_user_research_approach_for_improving_qualitative_evaluations_of_player_experience/23401361?file=41126138		168		Google Scholar	2014	Biometric storyboards : a games user research approach for improving qualitative evaluations of player experience	https://sussex.figshare.com/articles/thesis/Biometric_storyboards_a_games_user_research_approach_for_improving_qualitative_evaluations_of_player_experience/23401361?file=41126138	University of Sussex
10 Coordination in Organizations: An Integrative Perspective	Coordination, the process of interaction that integrates a collective set of interdependent tasks, is a central purpose of organizations. In this review we begin by discussing the origins of interest in coordination, tracing some of the classic perspectives. We present a review of recent literature on coordination in organizations arranged according to the mechanisms that help achieve it. We then go beyond this review to provide a framework to understand what different coordination mechanisms and activities accomplish. We propose that coordination mechanisms (such as routines, meetings, plans, and schedules) impact the work of organizations by creating three integrative conditions for coordinated activity: accountability, predictability, and common understanding. We end by examining the implications of such a perspective for future research on coordination in organizations.		Okhuysen, Gerardo A.; Bechky, Beth A.	The Academy of Management Annals	https://doi.org/10.5465/19416520903047533				Google Scholar	2009	10 Coordination in Organizations: An Integrative Perspective	https://journals.aom.org/doi/10.5465/19416520903047533	Academy of Management
Hidden Markov Model based Speech Synthesis: A Review	A Text-to-speech (TTS) synthesis system is the artificial production of human system. This paper reviews recent research advances in field of speech synthesis with related to statistical parametric approach to speech synthesis based on HMM. In this approach, Hidden Markov Model based Text to speech synthesis (HTS) is reviewed in brief. The HTS is based on the generation of an optimal parameter sequence from subword HMMs. The quality of HTS framework relies on the accurate description of the phoneset. The most attractive part of HTS system is the prosodic characteristics of the voice can be modified by simply varying the HMM parameters, thus reducing the large storage requirement.	TTS; speech corpus; Marathi phonemes	Kayte, Sangramsing N.; Mundada, Monica; Gujrathi, Jayesh	International Journal of Computer Applications	http://dx.doi.org/10.5120/ijca2015906965		5		Google Scholar	2015	Hidden Markov Model based Speech Synthesis: A Review	https://www.ijcaonline.org/research/volume130/number3/kayte-2015-ijca-906965.pdf	Foundation of Computer Science Inc.
Model-Driven Game Development Addressing Architectural Diversity and Game Engine-Integration	Model-Driven Game Development (MDGD) is an emerging research field, which uses models to specify some or all of the game elements that traditionally had been manually coded. The PhD thesis presents research in the MDGD domain, which is intended to push MDGD further towards industrialization by reducing two gaps: 1) The integration of MDGD tools and game engines, and 2) The support for a diversity of game architectures. These gaps have been identified through a literature review of existing MDGD approaches, which is also documented in the thesis. To reduce the first gap, the model-driven approach Engine Cooperative Game Modeling (ECGM) has been proposed, which uses a run-time game engine as the base for building a domain framework, and engine tools together with MDGD tools for creating game code and data. The code generator generates both game code and level data from game models, making the game models operable in the engine tools. ECGM has been evaluated through being instantiated with a Domain Specific Language (DSL), Reactive AI Language (RAIL) tools, and the commercial game engine Torque 2D. The DSL and engine were used to develop a prototype game, whose evaluation showed that ECGM can significantly improve the productivity and enable an efficient workflow. To reduce the second gap, the game architecture framework Game Worlds Graph (GWG) has been proposed. GWG has then been used as the conceptual base for a MDGD approach supporting a diversity of game architectures. The GWG-based MDGD approach employs the Global World, Local World, and Connector Type concepts in modeling languages, adding the architectural information to the modeled game elements, with which code for different architectures can be generated. To evaluate the approach, a DSL and its tool-chain were created following the approach, and a prototype game supporting three game architectures were developed. The results of the prototyping showed that the MDGD approach and the DSL resulted in significant improvement in productivity and workflow efficiency. Apart from providing a major research contribution bridging the two gaps described above, the GWG framework itself is a valuable research contribution. The framework can be used for 1) analyzing and classifying existing game architectures, which was proven through a systematic review of 40 game architectures, 2) exploring future architectures resulting in the design of a new game architecture, and 3) aiding MDGD. The RAIL and an extended version of RAIL are also themselves valuable research contributions. The RAIL is a DSL for modeling high-level AI in action/adventure games, and the extended RAIL adds network architecture support to RAIL. Both DSLs are supported by their tool-chains including a model-editor, a semantics validator, and a code generator. These two versions of RAIL can be reused in related MDGD projects or be used as the reference DSLs.		Zhu, Meng		http://hdl.handle.net/11250/2360005				Google Scholar	2015	Model-Driven Game Development Addressing Architectural Diversity and Game Engine-Integration	https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2360005	NTNU
Including Visually Impaired Players in a Graphical Adventure Game: A Study of Immersion	The aim of the project presented in this paper is that visually impaired and sighted players should be able to play the same game and share a gaming experience. The goal is that the game should be accessible to visually impaired players without any additional tools, such as text-to-speech, that may reduce the immersion. At the same time, sighted players should perceive the game as a regular game. This paper presents an evaluation of the game where the player immersion has been evaluated through a post test immersion questionnaire. The study was conducted with three independent groups: sighted players using graphics (n=10), blindfolded sighted players (n=10) and visually impaired players (n=9). Although progress in the game and the reported sense of control differed between groups, player immersion was very high in all groups. There were differences between the three groups only in one out of five immersion factors. The result shows that it has been possible to provide an immersive experience irrespective of whether the players are playing the game with graphics or using audio only.	Inclusive game design; immersion; questionnaire; audio games	Engstrom, Henrik; Brusk, Jenny; Ostblad, Per Anders	IADIS International Journal on Computer Science and Information Systems	https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A896470&dswid=3129				Google Scholar	2015	Including Visually Impaired Players in a Graphical Adventure Game: A Study of Immersion	https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A896470&dswid=3129	IADIS International Journal on Computer Science and Information Systems
22 - The Sims (2000): Who Let the Sims Out?			Loguidice, Bill; Barton, Matt	Vintage Games	https://doi.org/10.1016/B978-0-240-81146-8.00032-4		319-333		Google Scholar	2009	22 - The Sims (2000): Who Let the Sims Out?	https://www.sciencedirect.com/science/article/abs/pii/B9780240811468000324?via%3Dihub	Focal Press
Modeling and formal veri?cation of gaming storylines	Video games are becoming more and more interactive with increasingly complex plots. These plots typically involve multiple parallel storylines that may converge and diverge based on player actions. This may lead to situations that are inconsistent or impassable. Current techniques for planning and testing game plots involve naive means such as text documents, spreadsheets, and critical path testing. Recent academic research [1] [2] [3] examines the design planning problems, but neglect testing and veri?cation of the possible plot lines. These complex plots have thus until now been handled inadequately due to a lack of a formal methodology and tools to support them. In this dissertation, we describe how we develop methods to 1) characterize storylines (SChar), 2) de?ne a storyline description language (SDL), and 3) create a storyline veri?cation tool based in formal veri?cation techniques (StoCk) that use our SDL as input. SChar (Storyline Characterization) help game developers characterize the category of story line they are working on (e.g. linear, branching and plot) through a tool that give a set of guided questions. Our SDL allows its users to describe storylines in a consistent format similar to how they reason about storylines, but in such a way that it can be used for formal veri?cation. StoCk accepts storylines, described in SDL, to be formally veri?ed using SPIN for errors. StoCk is also examined in three common use cases found in the gaming industry used as a tool 1) during storyline creation 2) during quality assurance and 3) during storyline implementation. The combination of SChar, SDL, and StoCk provides designers, writers, and developers a novel methodology and tools to verify consistency in large and complex game plots.	Formal verification; Gaming storyline; Games; Modeling; Modeling tools	Holloway, Lane Thomas	UT Electronic Theses and Dissertations	http://hdl.handle.net/2152/41455				Google Scholar	2016	Modeling and formal verification of gaming storylines	https://repositories.lib.utexas.edu/items/5779ce34-dd94-4dab-8572-96fc44fa7395	The University of Texas at Austin
Transitioning from a meta-simulator to electrical applications: An architecture	Nowadays, simulators are used in different applications, such as benchmarking tools, for entertainment and educational purposes, to test scenarios that otherwise would not be possible to analyze (e.g., for security reasons) or to evaluate business and regulatory models. Nevertheless, simulators are usually tailor-made for a specific application. This paper proposes an original approach, an architecture for a software tool capable of simulating any electric power system application, the first energy-oriented meta-simulator. This tool would only require the definition of a set of behavior rules, easing the process of developing new simulators aimed at the energy sector. Its applications range from pure software simulators, that could be used with, for example, benchmarking, decision-making or competition purposes, to applications monitoring and controlling energy assets in real-time, such as hardware devices (e.g., sensors, actuators, or power plants) or digital twins. The proposed architecture resulted by studying previous tools and simulators with the objective of finding common blocks and elements to abstract them. For this reason, the proposed architecture intents to encompass any tool that aims to model the energy sector. Additionally, the proposed architecture is compliant with lightweight Internet of Things (IoT) protocols and smart systems and supports the synchronization with other frameworks at different levels.	Electricity market; Internet of Things; Meta-simulator; Energy platforms; Software architecture	Miguel M. Martin-Lopo; Jaime Boal; Alvaro Sanchez-Miralles	Simulation Modelling Practice and Theory	https://doi.org/10.1016/j.simpat.2019.02.007		177-198		Science Direct	2019	I2MTC, 2019 IEEE International Instrumentation & Measurement Technology Conference : Discovering New Horizons in Instrumentation and Measurement : 2019 conference proceedings : the Lords of the IMS: Expanding the Frontiers of Metrology Innovations : Grand Millennium Auckland, Auckland, New Zealand, May 20-23, 2019	https://www.sciencedirect.com/science/article/pii/S1569190X19300188?via%3Dihub	Elsevier BV
Transitioning from a meta-simulator to electrical applications: An architecture	Nowadays, simulators are used in different applications, such as benchmarking tools, for entertainment and educational purposes, to test scenarios that otherwise would not be possible to analyze (e.g., for security reasons) or to evaluate business and regulatory models. Nevertheless, simulators are usually tailor-made for a specific application. This paper proposes an original approach, an architecture for a software tool capable of simulating any electric power system application, the first energy-oriented meta-simulator. This tool would only require the definition of a set of behavior rules, easing the process of developing new simulators aimed at the energy sector. Its applications range from pure software simulators, that could be used with, for example, benchmarking, decision-making or competition purposes, to applications monitoring and controlling energy assets in real-time, such as hardware devices (e.g., sensors, actuators, or power plants) or digital twins. The proposed architecture resulted by studying previous tools and simulators with the objective of finding common blocks and elements to abstract them. For this reason, the proposed architecture intents to encompass any tool that aims to model the energy sector. Additionally, the proposed architecture is compliant with lightweight Internet of Things (IoT) protocols and smart systems and supports the synchronization with other frameworks at different levels.	Electricity market; Internet of Things; Meta-simulator; Energy platforms; Software architecture	Miguel M. Martin-Lopo; Jaime Boal; Alvaro Sanchez-Miralles	Simulation Modelling Practice and Theory	https://doi.org/10.1016/j.simpat.2019.02.007		177-198		Science Direct	2019	I2MTC 2019 IEEE International Instrumentation & Measurement Technology Conference : Discovering New Horizons in Instrumentation and Measurement : 2019 conference proceedings : the Lords of the IMS: Expanding the Frontiers of Metrology Innovations : Grand Millennium Auckland Auckland New Zealand May 20-23 2019	https://www.sciencedirect.com/science/article/pii/S1569190X19300188?via%3Dihub	Elsevier BV
Reducing Cognitive Impairment Among Dementia Users Through Mobile Application	Cognitive impairment includes the lacking ability to remember things, disorientation in remembering the current location, and the struggle to find the correct word. People with dementia (PwD) are often involved in this impairment. With that being said, this project proposes the use of a mobile application to help in improving their cognitive issues. To tackle this problem, features and functionality of a mobile application specifically for dementia users are identified which contributes to the development of a diary application. Identifying and gathering features from previous studies was the initial method. Development of the diary application followed the software development life cycle (SDLC) waterfall method and evaluation of the application was experimented with identified dementia users. The findings of this project are the application set of guidelines gathered from literature into the diary application. Four verified dementia people were involved in the evaluation of the effectiveness of the application. The evaluation of the application includes some good points. Some parts of the application are pointed out for their unsuitable design and suggestions are given to improve the application in the later future.		N. A. M. Hassan; A. Baharum; Z. H. A. Sani; K. Chau; N. A. M. Noor 	Pertanika J. Sci. & Technol. 29 (2): 863 - 883 (2021)	https://doi.org/10.47836/pjst.29.2.09		22		Google Scholar		Reducing cognitive impairment among dementia users through mobile application	https://doi.org/10.47836/pjst.29.2.09	ACM
SERIOUS GAMES AND REHABILITATION FOR ELDERLY ADULTS	"Healthcare is continually being improved, especially regarding the use of the current technologies. In the field of rehabilitation, the use of serious games and related technologies may help to develop new rehabilitation procedures. There are several approaches in the area of rehabilitation that invoke technology for a more comprehensive and dynamic learning process aimed at the physical and psychological recovery of persons with a disability or limitations, with a view to their possible recovery and reintegration. This contribution presents research on systems for rehabilitation, focusing on the elderly, based on exercises and serious games. It provides an overview of the state of the art, in order to support and guide future work in this area.,"" vol. 6, no. 1, pp. 275-283, 2018, [Online]. Available: http://www.globalscientificjournal.com/researchpaper/Serious-Games-and-Rehabilitation-for-Elderly-Adults.pdf%0Awww.globalscientificjournal.com"		H. Barbosa; A. V Castro; E. Carrapatoso, 	GSJ: Volume 6, Issue 1, January 2018	https://d1wqtxts1xzle7.cloudfront.net/55733329/Serious-Games-and-Rehabilitation-for-Elderly-Adults-libre.pdf?1517950916=&response-content-disposition=inline%3B+filename%3DSerious_Games_and_Rehabilitation_for_Eld.pdf&Expires=1724342329&Signature=N7v31RMY-LtJ8xsDmfUcrRvzsGNcs3ntxMKp8wE1q1y~MjANsiwbrpLGOUsoKys00VmxR0JipLyyV2xqt-PVWfraSsv8iz4WzhR4WOJu2tRU6Cbmm3o3Efk65s0Ox14egigT0GUctyjxD7g4oyleIWPpXITqxCuaUFFQBepBNxGr5ZyI-09dkEmqT8AwNiH1pbVBczN3FHeHWgA9KMOxl4C7QdAFaKhyCDuCC1FcdYYwaSDsURSRzHGvoGpZ~5Lr1xB9p4F2O7UMscz24a0LNc7cshcI4SVjrBO-ITsUKR2NStURJdjTCSNJd-v--f3uLWpDL8760Syt335cJc0www__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA				Google Scholar		Serious Games and Rehabilitation for Elderly Adults	https://d1wqtxts1xzle7.cloudfront.net/55733329/Serious-Games-and-Rehabilitation-for-Elderly-Adults-libre.pdf?1517950916=&response-content-disposition=inline%3B+filename%3DSerious_Games_and_Rehabilitation_for_Eld.pdf&Expires=1724342329&Signature=N7v31RMY-LtJ8xsDmfUcrRvzsGNcs3ntxMKp8wE1q1y~MjANsiwbrpLGOUsoKys00VmxR0JipLyyV2xqt-PVWfraSsv8iz4WzhR4WOJu2tRU6Cbmm3o3Efk65s0Ox14egigT0GUctyjxD7g4oyleIWPpXITqxCuaUFFQBepBNxGr5ZyI-09dkEmqT8AwNiH1pbVBczN3FHeHWgA9KMOxl4C7QdAFaKhyCDuCC1FcdYYwaSDsURSRzHGvoGpZ~5Lr1xB9p4F2O7UMscz24a0LNc7cshcI4SVjrBO-ITsUKR2NStURJdjTCSNJd-v--f3uLWpDL8760Syt335cJc0www__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA	
Personalized gamification to enhance implementation of eHealth therapy in youth mental healthcare	This dissertation focused on the added value of personalized gamification as a factor to enhance implementation potential of eHealth interventions in youth mental healthcare. Mental health disorders are the leading cause of disability in adolescents. It is important for these adolescents to go into therapy, as adolescence is a period in live in which essential developments occur on which mental health disorders have a negative impact. Although psychosocial therapies are effective in reducing psychiatric symptoms in adolescents with mental disorders, there is still room for improvement. For example, because of premature termination of treatment, poor attendance of treatment?sessions and a low or non?adherence to homework assignments...		van Dooren, M.	Delft University of Technology, 2020.	https://repository.tudelft.nl/record/uuid:9a8d3973-f5b5-4812-97ed-27e5c14afc34		201		Google Scholar	2020	Personalized gamification to enhance implementation of eHealth therapy in youth mental healthcare Personalized gamification to enhance implementation of eHealth therapy in youth mental healthcare	https://repository.tudelft.nl/record/uuid:9a8d3973-f5b5-4812-97ed-27e5c14afc34	TUDelft
Development of a Mobile Application Using Augmentative and Alternative Communication and Video Modelling for Autistic Children	"Purpose: The aim of this project is to report on a mobile application which has integrated
Augmented and Alternative Communication and video modeling approaches. A usability
testing has been conducted on the developed application.
Design/methodology/approach: The authors have developed a mobile application, AutiAct.
In order to do so, an interview has been conducted with teachers. Observations have been made
with 4 ASD children aged 7 to 12 years, each of whom has different types of severity, while
usability tests and interview questions have been conducted with 3 teachers from the National
Autism Society of Malaysia (NASOM) to test the effectiveness of AutiAct.
Findings: A mobile application, AutiAct, has been developed through the introduction of video
modeling and Augmentative Alternative Communication approaches to assist autistic children
in their everyday routine. The procedure in the application is simple and can be followed step
by step for autistic children. Based on the testing, all the respondents have 100% agreed that
AutiAct is an appropriate tool to assist and enhance autism children with their daily life skills.
Results also have shown that video modelling and AAC are an important tool to be used to
improve the abilities of autistic children.
Practical implications: Results have shown that AutiAct is reliable to help autistic children
improve their independent skills so that they will depend less on their parents as they grow
older and that it also embeds a character building for autistic children to help them shape their
personality and characteristics."	Mobile application; Augmented and Alternative Communication; Video Modeling; Autistic Children	W. F. W. Ahmad and N. A. B. Zulkharnain, 	Global Business and Management Research: An International Journal, Vol. 12, No. 4 (2020)	http://www.gbmrjournal.com/pdf/v12n4/V12N4-1.pdf		11		Google Scholar	2020	Development of a Mobile Application Using Augmentative and Alternative Communication and Video Modelling for Autistic Children.	http://www.gbmrjournal.com/pdf/v12n4/V12N4-1.pdf	GBMR
Computational methods for the verification of adaptive control systems	"Intelligent and adaptive control systems will significantly challenge current verification and validation (V&V) processes, tools, and methods for flight certification. Although traditional certification practices have produced safe and reliable flight systems, they will not be cost effective for next-generation autonomous unmanned air vehicles (UAVs) due to inherent size and complexity increases from added functionality. Affordable V&V of intelligent control systems is by far the most important challenge in the development of UAVs faced by both commercial and military aerospace industry in the United States. This paper presents a formal modeling framework for a class of adaptive control systems and an associated computational scheme. The class of systems considered include neural network-based flight control systems and vehicle health management systems. This class of systems and indeed all adaptive systems are hybrid systems whose continuum dynamics is nonlinear. Our computational procedure is iterative and each iteration has two sequential steps. The first step is to derive an approximating finite-state automaton whose behaviors contain the behaviors of the hybrid system. The second step is to check if the language accepted by the approximating automaton is empty (emptiness checking). The iterations are terminated if the language accepted is empty; otherwise, the approximation is refined and the iteration is continued. This procedure will never produce an ""error-free"" certificate when the actual system contains errors which is an important requirement in V&V of safety critical systems."		Prasanth, R., J. Boskovic; R. Mehra	Signal Processing, Sensor Fusion, and Target Recognition XIII, April 12, 2004 - April 14, 2004 Orlando, FL, United states	https://doi.org/10.1117/12.546128				Google Scholar	2004	Computational methods for the verification of adaptive control systems	https://doi.org/10.1117/12.546128	
MAR-CPS: Measurable Augmented Reality for Prototyping Cyber-Physical Systems	Cyber-Physical Systems (CPSs) refer to engineering platforms that rely on the inte- gration of physical systems with control, computation, and communication technologies. Autonomous vehicles are instances of CPSs that are rapidly growing with applications in many domains. Due to the integration of physical systems with computational sens- ing, planning, and learning in CPSs, hardware-in-the-loop experiments are an essential step for transitioning from simulations to real-world experiments. This paper proposes an architecture for rapid prototyping of CPSs that has been developed in the Aerospace Controls Laboratory at the Massachusetts Institute of Technology. This system, referred to as MAR-CPS (Measurable Augmented Reality for Prototyping Cyber-Physical Systems), includes physical vehicles and sensors, a motion capture technology, a projection system, and a communication network. The role of the projection system is to augment a physical laboratory space with 1) autonomous vehicles' beliefs and 2) a simulated mission environ- ment, which in turn will be measured by physical sensors on the vehicles. The main focus of this method is on rapid design of planning, perception, and learning algorithms for au- tonomous single-agent or multi-agent systems. Moreover, the proposed architecture allows researchers to project a simulated counterpart of outdoor environments in a controlled, indoor space, which can be crucial when testing in outdoor environments is disfavored due to safety, regulatory, or monetary concerns. We discuss the issues related to the design and implementation of MAR-CPS and demonstrate its real-time behavior in a variety of problems in autonomy, such as motion planning, multi-robot coordination, and learning spatio-temporal fields.		Omidshafiei, S., A. A. Agha-mohammadi, C. Yu Fan, N. K. Ure, J. P. How, J. Vian; R. Surati	AIAA Infotech@Aerospace Conference, 5-9 Jan. 2015 Reston, VA, USA	https://dspace.mit.edu/handle/1721.1/114875				Google Scholar	2015	MAR-CPS: Measurable Augmented Reality for Prototyping Cyber-Physical Systems	https://dspace.mit.edu/handle/1721.1/114875	American Institute of Aeronautics and Astronautics (AIAA)
Combination of simulation and model-checking for the analysis of autonomous vehicles' behaviors: A case study	Autonomous vehicles' behavioural analysis represents a major challenge in the automotive world. In order to ensure safety and fluidity of driving, various methods are available, in particular, simulation and formal verification. The analysis, however, has to cope with very complex environments depending on many parameters evolving in real time. In this context, none of the aforementioned approaches is fully satisfactory, which lead us to propose a combined methodology in order to point out suspicious behaviours more efficiently. We illustrate this approach by studying a non deterministic scenario involving a vehicle, which has to react to some perilous situation.	Autonomous vehicles; simulation; verification	Johan Arcile; Jeremy Sobieraj; Hanna Klaudel; Guillaume Hutzler	Multi-Agent Systems and Agreement Technologies	https://doi.org/10.1007/978-3-030-01713-2_21				Google Scholar	2018	Combination of Simulation and Model-Checking for the Analysis of Autonomous Vehicles,Ao Behaviors: A Case Study	https://doi.org/10.1007/978-3-030-01713-2_21	Springer, Cham
Towards Verification of Artificial Neural Networks	We consider the safety verification of controllers obtained via machine learning. This is an important problem as the employed machine learning techniques work well in practice, but cannot guarantee safety of the produced controller, which is typically represented as an artificial neural network. Nevertheless, such methods are used in safety-critical environments. In this paper we take a typical control problem, namely the Cart Pole System (a. k. a. inverted pendulum), and a model of its physical environment and study safety verification of this system. To do so, we use bounded model checking (BMC). The created formulas are solved with the SMT-solver iSAT3. We examine the problems that occur during solving these formulas and show that extending the solver by special deduction routines can reduce both memory consumption and computation time on such instances significantly. This constitutes a first step towards verification of machine-learned controllers, but a lot of challenges remain.		Scheibler, K.; L. Winterer, R. Wimmer; B. Becker	Methoden und Beschreibungssprachen zur Modellierung und Verifikation von Schaltungen und Systemen	https://core.ac.uk/download/pdf/268014938.pdf#page=30		11		Google Scholar	2015	Towards Verification of Artificial Neural Networks	https://core.ac.uk/download/pdf/268014938.pdf#page=30	CORE
The challenge of verification and testing of machine learning	"In our second post, we gave some background explaining why attacking machine learning is often easier than defending it. We saw some of the reasons why we do not yet have completely effective defenses against adversarial examples, and we speculated about whether we can ever expect such a defense.

In this post, we explore the types of guarantees one can expect a machine learning model to possess. We argue that the limitations of existing defenses point to the lack of verification of machine learning models. Indeed, to design reliable systems, engineers typically engage in both testing and verification:

By testing, we mean evaluating the system in several conditions and observing its behavior, watching for defects.

By verification, we mean producing a compelling argument that the system will not misbehave under a very broad range of circumstances.

Orthogonal to this issue is the question of which input values should be subject to verification and testing. Do we intend to verify or test the system only for ""naturally occurring"" legitimate inputs, or do we intend to provide guarantees for its behavior on arbitrary, degenerate inputs? Many software systems such as compilers have undefined behavior for some inputs."		Goodfellow, I.; N. Papernot 	Cleverhans-blog	https://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html				Google Scholar	2017	The challenge of verification and testing of machine learning	https://www.cleverhans.io/security/privacy/ml/2017/06/14/verification.html	Cleverhans-blog
Global-and-local attention networks for visual recognition	"Most recent gains in machine vision have originated from the development of network architectures which incorporate some form of attention. While biology is sometimes mentioned as a source of inspiration, the attentional mechanisms that have been considered by the computer vision community remain limited in comparison to the richness and diversity of the processes used by our visual system. Here, we describe a biologicallymotivated ""global-and-local attention"" (GALA) module which is shown to yield state-of-the-art object recognition accuracy when embedded in a modern deep neural network. We further describe ClickMe.ai, a largescale online experiment designed for human participants to identify diagnostic image regions for visual recognition in order to co-train a GALA network. Adding humans-inthe-loop is shown to significantly improve network accuracy, while also yielding visual representations that are more interpretable and more similar to those used by human observers."	Object recognition; deep learning; biological vision; human-in-the-loop machine learning; visual features	Linsley, D., D. Scheibler, S. Eberhardt; T. Serre	Benefits, 2018	https://ccneuro.org/2018/proceedings/1113.pdf		2		Google Scholar	2018	Global-and-local attention networks for visual recognition	https://ccneuro.org/2018/proceedings/1113.pdf	CCNeuro
Learning Global Additive Explanations for Neural Nets Using Model Distillation	Interpretability has largely focused on local explanations, i.e. explaining why a model made a particular prediction for a sample. These explanations are appealing due to their simplicity and local fidelity. However, they do not provide information about the general behavior of the model. We propose to leverage model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations take the form of feature shapes, which are more expressive than feature attributions. Through careful experimentation, we show qualitatively and quantitatively that global additive explanations are able to describe model behavior and yield insights about models such as neural nets. A visualization of our approach applied to a neural net as it is trained is available at https://youtu.be/ErQYwNqzEdc.		Tan, S., R. Caruana, G. Hooker, P. Koch; A. Gordo	Machine Learning for Health (ML4H) Workshop at NeurIPS 2018.	https://d1wqtxts1xzle7.cloudfront.net/84730300/1801.08640v2-libre.pdf?1650716563=&response-content-disposition=inline%3B+filename%3DDifference_in_Outcome_Following_Surgery.pdf&Expires=1724341634&Signature=P8gCJ5C4wPm0b7suGLKMK~tuR6VWQVLwNrkhvx1ckOC~~E9zgyt44vfiWdmFJA8zyZ4XM4kdnk4DFRBMiDoYycYj4jO22UJqkVahDJ18Tts2~tP5lTj3S8TSQuwB6QWYC-10Y83Sc8ywR0tOKnH0bIRMvN3MXoY4S4Xoc5RjB~mNKuZYFcx~YDXvEtoqdOpKgAaqhQ6VbGfNA4nnJfl3OZkdPv4kOcVx9yGnUsEjXd5baPfgXWUzupXUH9niK7jV~oz6V3k4PVeaPIq0wMeTLoMGGKOlXsk-QPuhPG6sIaEnF0wAg9CZx1s7nn9aHx9h0dSc9~n7WHXrzUPoVdJewQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA		13		Google Scholar	2018	Learning Global Additive Explanations for Neural Nets Using Model Distillation	https://d1wqtxts1xzle7.cloudfront.net/84730300/1801.08640v2-libre.pdf?1650716563=&response-content-disposition=inline%3B+filename%3DDifference_in_Outcome_Following_Surgery.pdf&Expires=1724341634&Signature=P8gCJ5C4wPm0b7suGLKMK~tuR6VWQVLwNrkhvx1ckOC~~E9zgyt44vfiWdmFJA8zyZ4XM4kdnk4DFRBMiDoYycYj4jO22UJqkVahDJ18Tts2~tP5lTj3S8TSQuwB6QWYC-10Y83Sc8ywR0tOKnH0bIRMvN3MXoY4S4Xoc5RjB~mNKuZYFcx~YDXvEtoqdOpKgAaqhQ6VbGfNA4nnJfl3OZkdPv4kOcVx9yGnUsEjXd5baPfgXWUzupXUH9niK7jV~oz6V3k4PVeaPIq0wMeTLoMGGKOlXsk-QPuhPG6sIaEnF0wAg9CZx1s7nn9aHx9h0dSc9~n7WHXrzUPoVdJewQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA	Microsoft Research
Safety reinforced driving	"Safety must be a major focus of any Autonomous Vehicle development. Given that Humans drive about 108 miles between fatal crashes, learning methods must be capable of error rates as low as 10-9, at least 6 orders of magnitude better than mainstream ML. Clearly, not all errors lead to an accident; there is a need to determine which are fatal. This paper focuses on quantifying safety, demonstrating that instead of using ""error"" one should define competence using full distribution and test statistics of composable metrics, which include well established safety components. A Reinforcement Learning (RL) formulation is introduced which composes such metrics into a reward, whereby safety and competence are defined in terms of the test statistics of that reward. A scalable multi-agent RL algorithm is presented, which is capable of achieving high 1% percentile of such metrics. Co-existence of a certified baseline non-AI with an AI driver is achieved using apprenticeship training. Results show that it is practical to achieve competence on episodes with hundreds of steps such that the worst 1% percentile of the safety metrics remains very high. Interesting quantized reward distributions during failure are presented."		Schwalb, Edward; Taslimi, Farzaneh, Kuecuekyan, Horen	AUVSI XPONENTIAL 2018	https://www.scopus.com/record/display.uri?eid=2-s2.0-85054632219&origin=resultslist&sort=plf-f&src=s&sid=c16ef929649bfaa559743c699b7ad40e&sot=b&sdt=b&s=TITLE%28Safety+reinforced+driving%29&sl=32&sessionSearchId=c16ef929649bfaa559743c699b7ad40e&relpos=1				Scopus	2018	Safety reinforced driving	https://www.scopus.com/record/display.uri?eid=2-s2.0-85054632219&origin=resultslist&sort=plf-f&src=s&sid=c16ef929649bfaa559743c699b7ad40e&sot=b&sdt=b&s=TITLE%28Safety+reinforced+driving%29&sl=32&sessionSearchId=c16ef929649bfaa559743c699b7ad40e&relpos=1	Association for Unmanned Vehicle Systems International
Digital twin system for highway traffic based on 3D GIS technology	Digital twin is the science that integrates multidisciplinary attributes and achieves the interaction and integration of virtual and reality through the fusion of physical world and information world. Digital twin has been applied in the transportation industry through BIM+GIS technology. The interaction of various transportation infrastructure model elements and the rational construction of twin data have become the bottleneck restricting its development. In the 3D GIS environment, with the guidance of digitally-generated five-dimensional structural model, a specific implementation technique for geometric, physical, and regular models that the virtual model should contain was proposed. The classification of transportation infrastructure and the characteristics of twin data were analyzed. Then, the layer relationship of its components was refined and the scheme of identification coding and storage of models was implemented. Furthermore, the system software was classified according to CAMP5, and the verification method was explained according to each layer to complete the system verification. Based on the virtual model and data fusion, the ideas of application development of pixel layer, representation layer and decision layer were combed, which could provide technical references for achieving digital twin in the traffic field. 	Building information modeling; Data fusion; Digital twin; Geographic information system; Identification analytic system; Transportation	Zheng, Weihao; Zhou, Xingyu; Wu, Hongping; Li Hongmei; Zhu Xintong; Wen, Linjiang	Computer Integrated Manufacturing Systems, CIMS	https://www.scopus.com/record/display.uri?eid=2-s2.0-85082691095&origin=resultslist&sort=plf-f&src=s&sid=0257f19813097ef55b18fa1a6c0f2e47&sot=b&sdt=b&s=TITLE-ABS-KEY%28Digital+twin+system+for+highway+traffic+based+on+3D+GIS+technology%29&sl=23&sessionSearchId=0257f19813097ef55b18fa1a6c0f2e47&relpos=1				Scopus	2020	Digital-twin-system-for-highway-traffic-based-on-3D-GIS-technology--GISJisuanji-Jicheng-Zhizao-XitongComputer-Integrated-Manufacturing-Systems-CIMS	https://www.scopus.com/record/display.uri?eid=2-s2.0-85082691095&origin=resultslist&sort=plf-f&src=s&sid=0257f19813097ef55b18fa1a6c0f2e47&sot=b&sdt=b&s=TITLE-ABS-KEY%28Digital+twin+system+for+highway+traffic+based+on+3D+GIS+technology%29&sl=23&sessionSearchId=0257f19813097ef55b18fa1a6c0f2e47&relpos=1	Computer Integrated Manufacturing Systems, CIMS
Business Language Driven Development: Joining business process models to automated tests	A typical problem in Software Engineering is how to guarantee that every system requirement is correctly implemented by source code. Many techniques were proposed, mostly based on taking notes of relationships between requirements and code in modeling and documenting artifacts. However, these techniques cannot guarantee that the artifacts are always synchronized with source code. Aiming at solving this problem, Behavior-Driven Development (BDD) is a specification technique that automatically checks that all functional requirements are treated properly by source code, through connecting the textual description of requirements to automated tests. Nevertheless, in some software development areas, such as Enterprise Information Systems (EIS), requirements are usually identified by analyzing business process models - which use graphical notations of the underlying processes logic. Therefore, the aim of this paper is to present Business Language Driven Development (BLDD), a method that aims to extend BDD, by connecting business process models directly to automated tests, thus guaranteeing that requirements are properly covered by code.	Behavior Driven Development; Business Process Modeling; Requirements Engineering; Enterprise Information Systems	Rogerio Atem De Carvalho; Fernando Luiz De Carvalho e Silva; R.S. Manhaes	Nucleo de Pesquisa em Sistemas de Informacao (NSI)	https://www.researchgate.net/publication/290580324_Business_Language_Driven_Development_Joining_business_process_models_to_automated_tests		16		Google Scholar	2012	Business Language Driven Development: Joining business process models to automated tests	https://www.researchgate.net/publication/290580324_Business_Language_Driven_Development_Joining_business_process_models_to_automated_tests	Nucleo de Pesquisa em Sistemas de Informacao (NSI)
Android Best Practices	Android Best Practices by Godfrey Nolan shows you how to make your Android apps stand out from the crowd with great reviews. Why settle for just making any Android app? Build a brilliant Android app instead that lets your users praise it for ease of use, better performance, and more.  Using a series of example apps which gradually evolve throughout this book, Android Best Practices brings together current Android best practices from user interface (UI)/user experience (UX) design, test-driven development (TDD), and design patterns (e.g., MVC) to help you take your app to the next level.  In this book you'll learn how to:  * Use Android design patterns for consistent UI experience on many devices  * Use agile techniques such as test-driven development, behavior-driven development, and continuous integration  * Improve the speed and overall performance of your app  * Organize an Android app using design patterns such as MVC/MVP  * Create and consume REST and SOAP web services  Designing and developing an app that runs well on many if not all the leading Android smartphones and tablets today can be one of the most daunting challenges for Android developers. Well, this book takes much of the mystery out of that for you.  After reading and using Android Best Practices, you'll become a much better Android app designer and developer, which in turn can make your apps better placed and more successful in the market place. 		Godfrey Nolan; David Truxall; Raghav Sood; Onur Cinar		https://books.google.ca/books?id=GJgQAwAAQBAJ&dq=Android+Best+Practices&lr=&source=gbs_navlinks_s		232		Google Scholar	2014	Android Best Practices	https://books.google.ca/books?id=GJgQAwAAQBAJ&dq=Android+Best+Practices&lr=&source=gbs_navlinks_s	Apress
XUnit Test Patterns: Refactoring Test Code	"Automated testing is a cornerstone of agile development. An effective testing strategy will deliver new functionality more aggressively, accelerate user feedback, and improve quality. However, for many developers, creating effective automated tests is a unique and unfamiliar challenge.  xUnit Test Patterns is the definitive guide to writing automated tests using xUnit, the most popular unit testing framework in use today. Agile coach and test automation expert Gerard Meszaros describes 68 proven patterns for making tests easier to write, understand, and maintain. He then shows you how to make them more robust and repeatable--and far more cost-effective.  Loaded with information, this book feels like three books in one. The first part is a detailed tutorial on test automation that covers everything from test strategy to in-depth test coding. The second part, a catalog of 18 frequently encountered ""test smells,"" provides trouble-shooting guidelines to help you determine the root cause of problems and the most applicable patterns. The third part contains detailed descriptions of each pattern, including refactoring instructions illustrated by extensive code samples in multiple programming languages."		Gerard Meszaros	Addison-Wesley Signature Series (Fowler)	https://books.google.ca/books?id=-izOiCEIABQC&dq=XUnit+Test+Patterns:+Refactoring+Test+Code&lr=&source=gbs_navlinks_s		944		Google Scholar	2007	XUnit Test Patterns: Refactoring Test Code	https://books.google.ca/books?id=-izOiCEIABQC&dq=XUnit+Test+Patterns:+Refactoring+Test+Code&lr=&source=gbs_navlinks_s	Pearson Education
Learning Behavior-Driven Development with JavaScript	This book is ideal for any JavaScript developer who is interested in producing well-tested code. If you have no prior experience with testing, Node.js, or any other tool, do not worry, as they will be explained from scratch.		Enrique Amodeo	Community experience distilled	https://books.google.ca/books?id=Fru9BgAAQBAJ&dq=Learning+Behavior-Driven+Development+with+JavaScript&lr=&source=gbs_navlinks_s		392		Google Scholar	2015	Learning Behavior-Driven Development with JavaScript	https://books.google.ca/books?id=Fru9BgAAQBAJ&dq=Learning+Behavior-Driven+Development+with+JavaScript&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
The Merb Way	Thousands of Ruby and Rails developers are discovering the extraordinary scalability, agility, flexibility, and performance offered by the new Merb MVC framework. The Merb Way is the first comprehensive guide to using, extending, and deploying Merb. Like the bestseller The Rails Way (Addison-Wesley, 2008), this book can either be read cover-to-cover as a tutorial or used for modular coverage that makes it an ideal task reference. Foy Savas systematically covers everything developers need to know to build production-quality Merb applications, offering detailed code examples for jumpstarting virtually any project.  Savas is not only involved in the Merb project as an open source contributor: He uses Merb every day as a professional developer. Drawing on his extensive practical expertise, he delves deeply into the Merb framework's architecture and source code, revealing its elegance and offering powerful best practices for using it. To maximize this book's practical value, he also covers the tools most widely used alongside Merb, including the DataMapper ORM, the RSpec tester (and associated behavior-driven development techniques), and several leading Merb plugins.		Foy Savas	Addison-Wesley Professional Ruby Series	https://books.google.ca/books?id=Vcl8sul1rKEC&dq=The+Merb+Way&lr=&source=gbs_navlinks_s		384		Google Scholar	2009	The Merb Way	https://books.google.ca/books?id=Vcl8sul1rKEC&dq=The+Merb+Way&lr=&source=gbs_navlinks_s	Pearson Education
JavaScript Testing with Jasmine: JavaScript Behavior-Driven Development	Get a concise introduction to Jasmine, the popular behavior-driven testing framework for JavaScript. This practical guide shows you how to write unit tests with Jasmine that automatically check for bugs in your application. If you have JavaScript experience--with knowledge of some advanced features--you'll learn how to write specifications for individual components, and then use those specs to test the code you write.  Throughout the book, author Evan Hahn focuses primarily on methods for testing browser-based JavaScript applications, but you'll also discover how to use Jasmine with CoffeeScript, Node.js, Ruby on Rails, and Ruby without Rails. You won't find a more in-depth source for Jasmine anywhere.		Front Cover Evan Hahn		https://books.google.ca/books?id=QlbljtaXtg4C&dq=JavaScript+Testing+with+Jasmine:+JavaScript+Behavior-Driven+Development&lr=&source=gbs_navlinks_s		52		Google Scholar	2013	JavaScript Testing with Jasmine: JavaScript Behavior-Driven Development	https://books.google.ca/books?id=QlbljtaXtg4C&dq=JavaScript+Testing+with+Jasmine:+JavaScript+Behavior-Driven+Development&lr=&source=gbs_navlinks_s	O'Reilly Media, Inc.
Integrating behavior driven development and programming by contract to improve correctness in computer programs	"In many professional development settings, Test Driven Development (TDD), and its evolutionary descendent, Behavior Driven Development (BDD) are increasingly used to improve code quality and reducing error rates. While valuable, they are considerably less rigorous than formal methodologies of program correctness via formal logic. A less rigorous methodology utilizing the ideas of formal methods but without requiring an actual full proof is also in use called ""Design by Contract"" (DBC) or ""Programming by Contract."" (PBC) This thesis will examine extending and combining the ideas behind TDD, BDD and PBC to improve the overall stability and quality of a system. It will examine using both TDD/BDD (hereinafter referred to as TDD) and PBC to enhance the quality of development throughout the software development life cycle, while aiding in the development and stabilization of sound architecture. A result of this examination is a tool that attempts to derive unit tests automatically by analyzing human written specifications for preconditions and post-conditions when coupled with data definitions. These results will be used to generate code to be run by a unit testing framework before deployment, either as part of a continuous integration environment or by individual developers. The tool will also attempt to generate wireframe classes implementing pre and post- conditions within the code and using runtime contract analysis to generate v information when an exception occurs, thereby helping to automate verification of bug fixes. Ultimately, the tool produced would be useful in a non-academic environment as a utility to extend the benefit of design by contract combined with TDD to the agile space."		Schoeneman, Larry	Bradley University ProQuest Dissertations & Theses	https://www.proquest.com/docview/1313769827?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses		253		Google Scholar	2012	Integrating Behavior Driven Development and Programming by Contract to Improve Correctness in Computer Programs	https://www.proquest.com/docview/1313769827?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses	Bradley University ProQuest Dissertations & Theses
Compliance checking on building models with the Gherkin language and Continuous Integration	In this paper we document our approach on applying Behaviour-Driven Development (BDD) and Continuous Integration (CI) from the software industry to the construction sector. We have provided a freely available open software toolset for the application of rules in the Gherkin syntax to an IFC building model. A prominent aspect of BDD and contrary to mvdXML, the formalization of rules in plain-test human-readable scenarios provides a basis for collaborative formalization of rules among stakeholders. At the same time our approach includes imperative program code that is fully extensible to incorporate for example external data sources and geometrical reasoning. Runnings test on every model revision (the CI concept) as opposed to, for example, upon model delivery ensures a proactive approach to compliance. Reusing existing open source frameworks allowed us to build a comprehensive solution for continuous and automated model checking, visualization and reporting in several hundred lines of program code.		Dion Moult; T.F. Krijnen	Proceedings of the EG-ICE 2020 Workshop on Intelligent Computing in Engineering	https://doi.org/10.14279/depositonce-9977		294-303		Google Scholar	2020	Compliance checking on building models with the gherkin language and continuous integration	https://research.tudelft.nl/en/publications/compliance-checking-on-building-models-with-the-gherkin-language-	Technische Universitat Berlin
Test-Driven Java Development	About This BookExplore the most popular TDD tools and frameworks and become more proficient in building applicationsCreate applications with better code design, fewer bugs, and higher test coverage, enabling you to get them to market quicklyImplement test-driven programming methods into your development workflowsWho This Book Is For  If you're an experienced Java developer and want to implement more effective methods of programming systems and applications, then this book is for you. What You Will LearnExplore the tools and frameworks required for effective TDD developmentPerform the Red-Green-Refactor process efficiently, the pillar around which all other TDD procedures are basedMaster effective unit testing in isolation from the rest of your codeDesign simple and easily maintainable codes by implementing different techniquesUse mocking frameworks and techniques to easily write and quickly execute testsDevelop an application to implement behaviour-driven development in conjunction with unit testingEnable and disable features using Feature TogglesIn Detail  Test-driven development (TDD) is a development approach that relies on a test-first procedure that emphasises writing a test before writing the necessary code, and then refactoring the code to optimize it.  The value of performing TDD with Java, one of the most established programming languages, is to improve the productivity of programmers, the maintainability and performance of code, and develop a deeper understanding of the language and how to employ it effectively.  Starting with the basics of TDD and reasons why its adoption is beneficial, this book will take you from the first steps of TDD with Java until you are confident enough to embrace the practice in your day-to-day routine.  You'll be guided through setting up tools, frameworks, and the environment you need, and will dive right in to hands-on exercises with the goal of mastering one practice, tool, or framework at a time. You'll learn about the Red-Green-Refactor procedure, how to write unit tests, and how to use them as executable documentation.  With this book you'll also discover how to design simple and easily maintainable code, work with mocks, utilise behaviour-driven development, refactor old legacy code, and release a half-finished feature to production with feature toggles.  You will finish this book with a deep understanding of the test-driven development methodology and the confidence to apply it to application programming with Java. Style and approach  An easy-to-follow, hands-on guide to building applications through effective coding practices. This book covers practical examples by introducing different problems, each one designed as a learning exercise to help you understand each aspect of TDD.		Viktor Farcic; Alex Garcia		https://books.google.ca/books?id=tRl1CgAAQBAJ&dq=Test-Driven+Java+Development&lr=&source=gbs_navlinks_s		284		Google Scholar	2015	Test-Driven Java Development	https://books.google.ca/books?id=tRl1CgAAQBAJ&dq=Test-Driven+Java+Development&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
Mastering Symfony	About This BookCreate a robust and reliable Symfony development pipeline using Amazon's cloud platformCut development and maintenance costs by defining crystal clear features and possible scenarios for each feature before implementationFollow detailed examples provided in each chapter to create a task management applicationWho This Book Is For  If you are a PHP developer with some experience in Symfony and are looking to master the framework and use it to its full potential, then this book is for you. Though experience with PHP, object-oriented techniques, and Symfony basics is assumed, this book will give you a crash course on the basics and then proceed to more advanced topics. What You Will LearnInstall and configure Symfony and required third-party bundles to develop a task management applicationSet up a continuous integration server to orchestrate automatic builds every time you add a new feature to your projectReduce maintenance costs dramatically using Behaviour Driven Development (BDD)Create a slick user interface using the Bootstrap frameworkDesign robust business logic using DoctrineBuild a comprehensive dashboard and secure your project using the Sonata projectImprove performance using Redis, Memcache, and VarnishCreate customized Symfony commands and add them to your consoleIn Detail  In this book, you will learn some lesser known aspects of development with Symfony, and you will see how to use Symfony as a framework to create reliable and effective applications. You might have developed some impressive PHP libraries in other projects, but what is the point when your library is tied to one particular project? With Symfony, you can turn your code into a service and reuse it in other projects.  This book starts with Symfony concepts such as bundles, routing, twig, doctrine, and more, taking you through the request/response life cycle. You will then proceed to set up development, test, and deployment environments in AWS. Then you will create reliable projects using Behat and Mink, and design business logic, cover authentication, and authorization steps in a security checking process. You will be walked through concepts such as DependencyInjection, service containers, and services, and go through steps to create customized commands for Symfony's console. Finally, the book covers performance optimization and the use of Varnish and Memcached in our project, and you are treated with the creation of database agnostic bundles and best practices. Style and approach  A step-by-step guide to mastering Symfony while developing a task management application. Each chapter comes with detailed examples.		Sohail Salehi		https://books.google.ca/books?id=r9zJDAAAQBAJ&dq=Mastering+Symfony&lr=&source=gbs_navlinks_s		290		Google Scholar	2016	Mastering Symfony	https://books.google.ca/books?id=r9zJDAAAQBAJ&dq=Mastering+Symfony&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
RSpec Essentials	About This BookExplore the concept of testability and how to implement tests that deliver the most valueMaximize the quality of your Ruby code through a wide variety of testsMaster the real-world tradeoffs of testing through detailed examples supported by in-depth discussionWho This Book Is For  This book is aimed at the software engineer who wants to make their code more reliable and their development process easier. It is also aimed at test engineers who need to automate the testing of complex systems. Knowledge of Ruby is helpful, but even someone new to the language should find it easy to follow the code and tests. What You Will LearnIdentify a unit of software for the purposes of testingManage test states with hooks, fixtures, and mocksHandle external web services in tests using various techniquesConfigure RSpec flexibly and cleanly using support code and environment variablesInteract with rich web apps in tests using CapybaraBuild the right feature with behavior-driven developmentCustomize matchers and failure messagesVerify correct development and production environmentsIn Detail  This book will teach you how to use RSpec to write high-value tests for real-world code. We start with the key concepts of the unit and testability, followed by hands-on exploration of key features. From the beginning, we learn how to integrate tests into the overall development process to help create high-quality code, avoiding the dangers of testing for its own sake.  We build up sample applications and their corresponding tests step by step, from simple beginnings to more sophisticated versions that include databases and external web services. We devote three chapters to web applications with rich JavaScript user interfaces, building one from the ground up using behavior-driven development (BDD) and test-driven development (TDD).  The code examples are detailed enough to be realistic while simple enough to be easily understood. Testing concepts, development methodologies, and engineering tradeoffs are discussed in detail as they arise. This approach is designed to foster the reader's ability to make well-informed decisions on their own. Style and approach  This comprehensive tutorial is packed with real-world examples of testing with RSpec. The most important features of RSpec are introduced in the early chapters and are used in examples of growing complexity in the following chapters. Concepts and methodologies are discussed in detail.		Mani Tadayon		https://books.google.ca/books?id=0dvJDAAAQBAJ&dq=RSpec+Essentials&lr=&source=gbs_navlinks_s		222		Google Scholar	2016	RSpec Essentials	https://books.google.ca/books?id=0dvJDAAAQBAJ&dq=RSpec+Essentials&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
A Formal Ontology for Describing Interactive Behaviors and Supporting Automated Testing on User Interfaces	Nowadays many software development frameworks implement Behavior-Driven Development (BDD) as a mean of automating the test of interactive systems under construction. Automated testing helps to simulate user's actions on the User Interface and therefore check if the system behaves properly and in accordance to scenarios that describe functional requirements. However, tools supporting BDD run tests on implemented User Interfaces and are a suitable alternative for assessing functional requirements in later phases of the development process. However, even when BDD tests can be written in early phases of the development process they can hardly be used with specifications of User Interfaces such as prototypes. To address this problem, this paper proposes to raise the abstraction level of both system interactive behaviors and User Interfaces by means of a formal ontology that is aimed at supporting test automation using BDD. The paper presents an ontology and an ontology-based approach for automating the test of functional requirements of interactive systems. We demonstrate the feasibility of this ontology-based approach to assess functional requirements in prototypes and full-fledge applications through an illustrative case study of e-commerce applications for buying flight tickets.	Behavior-Driven Development (BDD); automated requirements assessment; ontological modeling; user interfaces; prototyping; testing of interactive systems	Thiago Rocha Silva; Jean-Luc Hak; Marco Winckler	International Journal of Semantic Computing	https://doi.org/10.1142/S1793351X17400219		513-539		Google Scholar	2017	A Formal Ontology for Describing Interactive Behaviors and Supporting Automated Testing on User Interfaces	https://www.worldscientific.com/doi/abs/10.1142/S1793351X17400219	World Scientific Publishing Co Pte Ltd
Cucumber Cookbook	This book is intended for business and development personnel who want to use Cucumber for behavior-driven development and test automation. Readers with some familiarity with Cucumber will find this book of most benefit. Since the main objective of this book is to create test automation frameworks, previous experience in automation will be helpful.		Shankar Garg	Community experience distilled	https://books.google.ca/books?id=0IrGCQAAQBAJ&dq=Cucumber+Cookbook&lr=&source=gbs_navlinks_s		162		Google Scholar	2015	Cucumber Cookbook	https://books.google.ca/books?id=0IrGCQAAQBAJ&dq=Cucumber+Cookbook&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
Ruby on Rails Tutorial: Learn Web Development with Rails	Used by sites as varied as Hulu, GitHub, Shopify, and Airbnb, Ruby on Rails is one of the most popular frameworks for developing web applications, but it can be challenging to learn and use. Whether you're new to web development or new only to Rails, Ruby on RailsTM Tutorial, Seventh Edition, is the solution.  Best-selling author and leading Rails developer Michael Hartl teaches Rails by guiding you through the development of three example applications of increasing sophistication. The tutorial's examples focus on the general principles of web development needed for virtually any kind of website. The updates to this edition include full compatibility with Rails 7.  This indispensable guide provides integrated tutorials not only for Rails, but also for the essential Ruby, HTML, CSS, and SQL skills you need when developing web applications. Hartl explains how each new technique solves a real-world problem, and then he demonstrates it with bite-sized code that's simple enough to understand while still being useful. Whatever your previous web-development experience, this book will guide you to true Rails mastery.		Michael Hartl	Addison-Wesley Professional Ruby Series	https://books.google.ca/books/about/Ruby_on_Rails_Tutorial.html?id=-NnPEAAAQBAJ&source=kp_book_description&redir_esc=y		896		Google Scholar	2022	Ruby on Rails Tutorial: Learn Web Development with Rails	https://books.google.ca/books/about/Ruby_on_Rails_Tutorial.html?id=-NnPEAAAQBAJ&source=kp_book_description&redir_esc=y	Addison-Wesley Professional
Test-Driven Infrastructure with Chef: Bring Behavior-Driven Development to Infrastructure as Code	Since Test-Driven Infrastructure with Chef first appeared in mid-2011, infrastructure testing has begun to flourish in the web ops world. In this revised and expanded edition, author Stephen Nelson-Smith brings you up to date on this rapidly evolving discipline, including the philosophy driving it and a growing array of tools. You'll get a hands-on introduction to the Chef framework, and a recommended toolchain and workflow for developing your own test-driven production infrastructure.  Several exercises and examples throughout the book help you gain experience with Chef and the entire infrastructure-testing ecosystem. Learn how this test-first approach provides increased security, code quality, and peace of mind.		Stephen Nelson-Smith		https://books.google.ca/books?id=fT1PAQAAQBAJ&dq=Test-Driven+Infrastructure+with+Chef:+Bring+Behavior-Driven+Development+to+Infrastructure+as+Code&lr=&source=gbs_navlinks_s		308		Google Scholar	2013	Test-Driven Infrastructure with Chef: Bring Behavior-Driven Development to Infrastructure as Code	https://books.google.ca/books?id=fT1PAQAAQBAJ&dq=Test-Driven+Infrastructure+with+Chef:+Bring+Behavior-Driven+Development+to+Infrastructure+as+Code&lr=&source=gbs_navlinks_s	O'Reilly Media, Inc.
BeSoS: A Tool for Behavior-driven and Scenario-based Requirements Modeling for Systems of Systems	Systems of Systems (SoS), like connected vehicle systems, provide their functionality by the interaction of several constituent systems (CSs).[Problem] Due to the managerial, operational and evolutionary independence of the CSs in an SoS, requirements constantly change over time and linear, top-down requirements engineering methods cannot be applied without significant adaptations. New tools are needed that support the continuous and iterative specification and alignment of requirements across different levels of abstraction.[Principal Ideas] We propose to integrate the behavior-driven development (BDD) approach with an intuitive and executable scenario-based modeling of functional requirements. In this way, stakeholder expectations can be structured via features and documented in natural language as usage scenarios. Based on usage scenarios, the modeling of functional requirements can be driven by tests, allowing for the automated testing and analysis of requirements. This in turn supports the iterative specification of requirements and the alignment of stakeholder needs.[Contribution] In this paper we showcase the tool BeSoS that supports the iterative and behavior-driven specification of requirements in an SoS context. We propose a method and describe its tool components using an example. The tool is available here: https://vimeo. Com/512739942	System of Systems Engineering; Requirements Analysis; Scenario-based; Requirements Modeling; Requirements Specification	Carsten Wiecher; Joel Greenyer	CEUR Workshop Proceedings	https://ceur-ws.org/Vol-2857/pt1.pdf		7		Google Scholar	2021	BeSoS: A Tool for Behavior-driven and Scenario-based Requirements Modeling for Systems of Systems	https://ceur-ws.org/Vol-2857/pt1.pdf	CEUR Workshop
Jasmine JavaScript Testing Update	What you will learn      Understand and use the power of Jasmine to create better and more maintainable code bases     Drive your application development entirely by tests     Write modular and reusable code through the power of ECMA Script 6 (ES6) modules     Use asynchronous tests, stubs, and spies optimally     Test drive a React.js singlepage application     Optimize your code to unleash the power of tooling and automation		Zacharias Ragonha Paulo Vitor		https://sciendo.com/book/9781785283208		134		Google Scholar		Jasmine JavaScript Testing Update	https://sciendo.com/book/9781785283208	Packt Publishing Limited
TestBox: Behavior Driven Development	TestBox is a testing framework for ColdFusion (CFML) that is based on BDD (Behavior Driven Development) for providing a clean obvious syntax for writing tests. It also includes MockBox for mocking and stubbing.		Luis Majano		https://books.google.ca/books/about/TestBox.html?id=tniRrgEACAAJ&source=kp_book_description&redir_esc=y		142		Google	2015	TestBox: Behavior Driven Development	https://books.google.ca/books/about/TestBox.html?id=tniRrgEACAAJ&source=kp_book_description&redir_esc=y	CreateSpace Independent Publishing Platform
The Rails 3 Way	Ruby on Rails strips complexity from the development process, enabling professional developers to focus on what matters most: delivering business value via clean and maintainable code. The RailsTM 3 Way is the only comprehensive, authoritative guide to delivering production-quality code with Rails 3. Pioneering Rails expert Obie Fernandez and a team of leading experts illuminate the entire Rails 3 API, along with the idioms, design approaches, and libraries that make developing applications with Rails so powerful. Drawing on their unsurpassed experience and track record, they address the real challenges development teams face, showing how to use Rails 3 to maximize your productivity.  Using numerous detailed code examples, the author systematically covers Rails 3 key capabilities and subsystems, making this book a reference that you will turn to again and again. He presents advanced Rails programming techniques that have been proven effective in day-to-day usage on dozens of production Rails systems and offers important insights into behavior-driven development and production considerations such as scalability. Dive deep into the Rails 3 codebase and discover why Rails is designed the way it is, and how to make it do what you want it to do.		Obie Fernandez		https://books.google.ca/books/about/The_Rails_3_Way.html?id=slwLAqkT_Y0C&source=kp_book_description&redir_esc=y		704		Google Scholar	2010	The Rails 3 Way	https://books.google.ca/books/about/The_Rails_3_Way.html?id=slwLAqkT_Y0C&source=kp_book_description&redir_esc=y	Pearson Education
Towards Automated Requirements Checking Throughout Development Processes of Interactive Systems	The user-centered development process of interactive systems is iterative and, during multiple iterations, users have the opportunity to bring new requirements that are very likely to have an impact, not only in future development, but also affect previously developed artifacts. Manual testing of all artifacts when new requirements are introduced can be cumbersome and time consuming. For that, we need flexible methods to ensure continuous consistency and accuracy among the various artifacts employed to build interactive systems. The ultimate goal of this position paper is to briefly present our vision on an approach for automating the requirements assessment using a Behavior-Driven Development perspective. Thereby, automated tests can run early in the design process, providing a continuous quality assurance of requirements, and helping clients and teams to identify potential problems and inconsistencies before commitments with software implementation. 	Automated requirements checking; Behavior-driven development; Multi-artifact testing 	Thiago Rocha Silva; Marco Winckler	2nd Workshop on Continuous Requirements Engineering (CRE 2016) co-located with the 22nd International Conference on Requirements Engineering: Foundation for Software Quality, REFSQ	https://hal.science/hal-01712531/				Google Scholar	2016	Towards automated requirements checking throughout development processes of interactive systems	https://hal.science/hal-01712531/	HAL open science
Test-Driven Java Development, Second Edition: Invoke TDD principles for end-to-end application development, 2nd Edition	Key Features Explore the most popular TDD tools and frameworks and become more proficient in building applications Create applications with better code design, fewer bugs, and higher test coverage, enabling you to get them to market quickly Implement test-driven programming methods into your development workflowsBook Description  Test-driven development (TDD) is a development approach that relies on a test-first procedure that emphasizes writing a test before writing the necessary code, and then refactoring the code to optimize it.The value of performing TDD with Java, one of the longest established programming languages, is to improve the productivity of programmers and the maintainability and performance of code, and develop a deeper understanding of the language and how to employ it effectively.  Starting with the basics of TDD and understanding why its adoption is beneficial, this book will take you from the first steps of TDD with Java until you are confident enough to embrace the practice in your day-to-day routine.You'll be guided through setting up tools, frameworks, and the environment you need, and we will dive right into hands-on exercises with the goal of mastering one practice, tool, or framework at a time. You'll learn about the Red-Green-Refactor procedure, how to write unit tests, and how to use them as executable documentation.With this book, you'll also discover how to design simple and easily maintainable code, work with mocks, utilize behavior-driven development, refactor old legacy code, and release a half-finished feature to production with feature toggles.You will finish this book with a deep understanding of the test-driven development methodology and the confidence to apply it to application programming with Java. What you will learn Explore the tools and frameworks required for effective TDD development Perform the Red-Green-Refactor process efficiently, the pillar around which all other TDD procedures are based Master effective unit testing in isolation from the rest of your code Design simple and easily maintainable code by implementing different techniques Use mocking frameworks and techniques to easily write and quickly execute tests Develop an application to implement behavior-driven development in conjunction with unit testing Enable and disable features using feature togglesWho this book is for  If you're an experienced Java developer and want to implement more effective methods of programming systems and applications, then this book is for you.		Viktor Farcic; Alex Garcia		https://books.google.ca/books?id=NLZTDwAAQBAJ&dq=Test-Driven+Java+Development+-+Second+Edition:+Invoke+TDD+Principles+for+End-to-End+Application+Development&lr=&source=gbs_navlinks_s		324		Google Scholar	2018	Test-Driven Java Development - Second Edition: Invoke TDD Principles for End-to-End Application Development	https://books.google.ca/books?id=NLZTDwAAQBAJ&dq=Test-Driven+Java+Development+-+Second+Edition:+Invoke+TDD+Principles+for+End-to-End+Application+Development&lr=&source=gbs_navlinks_s	Packt Publishing Ltd
Ontological syntax highlighting	This paper deals with the special type of syntax highlighting which is powered by combination of DEMO methodology (Design & Engineering Methodology for Organizations), BDD technique (Behaviour-Driven Development). Main contribution of this paper is the idea how to highlight business relevant code in IDEs (Integrated Development Environments) and notice a developer about the fact that important part of code base is edited. The proposal of the implementation encounters usage of code coverage technique and user stories derived upon DEMO methodology.		Jiri Matula; Jaroslav Zacek	INTERNATIONAL CONFERENCE OF NUMERICAL ANALYSIS AND APPLIED MATHEMATICS	https://doi.org/10.1063/1.5043708				Google Scholar	2018	Ontological syntax highlighting	https://pubs.aip.org/aip/acp/article-abstract/1978/1/060006/796783/Ontological-syntax-highlighting	AIP Publishing
Executable Requirements in a Safety-Critical Context with Ada	When people who need the software and people who build the software do not understand each other, the success of a project may be impacted. In the same way, when it is difficult to know which test case corresponds to a given requirement, or if each and every requirement is fully covered by the test suite, we have a traceability issue that may also impact the success of the project. During the last few years, the agile community has suggested a new development paradigm in order to address this traceability issue. This approach is called Behaviour Driven Development (BDD). It is based on a new way of expressing requirements using a common language understandable by all parties at stake. After having described the Behaviour Driven Development in details, we introduce the XReq tool, an Open Source project developed by SOGILIS and part of the Open-DO project. It is designed to bring the Behaviour Driven Development to the Ada language and other statically typed languages. It also aims at facilitating the traceability of High and Low Level Tests in the context of DO-178B projects.	Behaviour Driven Development; Ada, Requirements; Testing; Safety-Critical Projects; DO- 178B	Christophe Baillon; Shanti Bouchez-Mongarde	Ada User Journal	https://ada-europe.org/archive/auj/auj-31-2.pdf#page=51		5		Google Scholar	2010	Executable requirements in a safety-critical context with Ada	https://ada-europe.org/archive/auj/auj-31-2.pdf#page=51	Ada Europe
ATDD by Example: A Practical Guide to Acceptance Test-Driven Development	With Acceptance Test-Driven Development (ATDD), business customers, testers, and developers can collaborate to produce testable requirements that help them build higher quality software more rapidly. However, ATDD is still widely misunderstood by many practitioners. ATDD by Example is the first practical, entry-level, hands-on guide to implementing and successfully applying it.  ATDD pioneer Markus Gartner walks readers step by step through deriving the right systems from business users, and then implementing fully automated, functional tests that accurately reflect business requirements, are intelligible to stakeholders, and promote more effective development.  Through two end-to-end case studies, Gartner demonstrates how ATDD can be applied using diverse frameworks and languages. Each case study is accompanied by an extensive set of artifacts, including test automation classes, step definitions, and full sample implementations. These realistic examples illuminate ATDD's fundamental principles, show how ATDD fits into the broader development process, highlight tips from Gartner's extensive experience, and identify crucial pitfalls to avoid. Readers will learn to      Master the thought processes associated with successful ATDD implementation     Use ATDD with Cucumber to describe software in ways businesspeople can understand     Test web pages using ATDD tools     Bring ATDD to Java with the FitNesse wiki-based acceptance test framework     Use examples more effectively in Behavior-Driven Development (BDD)     Specify software collaboratively through innovative workshops     Implement more user-friendly and collaborative test automation     Test more cleanly, listen to test results, and refactor tests for greater value  If you're a tester, analyst, developer, or project manager, this book offers a concrete foundation for achieving real benefits with ATDD now, and it will help you reap even more value as you gain experience.		Markus Gartner	Addison-Wesley Signature Series (Beck)	https://books.google.ca/books?id=RzOUlMjtfcEC&dq=ATDD+by+Example:+A+Practical+Guide+to+Acceptance+Test-Driven+Development&lr=&source=gbs_navlinks_s		240		Google Scholar	2012	ATDD by Example: A Practical Guide to Acceptance Test-Driven Development	https://books.google.ca/books?id=RzOUlMjtfcEC&dq=ATDD+by+Example:+A+Practical+Guide+to+Acceptance+Test-Driven+Development&lr=&source=gbs_navlinks_s	Addison-Wesley Professional
Towards Accountability Driven Development for Machine Learning Systems	With rapid deployment of Machine Learning (ML) systems into diverse domains such as healthcare and autonomous driving, important questions regarding accountability in case of incidents resulting from ML errors remain largely unsolved. To improve accountability of ML systems, we introduce a framework called Accountability Driven Development (ADD). Our framework reuses Behaviour Driven Development (BDD) approach to describe testing scenarios and system behaviours in ML Systems' development using natural language, guides and forces developers and intended users to actively record necessary accountability information in the design and implementation stages. In this paper, we illustrate how to transform accountability requirements to specific scenarios and provide syntax to describe them. The use of natural language allows non technical collaborators such as stakeholders and non ML domain experts deeply engaged in ML system development to provide more comprehensive evidence to support system's accountability. This framework also attributes the responsibility to the whole project team including the intended users rather than putting all the accountability burden on ML engineers only. Moreover, this framework can be considered as a combination of both system test and acceptance test, thus making the development more efficient. We hope this work can attract more engineers to use our idea, which enables them to create more accountable ML systems.	Accountability; Behaviour driven development; Machine learning; Model card	Chiu Pang Fung; Wei Pang; Iman Naja; Milan Markovic; Peter Edwards	CEUR Workshop Proceedings	http://ceur-ws.org/Vol-2894/short4.pdf		8		Google Scholar	2021	Towards accountability driven development for machine learning systems	https://researchportal.hw.ac.uk/en/publications/towards-accountability-driven-development-for-machine-learning-sy	CEUR-WS
The RSpec Book: Behaviour-driven Development with RSpec, Cucumber, and Friends	"Behaviour-Driven Development (BDD) gives you the best of Test Driven Development, Domain Driven Design, and Acceptance Test Driven Planning techniques, so you can create better software with self-documenting, executable tests that bring users and developers together with a common language.  Get the most out of BDD in Ruby with The RSpec Book, written by the lead developer of RSpec, David Chelimsky.  You'll get started right away with RSpec 2 and Cucumber by developing a simple game, using Cucumber to express high-level requirements in language your customer understands, and RSpec to express more granular requirements that focus on the behavior of individual objects in the system. You'll learn how to use test doubles (mocks and stubs) to control the environment and focus the RSpec examples on one object at a time, and how to customize RSpec to ""speak"" in the language of your domain.  You'll develop Rails 3 applications and use companion tools such as Webrat and Selenium to express requirements for web applications both in memory and in the browser. And you'll learn to specify Rails views, controllers, and models, each in complete isolation from the other.  Whether you're developing applications, frameworks, or the libraries that power them, The RSpec Book will help you write better code, better tests, and deliver better software to happier users."		David Chelimsky	Pragmatic Bookshelf Series	https://books.google.ca/books/about/The_RSpec_Book.html?id=0rxoPgAACAAJ&source=kp_book_description&redir_esc=y		420		Google Scholar	2010	The RSpec Book: Behaviour Driven Development with Rspec, Cucumber, and Friends	https://books.google.ca/books/about/The_RSpec_Book.html?id=0rxoPgAACAAJ&source=kp_book_description&redir_esc=y	Pragmatic Bookshelf
A comprehensive approach for software dependency resolution	Software reuse is prevalent in software development. It is not uncommon that one software product may depend on numerous libraries/products in order to build, install, or run. Software reuse is difficult due to the complex interdependency relationships between software packages. In this work, we presented four approaches to retrieve such dependency information, each technique focuses on retrieving software dependency from a specific source, including source code, build scripts, binary files, and Debian spec. The presented techniques were realized by a prototype tool, DEx, which is applied to a large collection of Debian projects in a comprehensive evaluation. Through the comprehensive analysis, we evaluate the presented techniques, and compare them from various aspects.	software dependency; software reuse; free and open source software; Debian	Zhang, Hanyu	Electronic Theses and Dissertations (ETD) 	http://hdl.handle.net/1828/3432				Google Scholar	2011	A comprehensive approach for software dependency resolution	https://dspace.library.uvic.ca/items/813ef826-f57f-4d9c-a4d8-3bfaa26e1786	University of Victoria
A Cognitive Theory of Trust			Hill, Claire A.; O'Hara, Erin Ann	Washington University Law Review	https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/walq84&id=1733&men_tab=srchresults		1717-1796		Google Scholar	2006	A cognitive theory of trust	https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/walq84&id=1733&men_tab=srchresults	Hein Online
Trust in Information Technology			D. Harrison McKnight	The Blackwell Encyclopedia of Management. Vol. 7 Management Information Systems	https://www.researchgate.net/profile/D-Mcknight-2/publication/297731446_Trust_in_Information_Technology/links/6093b72c458515d315fc3690/Trust-in-Information-Technology		329-331		Google Scholar	2005	Trust in information technology	https://www.researchgate.net/profile/D-Mcknight-2/publication/297731446_Trust_in_Information_Technology/links/6093b72c458515d315fc3690/Trust-in-Information-Technology	G. B. Davis (Ed.)
The Development and Evaluation of an Online Master's Module Using an Open-Source Software Package			Eduardo, Feliciana	ProQuest Dissertations & Theses	https://www.proquest.com/openview/f6bd433f7dd0be74a835915432ce9728/1?cbl=2026366&diss=y&pq-origsite=gscholar		24		Google Scholar	2007	The development and evaluation of an online master's module using an open-source software package	https://www.proquest.com/openview/f6bd433f7dd0be74a835915432ce9728/1?cbl=2026366&diss=y&pq-origsite=gscholar	University of Pretoria
Estimating Security Risk in Open Source Package Repositories: An Empirical Analysis and Predictive Model of Software Vulnerabilities	The scope, frequency, and severity of software vulnerabilities are increasing along with breaches related to insecure systems, while system security improvement methods have remained largely stagnant. Though research has examined large Free/Libre Open Source Software (F/LOSS) for patterns of security defects, a gap remains in the collection, analysis, and correlation of latent security defects across smaller projects and libraries reused by developers as references in their projects. Most studies identifying defects for vulnerability analysis and development of predictive models have relied on post-hoc mining of security defects identified by the community or external systems, rather than direct measurement. This quantitative, non- experimental, explanatory/predictive theory and model building research sequentially collected, processed, and analyzed source and project characteristics to evaluate an objective risk model for reuse of components hosted on GitHub. This study extends prior studies of relationships between software metrics, vulnerability frequency, and the corresponding level of security risk by expanding on vulnerability prediction techniques. A simplified implementation of the risk level estimation model (RLEM) was used to provide a quantitative measure that is easily generated and/or gathered by automated systems. Custom data collection software was developed to parse GitHub archives for all commits and to retrieve supporting repository information from publicly available API feeds for storage in a relational database. After retrieving project information and sizing data, the latest version of the code in the master branch was downloaded using git. Each project was compiled and processed using Fortify static code analysis (SCA). Of the 156 projects scanned, 28 failed during the build process and were replaced with new random selections. Once complete, the resulting dataset contained a total of 26,222 files for analysis using SCA. The results of the SCA process identified a total of 91,405 defects which were filtered to only those matching a vulnerability rule from the common weakness enumeration definitions published by the National Institute of Science and Technology, leaving a total of 24,597 detected vulnerabilities. A composite variable for estimated security risk (ESR) was computed from the output of SCA against projects collected from GitHub and analyzed in the context of the Common Vulnerability Scoring System version 2 to calculate ESR in a method that could be easily adopted by development teams. Multiple linear regression against the composite variable showed that the model was statistically significant and explained 80% of the observed ESR, which demonstrates potential usefulness in the development lifecycle.		Smith, Levii J	ProQuest Dissertations & Theses	https://www.proquest.com/docview/2290797881?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses		154		Google Scholar	2019	Estimating Security Risk in Open Source Package Repositories: An Empirical Analysis and Predictive Model of Software Vulnerabilities	https://www.proquest.com/docview/2290797881?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses	Capella University
Small business preference for software package	Until late 2016, the majority of small business uses QuickBooks and Excel to do their accounting work. However, the functions of these accounting tools are insufficient, and also the demands of companies are increasing. It has become inefficient and antiquated to use the software. A small business looked for new accounting software, but found that choosing an accounting software system is not straightforward, as every software package consists of different functions for users. The aim of this research was to analyse small business’ perceptions and preferences for accounting software systems in New Zealand. This is followed by an analysis of why users might want to replace their previous accounting software systems. In addition, the requirements for selecting accounting software were analysed. The main method used was qualitative research. Three people were interviewed regarding the reason for replacing their previous accounting software system. The first interviewee had not changed their accounting software system previously, but their reason for replacing their current accounting software system was that their previous accounting software system lacked the functionality he wanted. The second interviewee said e she disliked that the system frequently crashed and that it lacked the features she wanted. The reason why the last interviewee changed her previous accounting software system was that MYOB system could not improve her work efficiency.		Shuwen Li	Proceedings of the Applied Management Conference	http://researcharchive.wintec.ac.nz/id/eprint/6442				Google Scholar	2018	Small business preference for software package	https://researcharchive.wintec.ac.nz/id/eprint/6442/	Waikato Institute of Technology
Toward Decentralized Package Management	The mutation of the software economy toward crowdsourcing, the explosion of the number of de- vices and the increasing need for quickly-released software call for revisiting the way software are deployed and managed. The current approach adopted by most software package management sys- tems is to rely on a single distributor, who collects packages from upstream sources, tests, releases and distributes them through a centralized channel, called repository. In this paper, we identify the major downsides of this centralized architecture and promote a distributed approach for software de- ployment. That is, in a network consisting of interconnected symmetric peers, all the developers are allowed to release and distribute software asynchronously. The discovery and retrieval of software is achieved through the communication among peers. We highlight the impacts of using such an approach and define a specific format of metadata that supports distributed package release		Fabien Dagnat; Gwendal Simon; Xu Zhang	workshop on logics for component configuration	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=f58969041a5bb371b0c48ed49948d91a26d874c7		7		Google Scholar	2011	Toward Decentralized Package Management	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=f58969041a5bb371b0c48ed49948d91a26d874c7	Lococo
Why Free-Libre / Open Source Software (FLOSS)? Look at the Numbers!			David A. Wheeler		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0902474713fbbe229b9406d5df5f118b28160a8e		40		Google Scholar	2011	Why Free-Libre/Open Source Software (FLOSS)? Look at the Numbers!	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0902474713fbbe229b9406d5df5f118b28160a8e	
Open source software: perspectives for development	This report intends to help information technology decision-makers in developing countries understand the dynam- ics associated with implementing open source software solutions. We present information on the forces shaping the open source market in both the public and private sectors. Case studies, in Part II, illustrate ways in which Open Source Software is used or adapted to meet needs in developing regions. Perspectives from some who have exam- ined the market are also presented to encourage the exchange of information and promote discussion of this important topic.  Interest in Open Source Software (OSS) is increasing globally. Technology providers are broadening their support, deployments are increasing, investors are taking notice, new open source projects are appearing, and competitors and lobbyists are marketing against it. While the strength of these dynamics can be difficult to gauge, the trends are positive for OSS. However, concerns of an unproven business proposition, beliefs that the software is unreliable, along with technology terms, acronyms, and products that are unfamiliar to decision- makers necessitate continuing education and awareness building.  OSS is about choice. While initial OSS interest is brought about by the potential for lower cost, a longer-term benefit may be increased choice to consumers of Information and Communication Technology (ICT). The emer- gence of OSS creates more options to address ICT needs. Solutions can mix both proprietary and OSS compo- nents. New development models emphasize collaboration and community. Market competition increases. Projects, when coupled with a focus on interoperability and open standards, should benefit from a greater emphasis on process/business needs and less on technical underpinnings.  Government leaders have a key role to play. As software use expands into more areas of human endeavor, consideration should be given to improving access to the market for all software providers. The software market is dominated by a few proprietary providers who spend over $6.5 billion annually to communicate their message. There is a need to assist OSS initiatives in accessing market opportunities.  There is opportunity for local capacity development. The nature of OSS lends itself to providing an ICT environment based on local ownership and autonomy. Software can be adapted to address localization, while introducing more flexibility and independence to the software development process. With OSS growth in an early stage for development, business opportunities exist for various complementary solutions and services.  The debate: “open” versus “proprietary” software. The OSS debate is multifaceted and the rhetoric is increas- ing. Some organizations position the debate on the merits of commercial versus non-commercial software. With commercial interest in OSS increasing, the discourse should be centered on the merits of OSS versus propri- etary solutions.  OSS is but one part of an ICT strategy. Traditional development methodologies are still required to deliver high quality, usable and functional applications. OSS should be evaluated and deployed using the same methodologies and disciplined care as any other ICT solution. While some challenges related to open source use in developing countries may be unique because of the social and economic landscape, most dynamics are global, requiring consideration by decision-makers in all countries.		Paul Dravis	Information for Development Program	https://documents1.worldbank.org/curated/en/199011468779102922/pdf/276800Open0Sou1re0nov0200301public1.pdf		44		Google Scholar	2003	Open source software: perspectives for development	https://documents1.worldbank.org/curated/en/199011468779102922/pdf/276800Open0Sou1re0nov0200301public1.pdf	The World Bank
The open source software development phenomenon: An analysis based on social network theory	The OSS movement is a phenomenon that challenges many traditional theories in economics, software engineering, business strategy, and IT management. Thousands of software programmers are spending tremendous amounts of time and effort writing and debugging software, most often with no direct monetary compensation. The programs, some of which are extremely large and complex, are written without the benefit of traditional project management, change tracking, or error checking techniques. Since the programmers are working outside of a traditional organizational reward structure, accountability is an issue as well. A significant portion of internet e-commerce runs on OSS, and thus many firms have little choice but to trust mission-critical e-commerce systems to run on such software, requiring IT management to deal with new types of socio-technical problems. A better understanding of how the OSS community functions may help IT planners make more informed decisions and develop more effective strategies for using OSS software. We hypothesize that open source software development can be modeled as self-organizing, collaboration, social networks. We analyze structural data on over 39,000 open source projects hosted at SourceForge.net involving over 33,000 developers. We define two software developers to be connected ó part of a collaboration social network ó if they are members of the same project, or are connected by a chain of connected developers. Project sizes, developer project participation, and clusters of connected developers are analyzed. We find evidence to support our hypothesis, primarily in the presence of power-law relationships on project sizes (number of developers per project), project membership (number of projects joined by a developer), and cluster sizes. Potential implications for IT researchers, IT managers, and governmental policy makers are discussed	Open source software; social networks	Gregory Madey; Vincent Freeh; Renee Tynan	Americas Conference on Information Systems	https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1606&context=amcis2002		9		Google Scholar	2002	The open source software development phenomenon: An analysis based on social network theory	https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1606&context=amcis2002	Association for Information Systems Electronic Library
Developer Motivations, Social Aspects and Challenges of Open Source Software	The software industry has long used a proprietary approach (i.e. where a firm keeps its software source code protected from public). Open Source Software (OSS), which emerged as an alternative to the proprietary software, has seen a steady growth in the last decade. OSS is developed predominantly by volunteer developers participating in collaborative projects. The developers are the driving force behind OSS. It is therefore important for any firm engaged in OSS, to find out what motivates the developers to participate in OSS projects. Some pervious literature refers to the OSS community as a social movement. If so, this poses a challenge to firms, as a social movement consists of collective actions by its members, i.e. developers of OSS. The purpose of this study was to identify the motivations of OSS developers using a qualitative interview approach. The results were further analysed using existing social science theories, to determine whether the contribution of developers display aspects of volunteer participation in a social movement. Purposeful selection method was used to define the sample for the study, from an OSS development project from called Axis C++. The developers displayed a wide range of extrinsic motivations; however the intrinsic motivations seemed to be stronger as they were shared by many developers. Developers were committed to develop better software and maintain its quality and enjoyed writing software programs. A few developers seemed to be motivated by the opportunity to develop their reputation by participating in OSS projects. Some developers were motivated by the ability to develop their technical and personal skills by participating in an OSS project. This behaviour was associated with the belief that it would bring future career benefits. A significant finding is that the developers were motivated to protect software freedom (i.e. preserve the idea of open source), and to that end take action against proprietary software monopolies. The analysis revealed that the OSS community as displaying aspects of social movement participation. Overall the study highlighted a number of opportunities and threats to firms arising from the motivations of OSS developers.		Athukorale, Iranga Supun	ProQuest Dissertations & Theses	https://www.proquest.com/docview/2281200032?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses		108		Google Scholar	2006	Developer Motivations, Social Aspects and Challenges of Open Source Software.	https://www.proquest.com/docview/2281200032?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses	University of Surrey
The Effect of License Type and Status Hierarchy on Developer Motivation in Open Source Communities.	The motivational drivers of open source software developers have been identified and catalogued by various investigators since about 2000. Open source licenses range in restrictiveness between those that allow developers to mingle their code freely with proprietary code to those which prohibit such commingling In addition to formal rules, meritocracies emerge to reward effort and performance, and also to direct, coordinate, and punish other participants. We show how these variables are related to motivations.		Allyn, Mark; Misra, Ram; Kozyreva, Ana	Proceedings of the Northeast Business & Economics Association	https://openurl.ebsco.com/EPDB%3Agcd%3A8%3A28859848/detailv2?sid=ebsco%3Aplink%3Ascholar&id=ebsco%3Agcd%3A48108443&crl=c&link_origin=scholar.google.com		p76		Google Scholar	2008	The Effect of License Type and Status Hierarchy on Developer Motivation in Open Source Communities.	https://openurl.ebsco.com/EPDB%3Agcd%3A8%3A28859848/detailv2?sid=ebsco%3Aplink%3Ascholar&id=ebsco%3Agcd%3A48108443&crl=c&link_origin=scholar.google.com	EBSCO
Analysis of the Impact of Open Source Software			Nic Peeling; Julian Satchell		https://www.marchnetworks.com/wp-content/uploads/2013/09/Analysis20of20the20Impact20of20Open20Source20Software.pdf		51		Google Scholar	2001	Analysis of the impact of open source software	https://www.marchnetworks.com/wp-content/uploads/2013/09/Analysis20of20the20Impact20of20Open20Source20Software.pdf	QinetiQ
Software Size Uncertainty: The Effects of Growth and Estimation Variability	Examination of currently-accepted software cost, schedule, and defect estimation algorithms reveals a common acknowledgment that estimated software size is the single most influential independent variable. Unfortunately, “The most important business decisions about a software project are made at the time of minimum knowledge and maximum uncertainty.” This includes minimum knowledge and maximum uncertainty about a software product’s effec- tive size at the time when most estimating is done. Further complicating the issue of estimate uncertainty, in the au- thor’s opinion, is the lack of a commonly-accepted taxon- omy. This paper proposes definitions for and the relation- ship between two key contributors to software size uncer- tainty: growth and estimation process variability, both be- ing distributions, the dispersions of which decrease as a function of project progress.		Mike Ross		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ea171e8ddfd28301416bde1510868869dca5a7f0		9		Google Scholar	2007	Software Size Uncertainty: The Effects of Growth and Estimation Variability	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ea171e8ddfd28301416bde1510868869dca5a7f0	r2Estimating
Software Development Kits for Cloud Computing	High Performance processing is barely bidden in cloud service as due to sluggish and ineffective Virtual Machine message applied on the similar server process as fortunate with giant dormancy among distant components of cloud. The virtual machine cloud is based on device framework of similar hardware device. The contrivance enhanced ruined with Linux modernize at long back. The distributed memory is made with filled cloud amalgamation like newest version Cloud Ubuntu used for cloud storing and handling power of factors with transfer control protocol techniques that might appreciably upturn implementation. Conclusively, we take generated communiqué network that discharge replace transfer control network, raising more enactment development.	virtual machine cloud; cloud computing; transfer control protocol	Thirumurugan Shanmugam; Viji Vinod; S.V.Tresa Sangeetha; S.Radha Rammohan; T. Muralidharan	International Journal of Advanced Science and Technology	https://www.researchgate.net/profile/Thirumurugan-Shanmugam/publication/341643480_Software_Development_Kits_for_Cloud_Computing/links/5ecd045ca6fdcc79e6d9b253/Software-Development-Kits-for-Cloud-Computing.pdf		5069-5073		Google Scholar	2020	Software Development Kits for Cloud Computing	https://www.researchgate.net/profile/Thirumurugan-Shanmugam/publication/341643480_Software_Development_Kits_for_Cloud_Computing/links/5ecd045ca6fdcc79e6d9b253/Software-Development-Kits-for-Cloud-Computing.pdf	SERSC
SIGNALS BY SOFTWARE		Acquisition; carrier to noise; code division multiple access; Global Positioning System; indoor GPS; Labview; Matlab; navigation; satellite navigation System; signal to noise ratio; software GPS; VHDL	Laurent M. Duchateau	Dépôt Institutionnel de l’Université libre de Bruxelles	https://dipot.ulb.ac.be/dspace/bitstream/2013/211344/1/934646f3-35da-4bd9-9a4e-e753b54aece3.txt		92		Google Scholar	2002	SIGNALS BY SOFTWARE	https://dipot.ulb.ac.be/dspace/bitstream/2013/211344/1/934646f3-35da-4bd9-9a4e-e753b54aece3.txt	Université libre de Bruxelles
Software to support expert elicitation : An exploratory study of existing software packages	Expert elicitations are used to gather the informed opinion of experts on topics about which little or no knowledge is available. They can also be used to build consensus on controversial knowledge. Software packages can provide important support, but a lot of researchers are not well aware of that. The National Institute for Public Health and the Environment (RIVM) therefore made an overview of the different possibilities. For this overview scientific literature has been reviewed, supplemented with sources on the Internet. This has been funded from the strategic research program (SOR) of the institute.  There appear to exist software packages that provide support in: 1) the collaboration of experts and building consensus; 2) characterization of uncertainties; 3) selection of experts; 4) design and execution of the process of estimation and; 5) aggregation and reporting about outcomes.  When designing and executing the estimation process, software can assist in developing and analysing conceptual models. They can also assist in the assessment of scenarios and the estimation of model parameters. For the characterization of uncertainties only one type of software is available: the software of the Netherlands Environmental Assessment Agency (PBL).  Currently, supporting software appears not to be equipped to moderate expert elicitations by mail or by using Internet as an alternative for gathering a group of experts on one specific location.  As the exact and future usage of expert elicitation at RIVM is not known, it is not possible to provide specific advice on the use of software in these specific situations.		Devilee JLA; Knol AB	RIVM letter report 630003001	https://rivm.openrepository.com/entities/publication/9ba7ddb6-0628-431a-9fa4-e84c96b2f8e2				Google Scholar	2012	Software to support expert elicitation: An exploratory study of existing software packages	https://rivm.openrepository.com/entities/publication/9ba7ddb6-0628-431a-9fa4-e84c96b2f8e2	Rijksinstituut voor Volksgezondheid en Milieu RIVM
Reputation management of an Open Source Software system based on the trustworthiness of its contributions.	La industria del software depende cada vez más en componentes desarrollados externamente, siendo algunos proyectos de Código Abierto (Open Source). Estos componentes pueden depender a su vez de otros proyectos de software por lo que hay un cierto grado de incertidumbre sobre la calidad de estos componentes externos. Aunque la calidad de este software suele ser buena, eventos como la vulnerabilidad Heartbleed en OpenSSL han resaltado la importancia de gestionar estos elementos. Diferentes formas de asesorar sobre la calidad, respecto a la fiabilidad de un elemento, han sido analizadas teniendo en cuenta el origen e historial sobre los componentes individuales que forman un proyecto. Diferentes métricas han sido investigadas y finalmente un prototipo de prueba de concepto ha sido desarrollado		Garcia Garcia, Cristina	Trabajos Fin de Grado	http://hdl.handle.net/10651/38287		93		Google Scholar	2016	Reputation management of an Open Source Software system based on the trustworthiness of its contributions	https://digibuo.uniovi.es/dspace/handle/10651/38287	Repositorio Institucional de la Universidad de Oviedo
How copyleft uses license rights to succeed in the open source software revolution and the implications for article 2B			Robert W. Gomulkiewicz	Houston Law Review 36 Hous. L. Rev.	https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/hulr36&id=191&men_tab=srchresults		179-194		Google Scholar	1999	How copyleft uses license rights to succeed in the open source software revolution and the implications for article 2B	https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/hulr36&id=191&men_tab=srchresults	Hein Online
Open Source Software as Consumer Integration into Production	Open source software is emerging as a potentially important competitive force in the software industry, capturing the attention of venture capitalists and computing industry executives. Yet very little is known about how open source software, which is created collectively by individual volunteers, will compete with closed source software. This paper models open source software as consumer integration into production, in which users organize to produce a good for themselves. The model predicts that open source software will not compete in all product markets, but where it does, will be of higher quality than closed source software. An empirical study finds support for these predictions. 	Software; open source software; nonprofit; vertical integration; R&D productivity	Jennifer W. Kuan		https://dx.doi.org/10.2139/ssrn.259648		53		Google Scholar	2001	Open source software as consumer integration into production	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=259648	California State University Monterey Bay, College of Business
The Economics of Open Source Software: A Survey of the Early Literature	This paper reviews the recent literature on the economics of open source software. Two different sets of issues are addressed. The first looks at the incentives of programmers to participate in open source projects. The second considers the business models used by profit-making firms in the open source industry, and the effects on existing closed source firms. Some possible future research directions are also given.		Schiff Aaron	Review of Network Economics	https://doi.org/10.2202/1446-9022.1004 				Google Scholar	2002	The economics of open source software: A survey of the early literature	https://econpapers.repec.org/article/bpjrneart/v_3a1_3ay_3a2002_3ai_3a1_3an_3a5.htm	EconPapers
Open Source Software: The New Intellectual Property Paradigm	  Open source methods for creating software rely on developers who voluntarily reveal code in the expectation that other developers will reciprocate. Open source incentives are distinct from earlier uses of intellectual property, leading to different types of inefficiencies and different biases in R&D investment. Open source style of software development remedies a defect of intellectual property protection, namely, that it does not generally require or encourage disclosure of source code. We review a considerable body of survey evidence and theory that seeks to explain why developers participate in open source collaborations instead of keeping their code proprietary, and evaluates the extent to which open source may improve welfare compared to proprietary development. 		Stephen M. Maurer; Suzanne Scotchmer 	Economics and Information Systems	https://doi.org/10.3386/w12148				Google Scholar	2006	Open source software: the new intellectual property paradigm	https://www.nber.org/papers/w12148	Elsevier Science
“INFECTIOUS” OPEN SOURCE SOFTWARE: SPREADING INCENTIVES OR PROMOTING RESISTANCE?	Some free or open source software infects other software with its licensing terms. Popularly, this is called a viral license, but the software is not a computer virus. Free or open source software is a copyright-based licensing system. It typically allows modification and distribution on conditions such as source code availability, royalty free use and other requirements. Some licenses require distribution of modifications under the same terms. A license is infectious when it has a strong scope for the modifications provision. The scope arises from a broad conception of software derivative works. A strong infectious ambit would apply itself to modified software, and to software intermixed or coupled with non-open-source software. Popular open source software, including the GNU/Linux operating system, uses a license with this feature. This Article assesses the efficacy of broad infectious license terms to determine their incentive effects for open source and proprietary software. The analysis doubts beneficial effects. Rather, on balance, such terms may produce incentives detrimental to interoperability and coexistence between open and proprietary code. As a result, open source licensing should precisely define infectious terms in order to support open source development without countervailing effects and misaligned incentives.		Greg R. Vetter	RUTGERS LAW JOURNAL	https://www.law.uh.edu/faculty/gvetter/documents/vetter.infectiousoss.incentivesorresistance.final.pdf		110		Google Scholar	2004	Infectious open source software: Spreading incentives or promoting resistance	https://www.law.uh.edu/faculty/gvetter/documents/vetter.infectiousoss.incentivesorresistance.final.pdf	
Managing the bazaar: commercialization and peripheral participation in mature, community-led free/open source software projects 	The thesis investigates two fundamental dynamics of participation and collaboration in mature, community-led Free/Open Source (F/OS) software projects - commercialization and peripheral participation. The aim of the thesis is to examine whether the power relations that underlie the F/OS model of development are indicative of a new form of power relations supported by ICTs. Theoretically, the thesis is located within the Communities of Practice (CoP) literature and it draws upon Michel Foucault's ideas about the historical and relational character of power. It also mobilizes, to a lesser extent, Erving Goffman's notion of `face-work'. This framework supports a methodology that questions the rationality of how F/OS is organized and examines the relations between employed coders and volunteers, experienced and inexperienced coders, and programmers and nonprogrammers. The thesis examines discursive and structural dimensions of collaboration and employs quantitative and qualitative methods. Structural characteristics are considered in the light of arguments about embeddedness. The thesis contributes insights into how the gift economy is embedded in the exchange economy and the role of peripheral contributors. The analysis indicates that community-integrated paid developers have a key role in project development, maintaining the infrastructure aspects of the code base. The analysis suggests that programming and non-programming contributors are distinct in their make-up, priorities and rhythms of participation, and that learning plays an important role in controlling access. The results show that volunteers are important drivers of peripheral activities, such as translation and documentation. The term `autonomous peripherality' is used to capture the unique characteristics of these activities. These findings support the argument that centrality and peripherality are associated with the division of labour, which, in turn, is associated with employment relations and frameworks of institutional support. The thesis shows how the tensions produced by commercialization and peripheral participation are interwoven with values of meritocracy, ritual and strategic enactment of the idea of community as well as with tools and techniques developed to address the emergence of a set of problems specific to management and governance. These are characterized as `technologies of communities'. It is argued that the emerging topology of F/OS participation, seen as a `relational meshwork', is indicative of a redefinition of the relationship between sociality and economic production within mature, community-led F/OS projects.		Berdou, Evangelia	LSE History of Thought theses	http://etheses.lse.ac.uk/id/eprint/116				Google Scholar	2007	Managing the Bazaar: Commercialization and peripheral participation in mature, community-led Free/Open source software projects	http://etheses.lse.ac.uk/116/	London School of Economics and Political Science
Process Improvement and Risk Management in Off-The-Shelf Component-Based Development	Reusing software components from third-party vendors is one key technology to gain shorter time-to-market and better quality of the software system. These components, also known as OTS (Off-the-Shelf) components, come in two types: COTS (Commercial-Off-The-Shelf) and OSS (Open–Source-Software) components. To use OTS components successfully, it is necessary to know how the development processes and methods have to be adapted. Most current studies are either theoretical proposals without empirical assessment or case studies in similar project contexts. It is therefore necessary to conduct more empirical studies on how process improvement and risk management were performed and what were the results in various project contexts. The overall research design of the work was a sequential, mixed method design that consists of several empirical studies. The thesis contains five novel main contributions (C1 to C5) and one minor contribution (C6): C1. Better understanding on reusing in-house built components. A quantitative preliminary study shows that reusing in-house built components has the same challenges as OTS components related to requirements (re)negotiation, component documentation, and the specification of quality attributes. The informal communication between developers supplements component documentation. The component repository is not a key factor to successful component reuse. C2: Summarizing the state-of-the-practice of development and selection processes in OTS component-based projects. A qualitative pre-study, a quantitative main study, and a follow-up study with industrial seminar show that OTS component-based development processes are typically variations of well-known process models, mixed with OTS specific activities. Concerning the OTS component selection, we find that mainly familiarity-based and Internet searches with hands-on-trials processes are applied. We summarize the state-of-the-practice of OTS processes into seven scenarios and propose how to customize the processes in OTS component-based projects. C3: Empirical verification of the risk management proposals in OTS component- based development. The results of the main study reveal that allocating more effort into learning OTS components, performing the integration testing early, evaluating the quality of OTS components thoroughly, and following the update of OTS components have helped to mitigate corresponding risks. However, some problems, such as wrong estimation of the integration efforts and inefficient debugging, still happen frequently in practice and need further investigations. C4: Better understanding of decision-making in OTS component-based development. By comparing projects using COTS components with those using OSS components, we reveal who, why, and the results of using different OTS components. C5: Empirical validation of six newly published theses from the literature, which are related to process and risk management in OTS component-based development. The results of the main study support four theses and contradict the two others. C6. Improved understanding about performing an international survey in the ICT industry (minor contribution). The main study is probably the first software engineering survey using a representative subset (sample) of industrial companies. Lessons learned from performing such as study reveal that, at best, we can achieve a stratified-random sample of ICT companies, followed by a convenient sample of relevant projects.		Jingyue Li		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0cf2e8e1862aa0310aaa9e2ce688c50c046337db				Google Scholar	2006	Process improvement and risk management in Off-the-Shelf Component-based development	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0cf2e8e1862aa0310aaa9e2ce688c50c046337db	Norwegian University of Science and Technology
The Ecology of Open-Source Software Development	Open Source Software (oss) is an innovative method of developing software applications that has been very successful over the past eight to ten years. A number of theories have emerged to explain its success, mainly from economics and law. We analyze a very large sample of oss projects and find striking patterns in the overall structure of the development community. The distribution of projects on a range of activity measures is spectacularly skewed, with only a relatively tiny number of projects showing evidence of the strong collaborative activ- ity which is supposed to characterize oss. Our findings are consistent with prior, smaller-scale empirical research. We argue that these find- ings pose problems for the dominant accounts of oss. We suggest that the gulf between active and inactive projects may be explained by social-structural features of the community which have received little attention in the existing literature. We suggest some hypotheses that might better predict the observed ecology of projects.		Kieran Healy; Alan Schussman		https://flossmole.org/sites/flosshub.org/files/healyschussman.pdf		24		Google Scholar	2003	The ecology of open-source software development	https://flossmole.org/sites/flosshub.org/files/healyschussman.pdf	University of Arizona
Combining Static Source Code Analysis and Threat Assessment Modeling For Testing Open Source Software Security	Organizations that implement open source software in their system before they verify the software for security vulnerabilities are more vulnerable to attacks. Therefore, it is important to discover and fix vulnerabilities in open source software before their implementation. Nowadays different techniques exist that help in the vulnerability discovery. The goal of this project is to improve the security of open source software by discovering various source code vulnerabilities using static source code analysis technique, and design and architectural vulnerabilities by developing a threat risk model. I conducted a case study on a remote desktop connection manager application using two static analysis tools and one threat risk modeling tool. In the case study performed, I found that the static analysis tools discovered large number of different types of vulnerabilities on the application. I also discovered some design and architectural vulnerabilities using the threat risk modeling tool. The results obtained from the case study suggest that it is unsafe to deploy open source software in a system without first verifying it for vulnerabilities.		Abraham Ghebrehiwet Ghebremedhin		https://uia.brage.unit.no/uia-xmlui/bitstream/handle/11250/137555/masteroppgave.pdf?sequence=1		121		Google Scholar	2012	Combining static source code analysis and threat assessment modeling for testing open source software security	https://uia.brage.unit.no/uia-xmlui/bitstream/handle/11250/137555/masteroppgave.pdf?sequence=1	University of Agder
Software release planning under soft resource and dependency constraints	The goal of incremental software release planning is to assign planning items to future releases so that value is maximized while various resource, technical and other constraints are satisfied. We propose a new approach to this problem called soft-EVOLVE II, in which a project is first modeled using a highly-general model with a nonlinear objective function, and then a set of high-value release plans is generated using a hybrid genetic algorithm. Our formalization naturally extends the established EVOLVE II model by Ruhe et al., but is capable of expressing a much larger variety of constraints between features, in which traditional, Boolean feasibility constraints are replaced entirely by soft constraints. Such a formulation allows for highly-detailed modeling while taking uncertainty into account. We discuss the primary hypotheses and goals of this research, as well as potential validity threats and their control.		Mark Przepiora		https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=4e3npx4AAAAJ&citation_for_view=4e3npx4AAAAJ:u-x6o8ySG0sC				Google Scholar		Software release planning under soft resource and dependency constraints	https://scholar.google.ca/citations?view_op=view_citation&hl=en&user=4e3npx4AAAAJ&citation_for_view=4e3npx4AAAAJ:u-x6o8ySG0sC	
Analysis of issue and dependency management in open-source software projects	Modern software development relies on open-source software to facilitate reuse and reduce redundant work. Software developers use open-source packages in their projects without having insights into how these components are being developed and maintained. The aim of this thesis is to develop approaches for analyzing issue and dependency management in software projects. Software projects organize their work with issue trackers, tools for tracking issues such as development tasks, bug reports, and feature requests. By analyzing issue handling in more than 4,000 open-source projects, we found that many issues are left open for long periods of time, which can result in bugs and vulnerabilities not being fixed in a timely manner.  This thesis proposes a method for predicting the amount of time it takes to resolve an issue by using the historical data available in issue trackers. Methods for predicting issue lifetime can help software project managers to prioritize issues and allocate resources accordingly. Another problem studied in this thesis is how software dependencies are used.  Software developers often include third-party open-source software packages in their project code as a dependency. The included dependencies can also have their own dependencies. A complex network of dependency relationships exists among open-source software packages. This thesis analyzes the structure and the evolution of dependency networks of three popular programming languages. We propose an approach to measure the growth and the evolution of dependency networks. This thesis demonstrates that dependency network analysis can quantify what is the likelihood of acquiring vulnerabilities through software packages and how it changes over time. The approaches and findings developed here could help to bring transparency into open-source projects with respect to how issues are handled, or dependencies are updated.	software design; free software	Kikas, Riivo	TU vaitekirjad alates	http://hdl.handle.net/10062/62519				Google Scholar	2018	Analysis of issue and dependency management in open-source software projects	https://dspace.ut.ee/items/5710e30f-b8ba-4cf5-a177-c8971a6b9a88	Tartu Ulikool digiarhiiv ADA
Theoretical and Empirical Validation of Software Product Measures	In this paper we present and discuss a concrete method for validating measures of software product internal attributes and provide guidelines for its application. This method integrates much of the relevant previous work, such as measurement theory, properties of measures, and the Goal/Question/Metric paradigm (GQM). We identify two types of validation: theoretical and empirical. The former addresses the question “is the measure measuring the attribute it is purporting to measure?”, and the latter addresses the question “is the measure useful in the sense that it is related to other variables in expected ways?”		Lionel Briand; Khaled El Emam; Sandro Morasca	International Software Engineering Research Network	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ed6ded5a0881167c374b238cff2a49eb98f918b8				Google Scholar	1995	Theoretical and empirical validation of software product measures	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ed6ded5a0881167c374b238cff2a49eb98f918b8	Citeseer
Open Source Software from Commercial Firms - Tools, Complements, and Collective Invention	In recent years, open source software such as GNU/Linux, Apache, or Perl has received considerable attention. Commonly, the term is understood to imply that the software is developed by geographically dispersed volunteers. While this is true in many cases, corporate interest in and contributions to open source software have recently increased enormously, IBM being the most prominent example. The purpose of this paper is to analyze firms’ benefits and risks from contributing to software that does not allow to charge licensing fees, and that can be freely used and even modified by anyone. Corporate contributors to open source software are classified into users of the software and sellers of complements. For both groups, one of the biggest potential benefits from contributing to open source software is that outside parties, individuals as well as other firms, join in the collaborative development of the software. The central question is if this benefit does indeed materialize. Case studies, literature analysis, and a theoretical discussion show that an “open source community of commercial firms” is indeed possible, provided certain conditions are fulfilled. Finally, a parallel to the phenomenon of “collective invention” is drawn.	open source software; complements; collective invention	Joachim Henkel	Zeitschrift für Betriebswirtschaft	https://www.researchgate.net/profile/Joachim-Henkel/publication/251228522_Open_Source_Software_from_Commercial_Firms_-_Tools_Complements_and_Collective_Invention/links/54c3bb140cf219bbe4ec1cf5/Open-Source-Software-from-Commercial-Firms-Tools-Complements-and-Collective-Invention.pdf				Google Scholar	2004	Open source software from commercial firms–tools, complements, and collective invention	https://www.researchgate.net/profile/Joachim-Henkel/publication/251228522_Open_Source_Software_from_Commercial_Firms_-_Tools_Complements_and_Collective_Invention/links/54c3bb140cf219bbe4ec1cf5/Open-Source-Software-from-Commercial-Firms-Tools-Complements-and-Collective-Invention.pdf	
An Open Source Software Evaluation Model	The allure of free, industrial-strength software has many enterprises rethinking their open source strategies. However, selecting an appropriate open source software for a given problem or set of requirements is very challenging. The challenges include a lack of generally accepted evaluation criteria and a multitude of eligible open source software projects. The contribution of this work is a set of criteria and a methodology for assessing candidate open source software for fitness of purpose. To test this evaluation model, several important open source projects were examined. The results of this model were compared against the published results of an evaluation performed by the Defense Research and Development Canada agency. The proposed evaluation model relies on publicly accessible data, is easy to perform, and can be incorporated into any open source strategy.		Joel P. Confino; Phillip A. Laplante	International Journal of Strategic Information Technology and Applications (IJSITA)	https://doi.org/10.4018/jsita.2010101505		18		Google Scholar	2010	An open source software evaluation model	https://www.igi-global.com/article/open-source-software-evaluation-model/39113	IGI Global
The Uncertainty Principle in Software Engineering	This paper makes two contributions to software engineering research. First, we observe that un- certainty p ermeates software development but is rarely captured explicitly in software mo dels. We remedy this situation by presenting the Uncertainty Principle in Software Engineering (UPSE), which states that uncertainty is inherent and inevitable in software development pro cesses and pro ducts. We substantiate UPSE by providing examples of uncertainty in select software engineer- ing domains. We present three common sources of uncertainty in software development, namely human participation, concurrency, and problem-domain uncertainties. We explore in detail un- certainty in software testing, including test planning, test enactment, error tracing, and quality estimation. Second, we present a technique for mo deling uncertainty, called Bayesian b elief net- works, and justify its applicabili ty to software systems. We apply the Bayesian approach to a simple network of software artifacts based on an elevator control system. We discuss results, im- plications and p otential b ene ts of the Bayesian approach. The elevator system therefore serves as an example of applying UPSE to a particular software development situation. Finally we discuss additional asp ects of mo deling and managing uncertainty in software engineering in general	Software principles; software testing; uncertainty modeling; Bayesian networks	Hadar Ziv; Debra J. Richardson	ICSE'97, 19th International Conference on Software Engineering	http://jeffsutherland.org/papers/zivchaos.pdf		20		Google Scholar	1996	The uncertainty principle in software engineering	http://jeffsutherland.org/papers/zivchaos.pdf	
SOFTWARE METRICS AND RELIABILITY	he IEEE defines reliability as “The ability of a system or component to perform its required functions under stated conditions for a specified period of time.” To most project and software development managers, reliability is equated to correctness, that is, they look to testing and the number of “bugs” found and fixed. While finding and fixing bugs discovered in testing is necessary to assure reliability, a better way is to develop a robust, high quality product through all of the stages of the software lifecycle. That is, the reliability of the delivered code is related to the quality of all of the processes and products of software development; the requirements documentation, the code, test plans, and testing. Software reliability is not as well defined as hardware reliability, but the Software Assurance Technology Center (SATC) at NASA is striving to identify and apply metrics to software products that promote and assess reliability. This paper discusses how NASA projects, in conjunction with the SATC, are applying software metrics to improve the quality and reliability of software products. Reliability is a by-product of quality, and software quality can be measured. We will demonstrate how these quality metrics assist in the evaluation of software reliability. We conclude with a brief discussion of the metrics being applied by the SATC to evaluate the reliability .		Dr. Linda Rosenberg; Ted Hammer; Jack Shaw	9th international symposium on software reliability engineering	https://www.cs.du.edu/~snarayan/sada/teaching/COMP3705/lecture/p1/smandrelaib.pdf		8		Google Scholar	1998	Software metrics and reliability	https://www.cs.du.edu/~snarayan/sada/teaching/COMP3705/lecture/p1/smandrelaib.pdf	
Free/Libre and Open Source Software: Survey and Study	Although Open Source and Free Software are no new phenomenon, they have shown a considerable increase of their importance just in recent years. However, many aspects of this domain still appear unknown or even strange. Economic exchange relations, as they occur within the community of OS/FS developers as well as in the traditional parts of capitalist economies, are usually based on the fundamental principles of private property and monetary payments. However, these principles seem not to be applicable to OS/FS, and still this domains functions very well and gains more and more importance in the leading software markets. Based on an online survey on 2784 Open Source/Free Software developers, this report provides insights in fundamental features of the OS/FS community and its economic principles. It sheds a light on personal features of OS/FS developers, of their work and project organization, their motivations, expectations, and orientations. Finally, it illustrates the fundamental dividing lines that characterise mainly the OS/FS community and cause its outstanding position, which are the distinction between monetary and non-monetary rewards, the distinction between OS/FS and proprietary software, but also the internal distinction between Open Source Software and Free Software. The results of the study have shown that the OS/FS community is a rather young and predominantly male community with a strong professional background in the IT sector and a high educational level. The developers are mostly singles or only loosely associated with their partners. They feature a high degree of mobility, whereby the European Union appears as attractive only for developers from its member states, but not for developers from the United States of America or other world regions. Overall, developing OS/FS still resembles rather a hobby than salaried work. Besides (software) engineers and programmers, students play also a significant role in the community, but project performance and leadership is primarily a matter of professionals. Most of the developers feature networks that consist of rather few people. Nevertheless, we found a considerable large group of OS/FS developers that showed regular contacts to more than 50 other developers and that provided undoubtedly the “professional elite” within the community. Comparing the motives to start with the development of OS/FS and the motives to continue with it, we found an initial motivation for participation in the OS/FS community that rather aims at individual skills and the exchange of information and knowledge with other developers, but over time a maturing of the whole community with regard to both, commercial (material) and political aspects. To learn and to share knowledge have also been the most important issues of OS/FS developers’ expectations from other developers. Finally, regarding the main dividing lines we found the sample clearly one-sided with respect to the differences between Open Source/Free Software and proprietary software. Positive features are generally associated with OS/FS, and negative features with proprietary software. The difference between monetary and non-monetary rewards does not play a major role within the OS/FS community. The internal differentiation of the community by self-assignments to either the Open Source or to the Free Software community does not provoke a polarization of the community into two different parties. Rather, we found six distinguishable types of orientations in this respect, ranging from those who clearly assign themselves to one of the two domains and claiming fundamental differences between them to those who do not care to which domain they belong.		Rishab A. Ghosh; Ruediger Glott; Bernhard Krieger; Gregorio Robles		https://www.math.unipd.it/~bellio/FLOSS%20Final%20Report%20-%20Part%204%20-%20Survey%20of%20Developers.pdf		69		Google Scholar	2002	Free/libre and open source software: Survey and study	https://www.math.unipd.it/~bellio/FLOSS%20Final%20Report%20-%20Part%204%20-%20Survey%20of%20Developers.pdf	International Institute of Infonomics
Integrated Software Quality Evaluation: A Fuzzy Multi-Criteria Approach	Software measurement is a key factor in managing, controlling, and improving the software development processes. Software quality is one of the most important factors for assessing the global competitive position of any software company. Thus the quantification of quality parameters and integrating them into quality models is very essential. Software quality criteria are not very easily measured and quantified. Many attempts have been made to exactly quantify the software quality parameters using various models such as ISO/IEC 9126 Quality Model, Boehm's Model, McCall's model, etc. In this paper an attempt has been made to provide a tool for precisely quantifying software quality factors with the help of quality factors stated in ISO/IEC 9126 model. Due to the unpredictable nature of the software quality attributes, the fuzzy multi criteria approach has been used to evolve the quality of the software.	Civil Engineering; Software Quality Evaluation; Fuzzy Multi-Criteria Approach	Singh, Ajit Pratap; Challa, Jagat Sesh	Department of Civil Engineering	http://dspace.bits-pilani.ac.in:8080/jspui/handle/123456789/4096				Google Scholar	2011	Integrated software quality evaluation: a fuzzy multi-criteria approach	http://dspace.bits-pilani.ac.in:8080/jspui/handle/123456789/4096	Korea Science
Open Source Software Production: Climbing on the Shoulders of Giants	Open source software production is a successful new production model in which a public good is voluntarily provided. We argue that by studying this new production model we gain valuable insight for organization theory beyond software production. Under specific conditions this model can be generalized, contingent on the interplay of motivational, situational, and institutional factors. It is argued that a production model building on the shoulders of predecessors and peers depends on a well balanced portfolio of intrinsic and extrinsic motivation, low costs for contributors and governance mechanisms that do not crowd out intrinsic motivation.	Networks; open source; intrinsic and extrinsic motivation; public goods; property rights	Margit Osterloh; Sandra Rota; Bernhard Kuster		https://flosshub.org/sites/flosshub.org/files/osterlohrotakuster.pdf		34		Google Scholar	2003	Open source software production: Climbing on the shoulders of giants	https://flosshub.org/sites/flosshub.org/files/osterlohrotakuster.pdf	Institute for Research in Business Administration, University of Zurich
Software Quality Factors and Software Quality Metrics to Enhance Software Quality Assurance	Aims: Software quality assurance is a formal process for evaluating and documenting the quality of the work products during each stage of the software development lifecycle. The practice of applying software metrics to operational factors and to maintain factors is a complex task. Successful software quality assurance is highly dependent on software metrics. It needs linkage the software quality model and software metrics through quality factors in order to offer measure method for software quality assurance. The contributions of this paper build an appropriate method of Software quality metrics application in quality life cycle with software quality assurance. Design: The purpose approach defines some software metrics in the factors and discussed several software quality assurance model and some quality factors measure method. Methodology: This paper solves customer value evaluation problem are: Build a framework of combination of software quality criteria. Describes software metrics. Build Software quality metrics application in quality life cycle with software quality assurance. Results: From the appropriate method of Software quality metrics application in quality life cycle with software quality assurance, each activity in the software life cycle, there is one or more QA quality measure metrics focus on ensuring the quality of the process and the resulting product. Future research is need to extend and improve the methodology to extend metrics that have been validated on one project, using our criteria, valid measures of quality on future software project	software quality assurance; software metric; software quality factor; software life cycle	Ming-Chang Lee	British Journal of Applied Science & Technology	https://www.researchgate.net/profile/Ming-Chang-Lee-2/publication/263939913_Software_quality_factors_and_software_quality_metrics_to_enhance_software_quality_assurance_BJAST/data/00b4953c64d6e6be8b000000/Software-quality-factors-and-software-quality-metrics-to-enhance-software-quality-assurance-BJAST.pdf		27		Google Scholar	2014	Software quality factors and software quality metrics to enhance software quality assurance	https://www.researchgate.net/profile/Ming-Chang-Lee-2/publication/263939913_Software_quality_factors_and_software_quality_metrics_to_enhance_software_quality_assurance_BJAST/data/00b4953c64d6e6be8b000000/Software-quality-factors-and-software-quality-metrics-to-enhance-software-quality-assurance-BJAST.pdf	
REPUTATION IN OPEN SOURCE SOFTWARE	The 1990s and early 2000s have seen the dramatic rise of open source software, with the Linux operating system as the most salient example. This article focuses on the role of reputation in open source. It describes the importance of the reputations of hackers, software vendors, open source projects, and the open source movement. Although reputation has long been used as an explanation of hacker motivation, this article applies the concept of reputation at multiple levels, and identifies the inter-level relations. The article also emphasizes the particular importance of reputation in inter-firm competition within the open source arena, drawing on strategic management’s resource-based view of the firm to do so.		Andrew Watson		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=10b4374ade427bf9be0287c3faf21adf5fbd249c		30		Google Scholar	2005	Reputation in open source software	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=10b4374ade427bf9be0287c3faf21adf5fbd249c	Free/Open Source Research Community
Analyzing & Identifying Saas for Development of a Project by Calculating its Reputation	Assessing the quality of external software before integrating it in to the project development is very challenging now days. As IT industry is moving towards newly evolving tool named SaaS(Software as a Service) , the risk of integrating the external software to the project development has been increased . Presently integration of external software is going on, but they use the trad itional way of collecting the feedbacks to identify whether to use that external software into the project or not, which may produce an unfair results at the end of project deployment. So in this perspective we are going to propose an automated framework to rate and select a service by identifying quality and reputation .And we mainly focused on addressing the risk in proposing external software by using quality and reputation of it.	Software as a Service; Quality; Project development; Automation; risks	Bora Rama Rao.	Global Journal of Computer Science and Technology	https://core.ac.uk/download/pdf/539592632.pdf		7		Google Scholar	2012	Analyzing and Identifying SaaS for Development of a Project by calculating its Reputation	https://core.ac.uk/download/pdf/539592632.pdf	Global Journals Inc
Secured trust and reputation system : analysis of malicious behaviors and optimization	Reputation mechanisms offer a novel and effective way of ensuring the necessary level of trust which is essential to the functioning of any critical system. They collect information about the history (i.e., past transactions) of participants and make public their reputation. Prospective participants guide their decisions by considering reputation information, and thus make more informative choices. Online reputation mechanisms enjoy huge success. They are present in most e-commerce sites available today, and are seriously taken into consideration by human users. Existing reputation systems were conceived with the assumption that users will share feedback honestly. But, such systems like those in peer to peer are generally compromise of malicious users. This leads to the problem in cooperation, aggregation and evaluation. Some users want to use resources from network but do not want to contribute back to the network. Others manipulate the evaluations of trust and provide wrong estimation. We have recently seen increasing evidence that some users strategically manipulate their reports and behave maliciously. For proper protecting against those users, some kind of reputation management system is required. In some system, a trusted third entity exists and can aggregate the information. However, Peer-to-peer networks don’t have any central control or repository. Large size of distributed and hybrid networks makes the reputation management more challenging task. Hence reputation management system should perform all the tasks in distributed fashion. When these kinds of systems are implemented, peers try to deceive them to take maximum advantage. This thesis describes ways of making reputation mechanisms more trustworthy and optimized by providing defense mechanism and analysis. Different kinds of malicious behaviors exist and for each one, we present a complete analysis, simulation and a real use case example in distributed and non-distributed way 	Trust management; Reputation system; Distributed system; Game theory; Byzantine model 	Amira Bradai	Cryptography and Security 	https://theses.hal.science/tel-01127164/				Google Scholar	2014	Secured trust and reputation system: analysis of malicious behaviors and optimization	https://theses.hal.science/tel-01127164/	Institut National des Télécommunications
The Paradoxes of Free Software	This paper describes the legal structure of open source software and analyzes the likely issues to arise. A combination of copyright law and trademark law serves to permit the free distribution of open source software. The software is kept under copyright, but freely licensed under one of various open source licenses. The legal structure of open source is an elegant and robust use of intellectual property law that turns the customary use of intellectual property on its head, by using intellectual property laws, which normally are used to guard exclusive rights, to safeguard free access to and use of software. The paper further discusses how open source challenges economic and philosophical theories of intellectual property. Ironically, the open source movement, with its early roots in a decidedly socialist view of software, appears to vindicate a rather free-market view of intellectual property--that market mechanisms are more efficient in overcoming market failure than corrective legal measures. Philosophically, open sources may fit best with a natural rights/personality theory, especially where open source authors frequently give away all rights except their rights to attribution and to prevent distortion.  The paper further explores how open source may affect patent litigation, (especially with respect to the profound prior art problems in software patents) and other aspects of regulation of software (ranging from fair use in copyright to enforcement of licensing terms to restrictions on use of certain algorithms, like encryption). The openness of the software can cut both ways with respect to all those subjects. 	open source; free software; software; copyright; patent; trademark; licensing; bda	Stephen M. McJohn	George Mason Law Review	https://ssrn.com/abstract=956647		45		Google Scholar	2007	The paradoxes of free software	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=956647	SSRN
Open Source Software for the Public Administration	Open Source Software (OSS) and of Open Data Standards (ODS) are getting to be more and more accepted world wide in Public Administration (PA). A European joint project (COSPA) with the participation of eight countries studies the application advantages (and drawbacks) of OSS and ODS in PAs, as PAs are among the biggest computer- and software consumers, thus they should be very careful what to use, how to use. PAs spend every year a considerable amount of money for commercial off-the-shelf software licenses. By using appropriate technologies, such expenses might be either dramatically reduced, or re-routed to further develop local business ecosystems. This project aims at introducing, analyzing, and supporting the use of ODS and OS software for personal productivity and document management in European Pas		George L. Kovacs; Sylvester Drozdik; Paolo Zuliani; Giancarlo Succi	Workshop on Computer Science and Information Technologies CSIT’2004	https://www.researchgate.net/profile/Giancarlo-Succi/publication/228770161_Open_source_software_for_the_public_administration/links/09e4150a4226bae31e000000/Open-source-software-for-the-public-administration.pdf		8		Google Scholar	2004	Open source software for the public administration	https://www.researchgate.net/profile/Giancarlo-Succi/publication/228770161_Open_source_software_for_the_public_administration/links/09e4150a4226bae31e000000/Open-source-software-for-the-public-administration.pdf	 Institute for Contemporary Education JMSUICE
Government Preferences for Promoting Open-Source Software: A Solution in Search of a Problem	Governments around the world are making or considering efforts to promote open-source software (typically produced by cooperatives of individuals) at the expense of proprietary software (generally sold by for-profit software developers). This article examines the economic basis for these kinds of government interventions in the market. It first provides some background on the software industry. The article discusses the industrial organization and performance of the proprietary software business and describes how the open-source movement produces and distributes software. It then surveys current government proposals and initiatives to support open-source software and examines whether there is a significant market failure that would justify such intervention in the software industry. The article concludes that the software industry has performed remarkably well over the past 20 years in the absence of government intervention. There is no evidence of any significant market failures in the provision of commercial software and no evidence that the establishment of policy preferences in favor of open-source software on the part of governments would increase consumer welfare.		David S. Evans; Bernard J. Reddy	 Mich. Telecomm. & Tech. L. Rev.	https://repository.law.umich.edu/mttlr/vol9/iss2/3/				Google Scholar	2003	Government preferences for promoting open-source software: A solution in search of a problem	https://repository.law.umich.edu/mttlr/vol9/iss2/3/	Michigan Telecommunications and Technology Law Review
The Collaborative Integrity of Open-Source Software	This Article analyzes legal protection for open-source software by comparing it to the venerable civil law tradition of moral rights. The comparison focuses on the moral right of integrity, with which one may object to mutilations of her work, even after having parted with the copyright and the object that embodies the work. The parallel apparatus in open-source licensing is conditional permission to use a copyrighted work. The conditions include that source code be available and that software use be royalty free. These conditions facilitate open-source collaborative software development. At the heart of both systems is the right for creators to control the view that a work presents. In the open-source system, this is the Collaborative Integrity of open-source software. The history and legacy of moral rights help us better understand Collaborative Integrity in open-source software. The right of integrity in some international jurisdictions may apply to software, thus raising questions whether it hurts or helps open-source software. Building from these insights, this Article evaluates whether the Collaborative Integrity in open-source software deserves protection as a separate right, just as the right of integrity developed separately from pecuniary copyright in civil law jurisdictions. 	Software; foss; f/oss; open source; software development; oss; moral right; right of integrity; collaboration; programming; linux; GNU/Linux; GPL; general public license; OSD; open code; apache; SCO	Greg R. Vetter	University of Houston Law Center No. 2004-A-11	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=585921		139		Google Scholar	2004	The Collaborative Integrity of Open-Source Software	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=585921	SSRN
Software applications have on average 24 vulnerabilities inherited from buggy components		Network Security; Security; Small and Medium Business	Lucian Constantin	News	https://www.networkworld.com/article/940160/software-applications-have-on-average-24-vulnerabilities-inherited-from-buggy-components.html				Google Scholar	2015	Software applications have on average 24 vulnerabilities inherited from buggy components	https://www.networkworld.com/article/940160/software-applications-have-on-average-24-vulnerabilities-inherited-from-buggy-components.html	Net Work World
Large-scale Modeling, Analysis, and Preservation of Free and Open Source Software			Stefano Zacchiroli		https://upsilon.cc/~zack/talks/2017/2017-11-27-hdr.pdf		65		Google Scholar	2017	Large-scale Modeling, Analysis, and Preservation of Free and Open Source Software	https://upsilon.cc/~zack/talks/2017/2017-11-27-hdr.pdf	IRIF, Université Paris Diderot
 Resistance as Motivation for Innovation: Open Source Software 	"Resistance is frequently viewed as a negative aspect of human interaction. Although resistance manifests itself in numerous ways, resistance to change is frequent when individuals are introduced to new ideas or innovations. This form of resistance can limit forward progress of either an individual or an organization. However, a few papers investigated possible positive roles of resistance in human life. This paper proposes that resistance can be a positive motivator to achieve change. Open source software (OSS) is a technological innovation that is laden with aspects of resistance. One of the initial motivations for the development of open source software was ""psychological reactance"" on the part of a few software developers. Reactance is a limited part of the overall construct of resistance; specifically, resistance caused by external threats to an individual's freedom of choice, which generally manifests itself affectively. This paper looks at the role of resistance as a motivator for technological innovation from the perspective of open source softwre development. It also presents techniques for overcoming resistance to the adoption of open source software. Specific techniques presented are the Alpha and Omega strategies for overcoming resistance. Alpha strategies work by attempting to increase the approach forces towards some goal. Conversely, Omega strategies attempt to decrease the avoidance forces, thereby removing resistance to change. Both techniques are used in the context of open source software development to motivate participants."		Joseph F. Kavanagh	Communications of the Association for Information Systems	https://doi.org/10.17705/1CAIS.01336		13		Google Scholar	2004	Resistance as motivation for innovation: Open source software	https://aisel.aisnet.org/cais/vol13/iss1/36/	Communications of the Association for Information Systems
Software reliability and system reliability	"The chapter is mainly aimed at showing that, by using deliberately simple mathematics, the classical reliability theory can be extended in order to be interpreted from both hardware and software viewpoints. This being referred to as ""X-ware"". It will be shown that, even though the action mechanisms of the various classes of faults may be different from a physical viewpoint according to their causes, a single formulation can be used from the reliability modeling and statistical estimation viewpoints. Having a single formulation exhibit several advantages, both theoretical and practical, such as a) easier and more consistent modeling of hardware-software systems and of hardware-software interactions, b) adaptability of models for hardware dependability to software systems and vice versa, and c) mathematical tractabliiy. The second Section gives a general overview of the dependability concepts. The third Section is devoted to the failure behavior of an X-ware system disregarding the effect of restoration actions."		Jean-Claude Laprie; Karama Kanoun	Handbook for Software Reliability Engineering 	https://hal.science/hal-00761643/		27-69 		Google Scholar	1996	Software reliability and system reliability	https://hal.science/hal-00761643/	HAL Open Science
Open Source Software Research and Blockchain	This short paper investigates ways in which earlier open source software (OSS) research can help us explain blockchain-related phenomena. We review OSS literature and identify three such areas: 1) blockchain and OSS 2.0, 2) community development, and 3) forks.		Juho Lindman	Opportunities and Risks of Blockchain Technologies	https://acris.aalto.fi/ws/portalfiles/portal/16141610/dagrep_v007_i003_p099_s17132.pdf#page=17		3		Google Scholar	2017	3.5 Open Source Software Research and Blockchain	https://acris.aalto.fi/ws/portalfiles/portal/16141610/dagrep_v007_i003_p099_s17132.pdf#page=17	Aalto University
Predicting Software Reliability from Testing Taking into Account Other Knowledge about a Program	Inference from statistical testing is the only sound method available for estimating software reliability. However, if one ignores evidence other than testing (e.g., evidence from the track record of a developer, or from the quality of the development process), the results are going to be so conservative that they are often felt to be useless for decision-making. Bayesian inference is the main mathematical tool for taking into account such knowledge. Evidence from sources other than testing is modelled as prior probabilities (for values of the failure rate of the program) and is updated on the basis of test results to produce posterior probabilities. We explain these methods and demonstrate their use on simple examples. The measure of interest is the probability that a program satisfies a given reliability requirement, given that it has passed a certain number of tests. The procedures of Bayesian inference explicitly show the weights of prior assumptions vs. test results in determining this probability. We also demonstrate how one can model different assumptions about the fault- revealing efficacy of testing. We believe that these methods are a powerful aid for improving the quality of decision-making in matters related to software reliability.		Antonia Bertolino; Lorenzo Strigini	Quality Week'96	http://www.staff.city.ac.uk/~sm377/ls.papers/QW96BertolinoStrigini/QW96BertolinoStrigini.pdf		17		Google Scholar	1996	Predicting software reliability from testing taking into account other knowledge about a program	http://www.staff.city.ac.uk/~sm377/ls.papers/QW96BertolinoStrigini/QW96BertolinoStrigini.pdf	
Participant Satisfaction with Open Source Software	Open source software is increasingly seen as a viable alternative to conventional closed- source software. Developer/user satisfaction and involvement are common suggestions for measures of an open source project’s success; however, as yet no substantive research has been done on this topic.The purpose of this two-phase, sequential mixed methods research project will be to develop and test theory about factors that influence participant satisfaction with open source software. The central question of the proposed study is: What factors influence participant satisfaction with an open source application software project? The conceptual model for the proposed research will be based on McKeen, Guimaraes, and Wetherbe’s (1994) model of factors that influence the relationship between participa- tion in systems development and user satisfaction, modified and extended for an open source context. Previous research into the structure of open source projects has largely focused on identifying activities and roles that relate to code generation, rather than com- munity building or user support. The proposed research is intended to extend the descrip- tion of project participation to include community building and user support activities. Therefore, the first sub-question for the proposed research is: What types of contributions do participants make to open source application projects? In a typical open source project, community members have varying levels of participa- tion, ranging from passive users to core developers. This research also proposes to see if there are differences in satisfaction between users who take on different roles within projects, and the second sub-question is therefore: Do the factors that influence satisfaction with an open source application software project differ for different roles? If they do, in what way? The first qualitative phase of the proposed research will use key informant interviews combined with observation of a purposive sample of projects to gain a better understand- ing of the type and frequency of different types of participation in the projects. This phase will also confirm the preliminary model of overall factors that influence participant satis- ii faction with open source application software projects. The results of this phase will be used to develop a web-based questionnaire to gather quantitative data to test the model and associated propositions/hypotheses as phase 2. The results of this research will have both practical and theoretical implications. They will help developers of open source application software identify opportunities to increase user satisfaction, and they will help users identify ways in which they might contribute to projects. They will also contribute to the growing body of theory about open source soft- ware development and communities, as well as the literature about user satisfaction with information systems.		Brenda Lynne Chawner	Information Systems in the School of Information Management	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8773b26c0ca957da2d2d9951ff42f29173a17f6a		82		Google Scholar	2005	Participant satisfaction with open source software	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8773b26c0ca957da2d2d9951ff42f29173a17f6a	Victoria University of Wellington
Understanding open source software peer review: Review processes, parameters and statistical models, and underlying behaviours and mechanisms	Peer review is seen as an important quality assurance mechanism in both  industrial development and the open source software (OSS) community. The  techniques for performing inspections have been well studied in industry; in  OSS development, peer review practices are less well understood.  In contrast to industry, where reviews are typically assigned to specific  individuals, in OSS, changes are broadcast to hundreds of potentially  interested stakeholders. What is surprising is that this approach works very  well, despite concerns that reviews may be ignored, or that discussions will  deadlock because too many uninformed stakeholders are involved.    In this work we use a multi-case study methodology to develop a theory of OSS  peer review. There are three research stages. In the first stage, we examine the policies of 25 OSS projects to understand  the review processes used on successful OSS projects. We also select six  projects for further analysis: Apache, Subversion, Linux, FreeBSD, KDE, and  Gnome. In the second stage, using archival records from the six projects, we construct  a series of metrics that produces measures similar to those used in traditional  inspection experiments.  We measure the frequency of review, the size and  complexity of the contribution under review, the level of participation during  review, the experience and expertise of the individuals involved in the review,  the review interval, and number of issues discussed during review.  We create  statistical models of the review efficiency, review interval, and  effectiveness, the issues discussed during review, to determine which measures  have the largest impact on review efficacy. In the third stage, we use grounded theory to analyze 500 instances of peer  review and interview ten core developers across the six projects. This approach  allows us to understand why developers decide to perform reviews, what happens  when reviews are ignored, how developers interact during a review, what happens  when too many stakeholders are involved during review, and the effect of  project size on the review techniques. Our findings provide insights into the  simple, community-wide mechanisms and behaviours that developers use to  effectively manage large quantities of reviews and other development  discussions.    The primary contribution of this work is a theory of OSS peer review. We find  that OSS reviews can be described as (1) early, frequent reviews (2) of small,  independent, complete contributions (3) that, despite being asynchronously  broadcast to a large group of stakeholders, are reviewed by a small group of  self-selected experts (4) resulting in an efficient and effective peer review  technique.	OSS; Peer Review	Rigby, Peter C	Electronic Theses and Dissertations (ETD) 	http://hdl.handle.net/1828/3258				Google Scholar	2011	Understanding open source software peer review: Review processes, parameters and statistical models, and underlying behaviours and mechanisms	https://dspace.library.uvic.ca/items/85f2c7ee-88e0-4709-a977-03bb0e2447b4	University of Victoria
New Perspectives on Public Goods Production: Policy Implications of Open Source Software	For a variety of policy reasons, governments throughout the world are now adopting different legislative and administrative strategies that support the development of open source software (OSS). Some governments have actually begun to procure OSS, whereas others have channeled public funds to large-scale OSS projects. This paper summarizes the various legislative and administrative approaches taken by governments to promote OSS, and categorizes these activities into government procurement of OSS and public subsidies for OSS projects. Despite the fact that more and more public sectors have begun to migrate from proprietary software to OSS, this study reveals that governments find it difficult to legislate an explicit preference for open source software. Promoting OSS by administrative efforts, rather than by legislative action or subsidy, appears to be the most common practice among countries that have OSS policies. Moreover, this study finds that, although the “consideration” type of OSS legislation is much less controversial than the “preference” type of legislation, most of the proposed legislation is of the “preference” type. This paper argues that there are numerous factors affecting governmental policies toward OSS. Such factors may be economic, technical, political, or legal. The most fundamental argument of the study is that, in lending its support to OSS, the difference between a government user and a business user is that the government should take into account society’s long-term interests, not merely its own interests as a consumer. Although strong network effects do not necessarily imply software market failure, this study holds that other types of market failure may justify government intervention in the software market through support of OSS. OSS is preferable to proprietary software when the goal is to increase compatibility and network effects for consumers. Furthermore, in some developing countries, OSS can serve to bridge the digital divide between the “tech-haves” and the “tech have-nots” and facilitate domestic software development. Therefore, when two systems have similar suitability, this study argues that government should choose OSS over proprietary software.		Jyh-An Lee	STANFORD PROGRAM IN INTERNATIONAL LEGAL STUDIES	https://law.stanford.edu/wp-content/uploads/2015/03/LeeJyhAn2005.pdf				Google Scholar	2005	New Perspectives on Public Goods Production: Policy Implications of Open Source Software	https://law.stanford.edu/wp-content/uploads/2015/03/LeeJyhAn2005.pdf	STANFORD UNIVERSITY
DEVELOPER RESPONSIVENESS AND PERCEIVED USEFULNESS.	Perceived usefulness (PU) is a major determinant of IS use. Identifying antecedents of PU could give managers the ability to increase IS use. This research, which tests a model based on social exchange theory, suggests that PU depends on developers' responsiveness to user requests and users' trust in the software.	EXCHANGE theory (Sociology); INFORMATION resources management; MANAGEMENT; INFORMATION resources -- Use studies; SOCIAL exchange; PERCEPTION; SOCIAL interaction; INDUSTRIAL sociology; INFORMATION services; TECHNOLOGICAL innovations – Management	Gefen, David; Keil, Mark	Academy of Management Proceedings	https://doi.org/10.5465/ambpp.1996.4980832		313-317		Google Scholar	2017	DEVELOPER RESPONSIVENESS AND PERCEIVED USEFULNESS.	https://journals.aom.org/doi/abs/10.5465/ambpp.1996.4980832	Academy of Management
Is the Future of Software Development in Open Source? Proprietary vs. Open Source Software: A Cross Country Analysis	Many of them argue that the open source software is free and free from intellectual property protection clutches. This paper argues that this notion is a myth when taking into account the percentage of proprietary software usage all over the world. The governmental policies and decision to support or adopt one model or the other will have a large impact on the software industry. This study substantiates that the neutrality of government promotes innovation and development rather than supporting a particular model through a cross-country analysis of Europe, Brazil, China and India. The analysis shows that more governments are making laws and policies in support of open source softwares. 	Open Source Software (OSS); proprietary software; intellectual property rights; India; China; Brazil; Europe	Prof. K. D. Raju	Journal of Intellectual Property Rights	https://ssrn.com/abstract=985237		20		Google Scholar	2007	Is the future of software development in open source? Proprietary vs. open source software: A cross country analysis	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=985237	SSRN
Increasing Software Quality using the Provenance of Software Development Processes 	Today’s software development processes are complex. A lot of interaction occurs between developers, the tools they use, and even automatically between different tools. Examples of those interactions are entering a new requirement into the bug tracking system, committing new source code to the repository or automatic code style check during a check-in. To trace and understand the full process is hard. To get insight into these processes and to increase the quality of the resulting software release, we record information about the process during run-time. This information is called the Provenance of the process. With Provenance we can analyze and audit the software development process with the goals error detection, quality assurance, process validation, monitoring, statistical analysis, process optimization, or developer rating. For example, we can answer question such as “Which requirement causes most bugs in release version X?” or “How many commits did developer Y contribute to release Z?” The presentation describes the concepts of the Provenance data model and a software infrastructure for recording Provenance of software development processes. We show how the Provenance information helps to increase the quality of software releases and helps to give deep insight into the development process. 	Provenance; Software engineering; Software quality; software product assurance	Schreiber, Andreas	ESA Software Product Assurance Workshop 2013	https://elib.dlr.de/86228/				Google Scholar	2013	Increasing software quality using the provenance of software development processes	https://elib.dlr.de/86228/	ESA
The Rise and Evolution of the Open Source Software Foundation	Free and open source software (FOSS) project communities continue to grow and thrive. When such projects reach a critical point in their growth, corporations express in participating.Â  Corporations have more stringent robust software intellectual property (IP) management, however, and projects are not always up to the task. Neutral non-profit FOSS foundations have proved to be a solution to these problems, providing for the IP management needs of corporations while offering additional business and technical services to the project communities to encourage further growth and adoption. This article reviews how such neutral non-profit organizations have grown to meet the evolving legal, business, and technical needs of FOSS communities and businesses.		Paula Hunter; Stephen Walli	The Journal of Open Law, Technology and Society	https://www.jolts.world/index.php/jolts/article/view/64/0				Google Scholar	2013	The Rise and Evolution of the Open Source Software Foundation	https://www.jolts.world/index.php/jolts/article/view/64/0	JOLTS
Assessing Security Health of Open Source Software Packages	Open Source Software (OSS) is ubiquitous; OSS is becoming increasingly visible in organizations of varied sizes as well as by individual software developer(s). Individual software developers or groups or organizations that generate Package are generally known as upstream software sources. Conversely, the consumption of such Packages occurs in downstream projects. For instance: Android is an upstream project consumed by Qualcomm; Qualcomm further enriches android with specific hardware related drivers for downstream products made by companies like LG; finally, LG makes consumer products available for end-users. Here, an end-user is at the very end of the downstream whereas android is at the very top of upstream. Using OSS packages as components and dependencies during software development saves time and money in the software development process. Similarly, popular OSS packages receive contributions from interested software developers or organizations in the form of software contributions. Hence, a complex software supply chain exists wherever OSS packages are either developed or consumed. The existence of a software supply chain induces risk with association either directly or indirectly. The risk associated with OSS packages is inherited as part of its consumption and is multifold; This research primarily focuses on the software security risk inherited from dependencies and components for software development. This security risk can be attributed to software weaknesses and vulnerabilities in OSS components and their dependencies. Organizations, groups or individuals using OSS are interested in managing this risk, but often do not know how. In order to facilitate trustworthiness regarding decisions related to software security risk, this thesis proposes a data-driven analysis framework for OSS packages. This framework, called Advanced Package Enumeration (APE), is composed of three layers of abstraction linked to each other. In the first layer, the framework utilizes trusted databases to gather measures of a package. In the second layer, the framework generates metrics from measures. In the last layer, the framework performs a comparative analysis of packages. With the help of case studies, the use of the framework is demonstrated in determining security health of a software package.	CPE; CVE; CWE; software package; open source; weakness; software security; health; AP	Sai Uday Shankar Korlimarla	ProQuest Dissertations & Theses	https://www.proquest.com/docview/1859580572?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses		75		Google Scholar	2016	Assessing Security Health of Open Source Software Packages	https://www.proquest.com/docview/1859580572?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses	University of Nebraska at Omaha
A Public Domain Approach to Free and Open Source Software?		Software; code; FOSS; F/OSS; open source; software development; OSS; collaboration; programming; linux; GNU/Linux; GPL; general public license; OSD; open code; apache; SCO; viral code; patent; license; software license; implied license; copyleft; injunction	Greg R. Vetter	Ohio State Law Journal Furthermore, Forthcoming	https://dx.doi.org/10.2139/ssrn.2326852		10		Google Scholar	2013	A Public Domain Approach to Free and Open Source Software	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326852	SSRN
OPEN SOURCE – NEW RULES IN SOFTWARE DEVELOPMENT	Open source is a term for software published under licence that does not give any property rights to the developers. Recently, attempts have been made to explain open source as the result of the collaboration of purely self-interested, utility-maximizing individuals. This approach is at least partially misguided. Open source can only be understood if it is analyzed as a community based on norms and trust. We sketch how trust in open source projects can be conceptionalized and refer to the concept of swift trust. Swift trust is based on an intrinsically motivated observance of norms and thus helps to solve the social dilemma of open source code development. We go on to show that, under certain conditions, even mainly extrinsically motivated actors can be forced to adhere to these norms of cooperation, thus solving the second order social dilemma of norm enforcement. Commercial firms that want to use the innovatory powers of open source for their own purposes have to respect and adhere to these norm- and trust-based foundations of cooperation.		MARGIT OSTERLOH; SANDRA ROTA; MARC VON WARTBURG		https://www.researchgate.net/profile/Margit-Osterloh/publication/228589925_Open_source-New_rules_in_software_development/links/543faa640cf2fd72f99c839c/Open-source-New-rules-in-software-development.pdf		23		Google Scholar	2001	Open source-New rules in software development	https://www.researchgate.net/profile/Margit-Osterloh/publication/228589925_Open_source-New_rules_in_software_development/links/543faa640cf2fd72f99c839c/Open-source-New-rules-in-software-development.pdf	Institute for Research in Business Administration.
Practical approach to automate the discovery and eradication of open-source software vulnerabilities at scale			Aladdin Almubayed	black hat USA 2019	https://i.blackhat.com/USA-19/Thursday/us-19-Almubayed-Practical-Approach-To-Automate-The-Discovery-And-Eradication-Of-Open-Source-Software-Vulnerabilities-At-Scale.pdf		88		Google Scholar	2019	Practical approach to automate the discovery and eradication of open-source software vulnerabilities at scale	https://i.blackhat.com/USA-19/Thursday/us-19-Almubayed-Practical-Approach-To-Automate-The-Discovery-And-Eradication-Of-Open-Source-Software-Vulnerabilities-At-Scale.pdf	Netflix
Waiting for Usable Open Source Software? Don't Hold Your Breath!	There is a general consensus about the lack of usability in most open source software (OSS). Academics and practitioners have offered several suggestions to improve the usability of such software. However, a realistic assessment of OSS projects, specifically the motivations of OSS developers and their attitude toward software usability, lack of user feedback, and absence of usability experts in OSS projects, leads to the conclusion that strategies to improve OSS usability are unlikely to succeed anytime soon. The only exceptions will be OSS which enjoy sufficient financial support from individuals and organizations, and software that were developed by commercial software producers and later released under an open source license.	software usability; OSS; FLOSS; usability	Ravi Sen	Communications of the Association for Information Systems	https://doi.org/10.17705/1CAIS.02025		20		Google Scholar	2007	Waiting for usable open source software? Don't hold your breath!	https://aisel.aisnet.org/cgi/viewcontent.cgi?article=2608&context=cais	Association for Information Systems
Vulnerability Likelihood: A Probabilistic Approach to Software Assurance	The importance of software security is undeniable given the impact of software on our lives. Assurance about the security properties of a software artifact should ultimately translate into a quantitative measure of vulnerabilities. In this paper, we present the idea of vulnerability likelihood as a probabilistic approach to software assurance. Gaining assurance early in the software development cycle is of immense value in directing future efforts. So we first discuss vulnerability likelihood in the context of vulnerability prediction in software artifacts. We propose four types of program properties that can be observed in software artifacts to potentially determine their vulnerability likelihood. Then we discuss vulnerability likelihood in the context of vulnerability detection. We propose a technique to quantify the assurance in the solutions of checkers for vulnerability detection that use static analysis. And finally, we illustrate the importance of vulnerability likelihood in a software development methodology to measurably increase software assurance.		Rajeev Gopalakrishna; Eugene H. Spafford; Jan Vitek	CERIAS Tech Report	https://www.cerias.purdue.edu/apps/reports_and_papers/view/2782		10		Google Scholar	2006	Vulnerability likelihood: A probabilistic approach to software assurance	https://www.cerias.purdue.edu/apps/reports_and_papers/view/2782	CERIAS, Purdue University
Exploring the Effects of Process Characteristics on Product Quality in Open Source Software Development	There has been considerable discussion on the possible impacts of open source software development practices, especially in regard to the quality of the resulting software product. Recent studies have shown that analyzing data from source code repositories is an efficient way to gather information about project characteristics and programmers, showing that OSS projects are very heterogeneous in their team structures and software processes. However, one problem is that the resulting process metrics measuring attributes of the development process and of the development environment do not give any hints about the quality, complexity, or structure of the resulting software. Therefore, we expanded the analysis by calculating several product metrics, most of them specifically tailored to object-oriented software. We then analyzed the relationship between these product metrics and process metrics derived from a CVS repository. The aim was to establish whether different variants of open source development processes have a significant impact on the resulting software products. In particular we analyzed the impact on quality and design associated with the numbers of contributors and the amount of their work, using the GINI coefficient as a measure of inequality within the developer group.		Stefan Koch; Christian Neumann	Principle Advancements in Database Management Technologies: New Applications and Frameworks	http://dx.doi.org/10.4018/978-1-60566-904-5.ch006		28		Google Scholar	2010	Exploring the Effects of Process Characteristics on Products Quality in Open Source Software Development	https://www.igi-global.com/gateway/chapter/39353	IGI Global
Software quality attributes and trade-offs			Patrik Berander; Lars-Ola Damm; Jeanette Eriksson; Tony Gorschek; Kennet Henningsson; Per Jonsson; Simon Kagstrom; Drazen Milicic; Frans Martensson; Kari Ronkko; Piotr Tomaszewski		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=57d7ef7a35d480e2ebd41f66ece451c4d7a7a40a		100		Google Scholar	2005	Software quality attributes and trade-offs	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=57d7ef7a35d480e2ebd41f66ece451c4d7a7a40a	Blekinge Institute of Technology
Attack Patterns as a Knowledge Resource for Building Secure Software	Building software with an adequate level of security assurance for its mission becomes more and more challenging every day as the size, complexity, and tempo of software creation increases and the number and the skill level of attackers continues to grow. These factors each exacerbate the issue that, to build secure software, builders must ensure that they have protected every relevant potential vulnerability; yet, to attack software, attackers often have to find and exploit only a single exposed vulnerability. To identify and mitigate relevant vulnerabilities in software, the development community needs more than just good software engineering and analytical practices, a solid grasp of software security features, and a powerful set of tools. All of these things are necessary but not sufficient. To be effective, the community needs to think outside of the box and to have a firm grasp of the attacker’s perspective and the approaches used to exploit software.		Sean Barnum; Amit Sethi	OMG Software Assurance Workshop	http://www.orkspace.net/secdocs/Conferences/BlackHat/Federal/2007/Attack%20Patterns%20-%20Knowing%20Your%20Enemies%20in%20Order%20to%20Defeat%20Them-paper.pdf		31		Google Scholar	2007	Attack patterns as a knowledge resource for building secure software	http://www.orkspace.net/secdocs/Conferences/BlackHat/Federal/2007/Attack%20Patterns%20-%20Knowing%20Your%20Enemies%20in%20Order%20to%20Defeat%20Them-paper.pdf	CIGITAL, INC
Improvement of Open Source Software Usability: An Empirical Evaluation from Developers′ Perspective	User satisfaction has always been important for software success whether it is Open Source Software (OSS) or closed proprietary software. Even though we do not presume that OSS always has poor usability, as there are examples of good usable open source software, it would still be agreed that OSS usability has room for further improvement. This paper presents an empirical investigation to study the impact of some key factors on OSS usability from developers′ points of view. This is one of the series of four studies that we are conducting regarding improvement of OSS usability from OSS developers, users, contributors, and industry perspectives. The research model of this empirical investigation studies and establishes the relationship between the key usability factors from developers′ perspective and OSS usability. A data set of 106 OSS developers from 18 open source projects of varied size has been used to study the research model. The results of this study provide empirical evidence that the studied key factors play a significant role in improving OSS usability.		Arif Raza; Luiz F. Capretz; Faheem Ahmed	Advances in Software Engineering	https://doi.org/10.1155/2010/517532				Google Scholar	2010	Improvement of open source software usability: an empirical evaluation from developers' perspective	https://onlinelibrary.wiley.com/doi/full/10.1155/2010/517532	Wiley Online Library
Assessing Free/Open Source Software Quality	According to its proponents, one of the most acclaimed advantages of Free/Open Source Software (F/OSS) is its superior quality. However, this suggestion is an open issue, since there is little concrete evidence to justify whether F/OSS quality is indeed better or worse than that of proprietary software products. The general perspective of this article is to discuss the current status of F/OSS quality and to assess its performance in various aspects of quality, based on existing literature. Specifically, this article will provide some answers to various questions raised by the assertion concerning the quality of F/OSS. In this regard issues addressed in this article include the quality framework, through which F/OSS quality should be investigated and the performance of F/OSS in various quality factors within this quality framework. Answers to these issues are given by providing evidence from various research papers, empirical studies and reports based on experience about the quality of F/OSS products. The overall results seem to indicate that F/OSS has achieved an acceptable level of quality, although there is more to be done in order to outperform proprietary software.	Free Software; Open Source Software; Software Quality	Ioannis Samoladas; Ioannis Stamelos		https://d1wqtxts1xzle7.cloudfront.net/41833459/Assessing_freeopen_source_software_quali20160131-16550-1t1tkw1-libre.pdf?1454309508=&response-content-disposition=inline%3B+filename%3DAssessing_free_open_source_software_qual.pdf&Expires=1733484615&Signature=N8zWrS6oLb1wM2BkUYkk1NEs3bMZAqUDJqe4xS~crpZGznAGKHXEqEgYDxRNDyamqnuhiu~LwDC4aH-LZeNBqgnLE46dlo1lBKjo3rOm8Jkb0JwftoV66cXMb-dg8P9unI4ZDx-ywiZOgoR0c3QJuo3~6gcE-LdtdF8~46PRzdAvXRECPYyTCBS9M5EMnvqFiQ5Sqjv~23yEPa1BdSOErEBMFA00J3bZrArA7sqLgxkBVq14AahNqgWOXg7QOH6QOgxInM-I-xcF6nAPi94xPsOOmF8zx2gTUDMXkIK2WbYXda9yMLqQmj9LEODgT2Cpy2sFNjz7NgFGrBgRZSET7A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA		36		Google Scholar	2003	Assessing free/open source software quality	https://d1wqtxts1xzle7.cloudfront.net/41833459/Assessing_freeopen_source_software_quali20160131-16550-1t1tkw1-libre.pdf?1454309508=&response-content-disposition=inline%3B+filename%3DAssessing_free_open_source_software_qual.pdf&Expires=1733484615&Signature=N8zWrS6oLb1wM2BkUYkk1NEs3bMZAqUDJqe4xS~crpZGznAGKHXEqEgYDxRNDyamqnuhiu~LwDC4aH-LZeNBqgnLE46dlo1lBKjo3rOm8Jkb0JwftoV66cXMb-dg8P9unI4ZDx-ywiZOgoR0c3QJuo3~6gcE-LdtdF8~46PRzdAvXRECPYyTCBS9M5EMnvqFiQ5Sqjv~23yEPa1BdSOErEBMFA00J3bZrArA7sqLgxkBVq14AahNqgWOXg7QOH6QOgxInM-I-xcF6nAPi94xPsOOmF8zx2gTUDMXkIK2WbYXda9yMLqQmj9LEODgT2Cpy2sFNjz7NgFGrBgRZSET7A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA	University of Informatics Greece
Software Vulnerability Markets: Discoverers and Buyers	Some of the key aspects of vulnerability—discovery, dissemination, and disclosure—have received some attention recently. However, the role of interaction among the vulnerability discoverers and vulnerability acquirers has not yet been adequately addressed. Our study suggests that a major percentage of discoverers, a majority in some cases, are unaffiliated with the software developers and thus are free to disseminate the vulnerabilities they discover in any way they like. As a result, multiple vulnerability markets have emerged. In some of these markets, the exchange is regulated, but in others, there is little or no regulation. In recent vulnerability discovery literature, the vulnerability discoverers have remained anonymous individuals. Although there has been an attempt to model the level of their efforts, information regarding their identities, modes of operation, and what they are doing with the discovered vulnerabilities has not been explored. Reports of buying and selling of the vulnerabilities are now appearing in the press; however, the existence of such markets requires validation, and the natures of the markets need to be analyzed. To address this need, we have attempted to collect detailed information. We have identified the most prolific vulnerability discoverers throughout the past decade and examined their motivation and methods. A large percentage of these discoverers are located in Eastern and Western Europe and in the Far East. We have contacted several of them in order to collect firsthand information regarding their techniques, motivations, and involvement in the vulnerability markets. We examine why many of the discoverers appear to retire after a highly successful vulnerability-finding career. The paper identifies the actual vulnerability markets, rather than the hypothetical ideal markets that are often examined. The emergence of worldwide government agencies as vulnerability buyers has significant implications. We discuss potential factors that can impact the risk to society and the need for detailed exploration.	Risk management; software security; vulnerability discoverers; vulnerability markets	Abdullah M. Algarni; Yashwant K. Malaiya	International Journal of Computer and Information Engineering	https://d1wqtxts1xzle7.cloudfront.net/68167572/Software_Vulnerability_Markets_Discovere20210717-16515-1jmvhgu.pdf?1626564659=&response-content-disposition=inline%3B+filename%3DSoftware_Vulnerability_Markets_Discovere.pdf&Expires=1733484747&Signature=e11PLz~4tjF37AhTVnuiur11yvd2JOAS5b7jHwgKzp5ud8prvWYi~HdNqcOyYmoYzulX~RURQRr1WgSIyPYVxogWCJgo76uYZOe3QR9pdAHCUDctL4JDKsKHOOd6BmEYg2z9e5LtVckCDOXGi0iuUHHlvyBlaaHhj9RiOyuURxwXKhm2xDqyEcAdD4787hBW6Zm4btfr2N0XAkWw43tpFwSXOTW67GI3SycGBqiMffemKUVzmhSZC2OUgkA5mrvnKkUAdtWcJ19voT6-tjiwmJCS8cMvBmTHhg7~qfvlh5yYteIQRkz6~nrNtY55IetjGoWO-pBR0ycQAufjNN7jsQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA		11		Google Scholar	2014	Software vulnerability markets: Discoverers and buyers	https://d1wqtxts1xzle7.cloudfront.net/68167572/Software_Vulnerability_Markets_Discovere20210717-16515-1jmvhgu.pdf?1626564659=&response-content-disposition=inline%3B+filename%3DSoftware_Vulnerability_Markets_Discovere.pdf&Expires=1733484747&Signature=e11PLz~4tjF37AhTVnuiur11yvd2JOAS5b7jHwgKzp5ud8prvWYi~HdNqcOyYmoYzulX~RURQRr1WgSIyPYVxogWCJgo76uYZOe3QR9pdAHCUDctL4JDKsKHOOd6BmEYg2z9e5LtVckCDOXGi0iuUHHlvyBlaaHhj9RiOyuURxwXKhm2xDqyEcAdD4787hBW6Zm4btfr2N0XAkWw43tpFwSXOTW67GI3SycGBqiMffemKUVzmhSZC2OUgkA5mrvnKkUAdtWcJ19voT6-tjiwmJCS8cMvBmTHhg7~qfvlh5yYteIQRkz6~nrNtY55IetjGoWO-pBR0ycQAufjNN7jsQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA	World Academy of Science, Engineering and Technology
Quality of EIS support and maintenance services	Company X delivers integrated information systems for enterprises operating in various business areas. In addition to planning, implementing and deploying these systems, the company provides continuous support and maintenance services to its customers. However these services have not been systematically developed and quality of service can vary greatly.  The purpose of this thesis was to identify the problem areas causing low customer satisfaction and operational inefficiency. The objective was to understand how the identified problems are related to each other and present some ideas on how service quality could be improved.  Customers and Company X employees were interviewed and results were analyzed and compared to theoretical frameworks. Service quality gap analysis indicated that the service delivery processes were insufficient and lacking the support of top management. This caused the customers to perceive the service as unreliable and unresponsive even if the employees were knowledgeable and service oriented.  Recent indications of poor service quality and the identified potential for growth has increased the interest of the company’s top management. Changes are already being done, but the effectiveness of these changes and improvements on operational efficiency, service quality, and customer satisfaction need to be monitored and regularly analyzed. Similar activities as presented in this research can be performed to continually improve the services and a long-term focus on customer service orientation is needed to stay competitive.		Laakso, Tiina	Tampereen ammattikorkeakoulu	https://urn.fi/URN:NBN:fi:amk-2015072913876				Google Scholar	2015	Quality of EIS support and maintenance services	https://www.theseus.fi/handle/10024/97207	Theseus
Future Trends and Development Methods in Software Quality Assurance	The main purpose of this thesis is to study the future trends and development methods in software quality assurance from a testing point of view. The main scope of this thesis is to study what views and expectations customers and personnel of Comiq Ltd have about the future trends and development methods in software quality assurance, how those trends will impact the development methods in the future and what should be the focus areas of Comiq in the future to update and maintain employee’s high competence levels.  The background studies of the thesis focus on current predictions in relation to agile and DevOps principles as well as test automation. Agile and DevOps are the main trends that are commonly used in software development and quality assurance is an essential part of both agile and DevOps.  The research method was a quantitative data analysis. The sample data was collected in June 2017 by conducting an online questionnaire that was addressed to Comiq customers and personnel. A total of 93 people participated. The questionnaire had a total of 15 questions relating to the properties of quality assurance experts, test automation and methods of importance in achieving future quality goals.  Based on the study results, the most important properties of quality assurance experts are understanding the customer’s needs, communications skills, and continuous development of skills and understanding the business area. Test automation is currently seen as one of the most important factors in improving the quality of software. Overall quality can be assured by automating all necessary functionalities including regression testing. Test environments should be automated and version control and functional requirement management should continuously support testing.  The biggest obstacles seen by the participants in achieving the overall target state of quality assurance are lack of test automation, money, time, recourses, and current software development methods and efficient ways of working.  Test automation is an area that will continue to grow and it should be invested in more in the future. Results show that customers and personnel are aware of current trends and want to improve overall quality assurance by growing maturity of agile and DevOps and by involving testing as early as possible.		Williams, Paivi	Haaga-Helia ammattikorkeakoulu	https://urn.fi/URN:NBN:fi:amk-2017112217768				Google Scholar	2017	Future Trends and Development Methods in Software Quality Assurance	https://www.theseus.fi/handle/10024/135804	Theseus
A Case Study in Open Source Patch Submission	Although open source and the related processes have become an inherent part of the computer industry, companies and other contributors are often reluctant when it comes to active involvement and participation. The reasons are manifold. Strong ones are that most open source projects are self-governing, without a fixed road map or schedule, each which its own development process. This seems to make them unreliable and unpredictable, because personal intentions cannot be enforced and rely on the open source community in charge of leading the projects. Within an exemplary case study, a new feature spawning multiple layers of a GNU/Linux based operating system, and thus different open source projects, will be implemented and submitted for inclusion. All this bounded by a predefined, fixed time frame, which might be exemplary for schedule and budget driven company structures. The underlying development processes of the Linux kernel, a middle ware project called UDisks and the GNOME desktop will be considered and acted upon accordingly. As a whole, the feature submission failed, because it was not possible to include all the required changes in all target projects in time. The failure has two main reasons: First, it is caused by technical problems which could occur in every software project, and thus not related to open source processes in particular. Second, and more relevant for this research, delays occurred due to common obstacles one has to face in the individual open source development processes. Individuals cannot put as much pressure as they like on the projects and are kind of at their mercy. As an overall outcome, trying to include a new feature in different open source projects depending on each other is possible, however, unpredictable to a certain extent. The open source community has its own rules and processes, companies or other contributors cannot rely on being able to influence in whatever way they want. The advancements heavily depend on the relevant community and project members and thus the process involved. Conclusion: When submitting patches, always expect another iteration.		Holger Macht		https://homac.github.io/publications/A_Case_Study_in_Open_Source_Patch_Submission.pdf		70		Google Scholar	2012	A Case Study in Open Source Patch Submission	https://homac.github.io/publications/A_Case_Study_in_Open_Source_Patch_Submission.pdf	Friedrich-Alexander University Erlangen-Nuremberg
Free and open source cloud technology based on the type Software as a Service	The aim of this study is to design and implement the cloud computing technology with types of the services Software as a Service (SaaS). Objectives of the study is to analyze the free and open source cloud technology based on the type Software as a Service. The object of research is the process of using of the cloud computing technology with types of the services Software as a Service in education. The subject of research is the use of free cloud computing technology with types of the services Software as a Service in education for students of university. This article covers the free and open source cloud computing technology and its application in Web-based IT education. One of the technologies that can be used in teaching online IT courses is cloud computing. In this work the analysis and systematization of research on the use of free cloud-based ICT in education, research and organizational activities. Results of the study is planned to use for the design of methods e-learning education with the free and open source cloud technology.	loud computing; learning ICT; cloud-based environment; Software as a Service (SaaS)	Nataliya Mihailivna Kiyanovska		https://lib.iitta.gov.ua/id/eprint/716591/1/052-058_Kiyanovska.pdf		7		Google Scholar	2019	Free and open source cloud technology based on the type Software as a Service	https://lib.iitta.gov.ua/id/eprint/716591/1/052-058_Kiyanovska.pdf	
centralized package management using Stork	Managing the software installed on multiple systems can be one of the duller aspects of system administration. One has to deal with varied sets of packages, each replicated on numerous machines, and bring up new systems, with the complica- tion of those that are almost like the others, but not quite. In many cases, great amounts of time could be saved and more than a few mistakes avoided by using tools specifically created to make this job easier.		Justin Samuel; Jeremy Plichta; Justin Cappos	;LOGIN:	https://www.usenix.org/system/files/login/issues/login_feb_2008.pdf#page=27		27-31		Google Scholar	2008	centralized package management using Stork	https://www.usenix.org/system/files/login/issues/login_feb_2008.pdf#page=27	USENIX
Bootstrapping Trust in a “Trusted” Platform	For the last few years, many commodity computers have come equipped with a Trusted Platform Module (TPM). Ex- isting research shows that the TPM can be used to establish trust in the software executing on a computer. However, at present, there is no standard mechanism for establish- ing trust in the TPM on a particular machine. Indeed, any straightforward approach falls victim to a cuckoo attack. In this work, we propose a formal model for establishing trust in a platform. The model reveals the cuckoo attack problem and suggests potential solutions. Unfortunately, no instan- tiation of these solutions is fully satisfying, and hence, we pose the development of a fully satisfactory solution as an open question to the community.		Bryan Parno	HotSec	https://www.usenix.org/legacy/events/hotsec08/tech/full_papers/parno/parno.pdf		6		Google Scholar	2008	"Bootstrapping Trust in a"" Trusted"" Platform."	https://www.usenix.org/legacy/events/hotsec08/tech/full_papers/parno/parno.pdf	USENIX
Quality and the Reliance on Individuals in Free Software Projects	It has been suggested that the superior quality of many Free Software projects in comparison to their proprietary counterparts is in part due to the Free Software commu- nity’s extensive source code peer-review process. While many argue that software is best developed by individuals or small teams, the process of debugging is highly paral- lizable. This “one and many” model describes a template employed by many Free Software projects. However, re- liance on a single developer or maintainer creates a sin- gle point of failure that raises a number of serious quality and reliability concerns – especially when considered in the context of the volunteer-based nature of most Free Software projects. This paper will investigate the nature of problems raised by this model within the Debian Project and will ex- plore several possible strategies aimed at removing or de- emphasizing the reliance on individual developers.		Martin Michlmayr; Benjamin Mako Hill	Proceedings of the 3rd Workshop on Open Source Software Engineering	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c5c8a50f8a80074ff2d795b994182eb4e4c8653f#page=105		105-109		Google Scholar	2003	Quality and the reliance on individuals in free software projects	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c5c8a50f8a80074ff2d795b994182eb4e4c8653f#page=105	Citeseer
Issues in Global Software Development:  A Critical Review	A number of companies have employed Global Software Development (GSD) methodology as a useful tool for their  software development practices. GSD is a contractual relationship between client and vendor organizations in which a  client outsources all or some part of its software development activities to a vendor. The vendor in return provides the  agreed services in lieu of certain amount of remuneration. The main reasons to select the GSD technique include re- duc ed co st, faster development and access to skilled manpower. Though GSD is emerging as an effective technique, but  it suffers from many challenges like poor communication, lack of trust and coordination. These challenges pose serious  risk to the smooth execution of the GSD projects. In this paper, we present a comparative study on GSD to highlight its  merits and demerits. Our findings reveal that much of the research in this area has been focused on addressing issues  faced by client organizations, however, vendor side in the GSD relationship is much ignored due to which this area is  still immature; and, hence, further research work is required to be undertaken to address the issues faced by the vendor  organizations.	Communication; Co-Ordination and Trust; Global Software Development (GSD); Distributed Software; Development (DSD)	Sami ul Haq; Mushtaq Raza; Asraf Zia; M. Naeem Ahm	J. Software Engineering & Applic	https://doi.org/10.4236/jsea.2011.4100		590-595		Google Scholar	2011	Issues in global software development: A critical review	https://www.scirp.org/html/7765.html	Scientific Research Open Access
Semiformal process model for technology transfer	The author describes a semiformal process model for technology transfer at Ferranti Computer Systems. Experience has shown that satisfactory transition takes place only when staff members move from one group to the next either with the technology or with the problem.		G. Glynn	Proceedings. 12th International Conference on Software Engineering	https://doi.org/10.1109/ICSE.1990.63643		334-335		Google Scholar	1990	Semiformal process model for technology transfer	https://www.computer.org/csdl/proceedings-article/icse/1990/00063643/12OmNx8OuDD	IEEE
 QAM: proposed model for quality assurance in CBSS 	Component-based software engineering (CBSE) / Component-Based Development (CBD) lays emphasis on decomposition of the engineered systems into functional or logical components with well-defined interfaces used for communication across the components. Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is much different from the traditional approach, quality assurance for component-based software development is a new topic in the software engineering research community. Because component-based software systems are developed on an underlying process different from that of the traditional software, their quality assurance model should address both the process of components and the process of the overall system. Quality assurance for component-based software systems during the life cycle is used to analyze the components for achievement of high quality component-based software systems. Although some Quality assurance techniques and component based approach to software engineering have been studied, there is still no clear and well-defined standard or guidelines for component-based software systems. Therefore, identification of the quality assurance characteristics, quality assurance models, quality assurance tools and quality assurance metrics, are under urgent need. As a major contribution in this paper, I have proposed QAM: Quality Assurance Model for component-based software development, which covers component requirement analysis, component development, component certification, component architecture design, integration, testing, and maintenance.	component-based software systems CBSS; QAM; software architecture; components; quality assurance 	Kharb L	Advances in Science and Technology. Research Journal	http://dx.doi.org/10.12913/22998624/59084		50--57		Google Scholar	2015	QAM: PROPOSED MODEL FOR QUALITY ASSURANCE IN CBSS	https://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-41ec3548-403a-4c8d-804b-e90fd67204c5	BazTech
Open model methodology: a new approach to the development of user interfaces based on knowledge processing	In this paper, we propose a new interface development methodology called the open model methodology, based on the open model of user interfaces which divides the interactive system into six layers: I/O Media, Concept, Semantic/Syntax, Domain, Mode/Style and Computation. Knowledge is used as the basis for developing and maintaining interfaces. The interface development is divided into three parallel streams: BEHAVIOURS, MEAN and DO. Each of these streams maintains a mini-refinement-loop consisting of two steps: knowledge acquisition and testing. Using this approach, we have designed a prototype interface called KZ3. The experimental results indicate that the methodology has a bright future.		Xing Du; Xie Li	Software Engineering Journal	https://doi.org/10.1049/sej.1992.0024				Google Scholar	1992	OPEN MODEL METHODOLOGY - A NEW APPROACH TO THE DEVELOPMENT OF USER INTERFACES BASED ON KNOWLEDGE PROCESSING	https://digital-library.theiet.org/doi/10.1049/sej.1992.0024	The Institution of Engineering and Technology
Modelling the critical success factors of agile software development projects in South Africa	Background: The continued in failure of agile and traditional software development projects have led to the consideration, attention and dispute to critical success factors that are the aspects which are most vital to make a software engineering methodology fruitful. Although there is an increasing variety of critical success factors and methodologies, the conceptual frameworks which have causal relationship are limited. Objective: The objective of this study was to identify and provide insights into the critical success factors that influence the success of software development projects using agile methodologies in South Africa. Method: Quantitative method of collecting data was used. Data were collected in South Africa through a Web-based survey using structured questionnaires. Results: These results show that organisational factors have a great influence on performance expectancy characteristics. Conclusion: The results of this study discovered a comprehensive model that could provide guidelines to the agile community and to the agile professionals. 		Tawanda B. Chiyangwa; Ernest Mnkandla	South African Journal of Information Management	https://hdl.handle.net/10520/EJC-cd6fb1ea0				Google Scholar	2017	Modelling the critical success factors of agile software development projects in South Africa	https://journals.co.za/doi/abs/10.4102/sajim.v19i1.838	Sabinet African Journals
Modeling Platform Ecosystems	Platforms facilitate the creation of complementary modulesby third parties and act as intermediaries between different groups of ac-tors. Due to the high degree of collaboration between actors, ecosystemsevolve around such platforms. As a result of digitalization, platform busi-ness models are becoming viable in more and more domains. Despite theincreasing variety of different platform ecosystems, means to clearly spec-ify them are scarce. This conceptual ambiguity impedes comparabilityof research and knowledge accumulation. To solve this problem, we pro-pose a domain-specific platform ecosystems modeling language (PEML)which builds on seminal platform ecosystem literature. We demonstratePEML by modeling two real-life platform ecosystems based on onlinecase studies. Our results support researchers and practitioners alike inclearly specifying platform ecosystems.		Pauli, Tobias; Marx, Emanuel; Dunzer, Sebastian; Matzner, Martin	CEUR Workshop Proceedings	https://open.fau.de/handle/openfau/15055		17 - 30		Google Scholar	2020	Modeling platform ecosystems	https://open.fau.de/handle/openfau/15055	Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)
Towards automatic graphical concrete syntax generation for domain specific modeling langages	Model Driven Engineering (MDE) is an approach for designing complex systems based on, creating, manipulating and analyzing various models. Multiple Domain Specific Modeling Languages (DSMLs) are used for the creating of these models. It is then necessary to define the graphical representation of these languages. This paper describes concepts and mechanisms allowing to guide and to assist an expert from any engineering domain to define and formalize the concrete syntax of a graphical DSML considered as relevant in this domain. We define multiple classifications of the abstract syntax elements based both on the abstract syntax and on the concrete syntax. Grounded on those classifications, we present how a part of the concrete syntax can be generated automatically from an abstract syntax by a graphical role election.		Nastov, Blazo; Pfister, François	Ingénierie des Systèmes d'Information	https://doi.org/10.3166/ISI.20.2.67-91				Google Scholar	2015	Towards automatic graphical concrete syntax generation for domain specific modeling languages [Vers la génération des syntaxes concrétes graphiques pour les langages de modélisation métier]	https://openurl.ebsco.com/EPDB%3Agcd%3A5%3A8347157/detailv2?sid=ebsco%3Aplink%3Ascholar&id=ebsco%3Agcd%3A103744993&crl=c&link_origin=scholar.google.ca	EBSCO
A Macro System with Class Objects for the Java Language	This paper presents OpenJava, which is a macro system that we have developed for Java. With traditional macro systems designed for non object-oriented languages, it is difficult to write a number of macros typical in object-oriented programming since they require the ability to access a logical structure of programs. One of the drawbacks of traditional macro systems is that abstract syntax trees are used for rep- resenting source programs. This paper first points out this problem and then shows how OpenJava addresses this problem. A key idea of Open- Java is to use metaobjects, which was originally developed for reflective computing, for representing source programs.				https://www.researchgate.net/publication/242142350_A_Macro_System_with_Class_Objects_for_the_Java_Language						A macro system with class objects for the Java language		
An Object-Oriented Analysis and Design of Management Functions for Distributed Access Node Systems	Object-oriented technology promotes a better understanding of requirements and results in more modifiable and maintainable applications providing other benefits such as reusability, extensibility, robustness, reliability and scalability. In this paper, we present the object-oriented application framework for efficient system analysis and design. The software framework we presented comprises the application framework classes and the design pattern as a framework of the software architecture. The application framework classes provide a set of application foundation class that can support easy design and implementation of the application software. The design pattern can be used as the architectural framework by providing unified design concept and principle. In this paper, we present the application framework classes for DANS (Distributed Access Node System) management and describe the system analysis and design based on the framework classes. And we also propose the Actor-Reactor pattern to provide the efficient and reusable software architecture for the distributed and multithreaded environment.		 JinOh Kim; IkKyun Kim; WangHwan Lee; HyupJong Kim	 Korean Institute of Information Scientists and Engineers Journal of KISS(A): Computer Systems and Theory 	https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE00600809		10		GoogleScholar	1998	An object-oriented analysis and design of management functions for Distributed Access Node Systems	https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE00600809	DBpia
Aspect-Oriented Programming to Improve Modularity of Object-Oriented Applications	The separation of concerns design principle improves software reutilization, understandability, extensibility and maintainability. By using the objectoriented paradigm, it is not always possible to separate into independent modules the different concerns of an application. The result is that the source code of crosscutting concerns are tangled and scattered across the whole application. Aspect-oriented programming offers a higher level of modularity, providing a solution for the code tangling and scattering problem. To show how aspectoriented programming can be used as a suitable mechanism to improve the modularity of object-oriented applications, this divulgative article presents the implementation of a typical design pattern following both the object- and aspectoriented paradigms. The two approaches are compared from the modularity perspective, establishing a discussion on the benefits provided and is current use.	aspect-oriented programming; modularity; crosscutting concerns; AspectJ; separation of concerns	Jose M. Felix; Francisco Ortin	Journal of Software	https://www.jsoftware.us/show-84-1233-1.html		2454-2460 		Google	2014	Aspect-oriented Programming to Improve Modularity of Object-oriented Applications	https://www.jsoftware.us/show-84-1233-1.html	Journal of Sotware
Aspect-Oriented Requirements Engineering for Advanced Separation of Concerns: A Review	Software engineering was introduced to cope with software crisis with two fundamental principles: separation of concerns and modularity. Many programming paradigms have been proposed and used while considering the fundamental principles from the early days. Complex software systems were successfully modularized but complete separation of concerns is still impossible to achieve using today’s most popular programming paradigms such as object-oriented programming. There are some concerns which could not be separated properly in a single class or module due to their highly coupling with other classes or modules’ behaviors. We call such unmodularized concerns as crosscutting concerns and these are responsible for scattering and tangling. Aspects are the natural evolution of the object-oriented paradigm. They provide a solution to some difficulties encountered with object-oriented programming, sometimes scattering and tangling. Hence, Aspect-Oriented Software Development (AOSD) is another step towards achieving improved modularity during software development. It gives emphasis to the separation of crosscutting concerns i.e. advanced separation of concerns and encapsulation of crosscutting concerns in separate modules, known as aspects. It later uses composition mechanism to weave them with other core modules at loading time, compilation time, or run-time. Aspect-Oriented Requirements Engineering (AORE) is an early phase in AOSD that supports separation of crosscutting concerns at requirements level. It does not replace but rather complements any of the existing requirements methodologies. Over the last few years, several research efforts have been devoted to this area. Several techniques for crosscutting concern identification have already been proposed. In this paper, an attempt is made to review the existing approaches and understand their contribution to requirements engineering.	Separation of Concerns; Crosscutting Concerns; Aspect-Oriented Software Development; Aspect-Oriented Requirements Engineering	Narender Singh; Nasib Singh Gill	nternational Journal of Computer Science Issues	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d316cfbc349d3e7e5b1d6e510b94a3765fe60cc5#page=311				Google Scholar	2011	Aspect-oriented requirements engineering for advanced separation of concerns: A review	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d316cfbc349d3e7e5b1d6e510b94a3765fe60cc5#page=311	IJCSI
Automatic Recommendation of Software Design Patterns: Text Retrieval Approach	Design pattern is a reusable solution to a commonly occurring design problem in certain context. Using design patterns in software development improves the product’s quality, understandability and productivity. However, it is a challenging task for novice developers to select the right design pattern to solve a design problem. The paper proposes a methodology for the automatic selection of the fit design pattern from a list of patterns. The proposed methodology is based on Text retrieval approach where the design problem scenarios are described in natural language. A vector space model (VSM) was created for the catalogue of design patterns. A vector of features consists of unigrams and bigrams is generated for the given design problem scenario. The recommended design pattern is the closest to the problem scenario. The proposed mechanism was evaluated using the Gang of four design patterns and the experimental results showed the effectiveness of the proposed methodology. 	Design pattern selection; Gang of four; Information retrieval; Recommendation; Vector space model	Abeer Hamdy; Mohamed Elsayed	Journal of Software 	https://doi.org/10.17706/jsw.13.4.260-268 		260-268 		Google Scholar	2018	Automatic recommendation of software design patterns: text retrieval approach	https://www.jsoftware.us/index.php?m=content&c=index&a=show&catid=193&id=2863	Journal of Sotware
Beyond object-oriented software development	The book presents some very interesting and excellent articles for this divergent title. The 22 chapters presented here cover core topics of computer science such as visualization of large databases, security, ontology, user interface, graphs, object oriented software developments, and on the engineering side filtering, motion dynamics, adaptive fuzzy logic, and hyper static mechanical systems. It also covers topics which are combination of computer science and engineering such as meta computing, future mobiles, colour image analysis, relative representation and recognition, and neural networks. The book will serve a unique purpose through these multi-disciplined topics to share different but interesting views on each of these topics. 		Dil Hussain 	Advances in Computer Science and IT	https://books.google.ca/books?id=thCQDwAAQBAJ&dq=Beyond+object-oriented+software+development&lr=&source=gbs_navlinks_s		428		Google Scholar	2009	Beyond object-oriented software development	https://books.google.ca/books?id=thCQDwAAQBAJ&dq=Beyond+object-oriented+software+development&lr=&source=gbs_navlinks_s	
Controlling the Destruction Order of Singleton Objects	The Singleton pattern [1] is a solution to (some of) the drawbacks of using global variables. Among its advantages is that the instance is always created prior to being referenced (this effectively solves the problem of initialization order when several interdependent instances are involved). This article examines some of the existing Singleton realizations in C++ and their drawbacks, and presents a solution to the complement problem, namely, the destruction order of global instances. To this end, I propose to use a Destruction Manager, so that all the singleton objects register with it, specifying interdependencies between them. Following [2], the Destruction Manager is analyzed as a composite design pattern.		E Gabrilovich	CC PLUS PLUS USERS JOURNAL 	https://gabrilovich.com/publications/papers/Gabrilovich1999CDO.pdf		10		Google Scholar	1999	Controlling the destruction order of singleton objects	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5c7b182be7d627bb9fba1b892cceaecfaf8c7f07	Citeseer
Database Pointers in Navigational and Object–Oriented Database Management Systems: A Comparison	As information systems and, more specifically, database management systems, attempt to model particular application environments, they must be able to account for and keep track of how the entities in the environments relate to each other. In the first or navigational generation of DBMS, relationships were maintained by pointer chains that connected the records representing the related entities. In the second or relational generation of DBMS, the tuples, representing related entities were not connected by pointers, but could be “joined” at query time based on common values of particular fields. In the third or object–oriented generation of DBMS, there are two major structural approaches. One is a pointer–based approach while the other, is designed to add advanced, object–oriented features to the relational model. Recently, perhaps inevitably, interest in the object–oriented pointer–based approach has led to questions of whether it is, in some sense, a return to navigational DBMS. While object–oriented database clearly has major features that go far beyond the capabilities of first generation DBMSs, this article will show that a comparative analysis of pointer usage in navigational DBMS and in object– oriented DBMS can yield interesting results, plus a better understanding of the object–oriented DBMS paradigm.		Mark Gillenson; Raymond D. Frost; Michael G. Kilpatrick	Journal of Database Management	https://doi.org/10.4018/jdm.1995100102 		10		Google Scholar	1995	Database pointers in navigational and object-oriented database management systems: a comparison	https://www.igi-global.com/gateway/article/51155	IGI Global
Describing and Composing Patterns Using Role Diagrams	Design patterns are patterns of classes and objects that represent solutions to recurring design problems. They are usually described using class diagrams. Class diagrams, however, often intertwine the actual solution with efficient ways of implementing it. This paper uses role diagrams to describe and com- pose patterns. Role diagrams help designers focus on the collaborations and distribution of responsibilities between objects. Role diagrams also are a better starting point for composing patterns. This paper presents several examples and reports on first experiences with using role diagrams for composing patterns which have been promising.		D Riehle	Proceedings of the 1996 Ubilab Conference	https://www.riehle.org/computer-science/research/1996/ubilab-woon-1996.pdf		16		Google Scholar	1996	Describing and composing patterns using role diagrams	https://www.riehle.org/computer-science/research/1996/ubilab-woon-1996.pdf	riehle
DESIGN PATTERN COMPONENTIZATION: THE DECORATOR LIBRARY IN C# 	Is it possible to convert design patterns into reusable components? This paper describes the study of the Decorator structural pattern and how this succeeded in producing a reusable library in C# 4.0 that satisfies the same requirements and intent of the original pattern. This paper describes the usage of the Decorator library, with a practical example and then proceeds to demonstrate the implementation of the library. The advantages and disadvantages of using the Decorator library rather than a traditional Decorator implementation are also discussed. 	Design Pattern; Decorator Pattern; Duck Typing; Reuse; Library Design; Genericity 	Alastair van Leeuwen 	IADIS International Conference Applied Computing 2012	https://www.iadisportal.org/digital-library/design-pattern-componentization-the-decorator-library-in-c		7		Google Scholar	2012	Design pattern componentization: the decorator library in C#	https://www.iadisportal.org/digital-library/design-pattern-componentization-the-decorator-library-in-c	International Association for Development of the Information Society 
Enabling better research through a new paradigm for access and usability of atmospheric science data	With emergence of better technology, the rate of data gathering has grown enormously. However, technology has not been fully exploited in the scientific data analysis process. There are obstacles in data preprocessing that the scientists need to overcome before proceeding with the analysis. The first obstacle is designing and developing the preprocessing algorithms for the data sets required for their study. Most data archiving centers provide search and access capabilities for locating and retrieving data. However, these centers do not provide preprocessing algorithms that a scientist could utilize for data preparation and transformation. This leads to redundancy of time and effort as multiple scientists using the same data set end up developing the same preprocessing algorithms. The second major preprocessing obstacle faced by the scientists is data in the different variety of formats. Unfortunately in atmospheric science, there is no single standard data format. Data created by different centers or scientists can be created in numerous formats. This leads to the dat-/application interoperability problem where an application cannot utilize data in a different data format. Moreover, some of these formats can be complex and require a great deal of effort to decode. This investigation looked at solutions to solve these two problems. The solution for providing preprocessing capabilities to scientists is to encapsulate the required algorithms along with the data as an Intelligent Data Object. These objects then can be used through their set of interfaces to invoke processing algorithms. A prototype Data Ordering System built on this concept is demonstrated in this study. An interchange technology is researched as a solution for the data/application interoperability problem. This interchange technology consisting of Markup files and an associated library provide an elegant solution to this problem. Detailed procedures for utilizing this interchange technology for atmospheric science data sets were compiled in this study. This research also compared these two solutions against other approaches taken by different researchers. 	Pure sciences; Earth sciences; Atmospheric science data; Data access; Intelligent data objects; Interchange technology	Ramachandran, Rahul	ProQuest Dissertations & Theses 	https://www.proquest.com/docview/276299394?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses		236		Google Scholar	2002	Enabling better research through a new paradigm for access and usability of atmospheric science data	https://www.proquest.com/docview/276299394?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses	The University of Alabama 
					https://books.google.ca/books?hl=en&lr=&id=r5-jAgAAQBAJ&oi=fnd&pg=PP1&dq=%22First+UK+Colloquium+on+Object+Technology+and+Systems+Re-Engineering%22&ots=Vai9cjrrZ7&sig=cQ-G9Xj2A1T2--QcIEBlI-NV5n8#v=onepage&q=%22First%20UK%20Colloquium%20on%20Object%20Technology%20and%20Systems%20Re-Engineering%22&f=false						First UK Colloquium on Object Technology and Systems Re-Engineering (COTSR)		
Identifying Software Complexity Topics with Latent Dirichlet Allocation on Design Patterns	The scientific literature has paid limited attention to studying software complexity subjects from the design point of view. There is a significant number of papers that study software complexity in relation with maintenance, refactoring, source code changes and that establish metrics for measuring software complexity. This paper compares design patterns and software complexity in order to identify trends of research in the software complexity area. For this purpose, we assess the strengths and weaknesses of software complexity scientific articles through the lens of design patterns. We have reviewed 1068 papers via latent Dirichlet allocation technique (LDA) for our work. We found that existing software complexity paths disproportionate empha- sis in how software complexity could benefit from design patterns instead on how contributions to design patterns can benefit from software complexity.	latent Dirichlet allocation; design patterns; software complexity; software complexity topic modelling	Sabina-Cristiana NECULA; Catalin STRIMBEI	Informatica Economica	https://revistaie.ase.ro/content/92/01%20-%20necula,%20strimbei.pdf		https://revistaie.ase.ro/content/92/01%20-%20necula,%20strimbei.pdf		Google Scholar	2019	Identifying Software Complexity Topics with Latent Dirichlet Allocation on Design Patterns	https://revistaie.ase.ro/content/92/01%20-%20necula,%20strimbei.pdf	INFOREC Association 
Information Science: Towards an Information Paradigm	For the past century, the fundamental principle guiding information management (library science, documentation, archives) rests on the idea that knowledge is universal and that, consequently, it can be organised in the same fashion everywhere. Besides, this organisation is made more efficient when the rules are well established and professionals are encouraged to use them.  The documents upon which knowledge is stored must be described in the most objective manner possible and the places where their processing and storage are carried out must be considered as technical systems whose inputs and efficiency can be measured. Finally, the users, who wish to use this knowledge, can be classified in order to better meet their needs.  Globally, this vision is consistent with a paradigm that can be qualified as classic, insofar as it has had an important impact on processing. Three principals are important; they are universality-standardisation, object-document and organisation-system.  Without questioning the contribution of this paradigm to the description and improvement of document information systems, it is nevertheless clear that it qualifies a much too technical approach to documentation. It does not enable one to understand the behaviours and practices of users (and even less those of non-users), who must comply with the organisation of this information system. Nor does it leave room for a discussion of the merits of practices given that professionals must comply with the universal standard regardless of where they work. This article suggests a new approach that places the human (author, mediator, user) at the centre of phenomena observed in information research. This approach is based on a paradigm that we label as informational and is completely described is a communication approach; examining this discipline becomes relevant. The founding concepts are intent, meaning and personalisation.  The purpose is not to replace one paradigm with another. Each conveys a specific outlook on the world. The goal is to establish information science as a human science.		Hubert Fondin 	Documentation et bibliothèques	https://doi.org/10.7202/1030297ar				Google Scholar	2015	Information science: towards an information paradigm	https://id.erudit.org/iderudit/1030297ar	Erudit
Manifest 3D: A Framework to Develop 3D Graphics Applications	The construction of 3D graphics applications is tedious work that consumes much more time than conventional 2D graphics applications, requiring, besides, specific knowledge about 3D geometry operations. To alleviate these problems several object-oriented frameworks for 3D graphics application construction have been proposed in the literature. They attempt to provide a generic architecture for building 3D graphics applications, but, in general, they tighten their designs to specific 3D rendering libraries, or they propose ad-hoc abstractions that do not respond to generic design problems. That is, they are not designed following problem-centered design rules that can encourage a better solution in terms of flexibility and adaptability of the solution. With the goal of minimizing these problems, Manifest3D, a framework to develop 3D graphics applications, was developed. The framework design was thoroughly driven by design patterns. This process brings benefits that impact directly on understanding, reuse, evolution, analysis and documentation management of the framework. As a consequence, it makes the instantiation of applications easier by composition of basic behaviors. In this paper, the main design aspects of Manifest3D are presented, as well as several examples of its instantiation to build 3D graphics visualizations.	Object-Oriented Frameworks; Design Patterns; 3d Software Visualization	Alfredo Teyseyre; Ricardo Orosco; Marcelo Campo		https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e962c97da0cd77a1ccaaf90b1859d9795e0a49cc		13		Google Scholar	1999	Manifest3D: a framework to develop 3D graphics applications	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e962c97da0cd77a1ccaaf90b1859d9795e0a49cc	Citeseer
Momentum Exchange Between Light and Nanostructured Matter	An object’s translational and rotational motion is associated with linear and angular momenta. When multiple objects interact the exchange of momentum dictates the new system’s motion. Since light, despite being massless, carries both linear and angular momentum it too can partake in this momentum exchange and mechanically affect matter in tangible ways. Due to conservation of momentum, any such exchange must be reciprocal, and the light therefore acquires an opposing momentum component. Hence, light and matter are inextricably connected and one can be manipulated to induce interesting effects to the other. Naturally, any such effect is facilitated by having strongly enhanced light-matter interaction, which for visible light is something that is obtained when nanostructured matter supports optical resonances. This thesis explores this reciprocal relationship and how nanostructured matter can be utilised to augment these phenomena.  Once focused by a strong lens, light can form optical tweezers which through optical forces and torques can confine and manipulate small particles in space. Metallic nanorods trapped in two dimensions against a cover glass can receive enough angular momentum from circularly polarised light to rotate with frequencies of several tens of kilohertz. In the first paper of this thesis, the photothermal effects associated with such optical rotations are studied to observe elevated thermal environments and morphological changes to the nanorod. Moreover, to elucidate upon the interactions between the trapped particle and the nearby glass surface, in the thesis’ second paper a study is conducted to quantify the separation distance between the two under different trapping conditions. The particle is found to be confined ⇠30-90 nm away from the surface.  The momentum exchange from a single nanoparticle to a light beam is negligible. However, by tailoring the response of an array of nanoparticles, phase-gradient metasurfaces can be constructed that collectively and controllably alter the incoming light’s momentum in a macroscopically significant way, potentially enabling a paradigm shift to flat optical components. In the thesis’ third paper, a novel fabrication technique to build such metasurfaces in a patternable polymer resist is investigated. The technique is shown to produce efficient, large-scale, potentially flexible, substrate-independent flat optical devices with reduced fabricational complexity, required time, and cost.  At present, optical metasurfaces are commonly viewed as stationary objects that manipulate light just like common optical components, but do not themselves react to the light’s changed momentum. In the last paper of this thesis, it is realised that this is an overlooked potential source of optical force and torque. By incorporating a beam-steering metasurface into a microparticle, a new type of nanoscopic robot – a metavehicle – is invented. Its propulsion and steering are based on metasurfaceinduced optical momentum transfer and the metavehicle is shown to be driven in complex shapes even while transporting microscopic cargo.		Andren, Daniel	ProQuest Dissertations & Theses	https://www.proquest.com/docview/2606903596/fulltextPDF/91DEC0885D6A4018PQ/1?accountid=12543&sourcetype=Dissertations%20&%20Theses		97		Google Scholar	2021	Momentum Exchange Between Light and Nanostructured Matter	https://www.proquest.com/docview/2606903596/fulltextPDF/91DEC0885D6A4018PQ/1?accountid=12543&sourcetype=Dissertations%20&%20Theses	Chalmers Tekniska Hogskola (Sweden) 
					https://books.google.ca/books?id=4dZWn8rnU2cC&dq=%22Multithreaded+programming+with+the+Command+pattern%22&lr=&source=gbs_navlinks_s						Multithreaded programming with the Command pattern		
Nucleus C++ : an overview		Software development; Object oriented; Real time	BROOKS, L	Real-time systems conference 	https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=2860191		4		Google Scholar	1997	Nucleus C++-an overview	https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=2860191	Pascal and Francis
Object-Oriented Technology and Computing Systems Re-Engineering	This book delivers the latest developments in object technology and their impact in computing systems re-engineering. Object-oriented programming is here shown to provide support for constructing large scale systems that are cheaply built and with reusable components, adaptable to changing requirements and use efficient and cost-effective techniques.Internationally recognised authorities from Finland, France, Germany, Italy, Poland, Spain, the UK and the USA here record their research and development work on the industrial techniques and structured object-oriented methodologies in forward and reverse engineering of computing systems. This book takes stock of progress of that work showing its promise and feasibility, and how its structured technology can overcome the limitations of forward engineering methods used in industry. Forward methods are focused in the domain of reverse engineering to implement a high level of specification for existing software.The book contains the selected, quintessential content of the first UK Colloquium on Object Technology and Systems Re-Engineering held at Oxford University in 1998. The conference was sponsored by British Telecom Laboratories, EMSI limited and the OOSP Specialised Group of The British Computer Society. - Delivers the latest developments in object technology and their impact in computing systems re-engineering - Provides support for constructing large scale systems that are cheaply built and with reusable components, adaptable to changing requirements and use efficient and cost-effective techniques - Contains the content of the first UK Colloquium on Object Technology and Systems Re-Engineering held at Oxford University in 1998		H. S. M. Zedan; A Cau		https://books.google.ca/books?id=r5-jAgAAQBAJ&dq=%22Object-oriented+technology+and+computing+systems+re-engineering%22&lr=&source=gbs_navlinks_s		208		Google Scholar	1999	Object-oriented technology and computing systems re-engineering	https://books.google.ca/books?id=r5-jAgAAQBAJ&dq=%22Object-oriented+technology+and+computing+systems+re-engineering%22&lr=&source=gbs_navlinks_s	Elsevier
Parametric Aspects: A Proposal	Aspect-Oriented Software Development (AOSD) provides bet- ter design solutions to problems where Object Oriented Development produces tangled and scattered designs. Nevertheless, there are still sev- eral problems for which AOSD is not helpful. An example is the imple- mentation of some design patterns. While it has been shown that some of them can be implemented in a domain-independent way by the use of AOSD [1], there is still a group of them for which current AOSD tech- niques are of little use. This paper proposes to extend Aspect Oriented Languages with parametric aspects. This extension can integrate seam- lessly into Aspect Oriented Languages like AspectJ, and allows to provide a better design solution for problems for which current AOSD techniques are of little help, improving software reuse and reducing its complexity, thus facilitating the software evolution process. Two representative ex- amples are used in order to expose the proposed extension: the imple- mentation of the abstract factory pattern in a domain-independent way, and that of a simple Enterprise Java Bean.		Alvarez, Jordi 	ECOOP'04 Workshop on Reflection, AOP, and Meta-Data for Software Evolution (RAM-SE) 	https://www.is.c.titech.ac.jp/report-c/C-196.pdf#page=100		91-99		Google Scholar	2004	Parametric aspects: a proposal	https://www.is.c.titech.ac.jp/report-c/C-196.pdf#page=100	Institute of Science Tokyo
	Robert Martin, Editor-in-Chief of C++ Report, presents the long-awaited follow-up to C++ Gems. Since the publication of the first book, the C++ language has experienced very many changes. The ISO has adopted a standard for the language and its library. The Unified Modeling Language has affected software development in C++, and Java has changed things as well. Through all of these turbulent changes, C++ Report has been the forum for developers and programmers to share their experience and discuss new directions for the industry. More C++ Gems picks up where the first book left off, presenting tips, tricks, proven strategies, easy-to-follow techniques, and usable source code. This book contains the very best from the most renowned experts in the field.		Robert C. Martin 		https://books.google.ca/books?id=gYNVsCaxfYgC&dq=%22Patterns+for+mapping+OO+applications+to+relational+databases%22&lr=&source=gbs_navlinks_s		525		Google Scholar	2000	Patterns for mapping OO applications to relational databases	https://books.google.ca/books?id=gYNVsCaxfYgC&dq=%22Patterns+for+mapping+OO+applications+to+relational+databases%22&lr=&source=gbs_navlinks_s	Cambridge University Press 
Progress on Patterns: Highlights of PloP/94	Software patterns support an emerging family of software design techniques. At the first annual conference on Pattern Languages of Programming, the pattern community refined its understanding of what makes a good software design pattern. Attendees explored and refined patterns in writers’ workshops, using techniques borrowed from literature and poetry. Many different pattern forms were found to be work well, though most exhibited the basics of the Alexandrian form. There was a disproportionate representation of patterns for distributed processing, client/server architectures, and finite state machines. These are all maturing domains, for which patterns capture established good practice. Good patterns aren’t necessarily inventive, but capture hard-won experience in software domains of broad interest.		James O. Coplien	Proceedings of Object Expo Europe 	https://scholar.googleusercontent.com/scholar?q=cache:SdcMmbcVL_UJ:scholar.google.com/&hl=en&as_sdt=0,5		7		Google Scholar	1994	Progress on patterns: highlights of PLoP/94	https://scholar.googleusercontent.com/scholar?q=cache:SdcMmbcVL_UJ:scholar.google.com/&hl=en&as_sdt=0,5	AT&T Bell Laboratories 
					https://landingpage.bsigroup.com/LandingPage/Series?UPI=BS%20EN%2015531						Public transport. Service interface for real-time information relating to public transport operations. Context and framework		
Reifying Design Patterns as Metalevel Constructs	A design pattern describes a structure of communicating components that solves a commonly occurring design problem. Designing with patterns offers the possibility of raising the abstraction level at which design is performed, with improvements in clarity, understanding, and facility of maintenance of applications. However, in their most common presentation, design patterns are informal pieces of design process, which application is not reflected in the operational system, and the potential advantages of a more principled design are not realized. This work proposes to organize design in such a way that pattern applications remain explicit in the operational systems. A reflective architecture is proposed, where patterns are reified as metalevel constructs.	Design patterns; metalevel architecture; metaobjects; computational reflection	 Marcos, Claudia A.; Campo, Marcelo; Pirotte, Alain	Electronic Journal of SADIO 	https://sedici.unlp.edu.ar/handle/10915/134875		17-29 		Google Scholar	1999	Reifying design patterns as metalevel constructs	https://sedici.unlp.edu.ar/handle/10915/134875	SEDICI
SoCoEMo-COTS: A Software Economic Model for Commercial Off-The-Shelf (COTS) Based Software Development	With component-based reuse, software development is achieved through the planned integration of pre-existing software components. Commercial-off-the-shelf (COTS) Based Development (CBD) is one of the systematic reuse approaches promising gains in cost, operational quality, functionality, time to market and maintenance overheads. This is an increasingly popular paradigm for software development, but one that is not without risks and associated costs. In fact, an increasing number and variety of COTS components become available but it is important to understand the costs, benefits, and risks entailed in using these components. This paper presents an economic model for any organization willing to adopt a CBD approach for its systems development. Such model which quantifies the predicted benefits and return on investment of CBD can help managers make the good decision to adopt or not such a reuse approach. We denote our model SoCoEMo-COTS for “Software Cost Estimation Model for COTS”.	COTS; cost estimation model; investment cycles	Sana Ben Abdallah Ben Lamine; Lamia Labed Jilani; Henda Hajjami Ben Ghezala	Software Engineering Research and Practice	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3a323b0356fbe34ae9e07ce0d524eb19c029b045		7		Google Scholar	2006	SoCoEMo-COTS: a software economic model for commercial off-the-shelf (COTS) based software development	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3a323b0356fbe34ae9e07ce0d524eb19c029b045	Citeseer
Strategy Pattern as a Variability Enabling Mechanism in Product Line Architecture	Business applications that share a common architecture and a set of reusable components, implemented by the Software Product Line (SPL) approach to software reuse, can benefit from handling the variability with extensive use of architectural design patterns. Use of the patterns within the Product Line Architecture (PLA) frameworks, yields a number of benefits toward the improvement of maintainability of the applications which are part of a SPL family. This paper presents use of the Strategy pattern within PLA as a preplanned variability enabling mechanism for SPL. 	Strategy; Pattern; Software Product Lines; Variability; Framework; Reuse; Architecture	Zdravko Rosko	Central European Conference on Information and Intelligent Systems 	https://www.bib.irb.hr:8443/634816/download/634816.ZdravkoRosko2012V2.pdf		9		Google Scholar	2012	Strategy Pattern as a Variability Enabling Mechanism in Product Line Architecture	https://www.bib.irb.hr:8443/634816/download/634816.ZdravkoRosko2012V2.pdf	Institut Ruder Boskovic
Telescope: An Object-Oriented Architecture for Visualization Systems	The construction of information visualization systems is a difficult task. The provision of an object-oriented software architecture for this kind of systems can help to reduce this difficulty. However, this approach is not currently used in most visualization systems. In this work, Telescope, an object-oriented architecture for visualization systems, is presented. Its main goal is to facilitate this construction, taking advantage of the benefits of the object-oriented paradigm (reusability, extensibility, and maintainability). The Telescope architecture is based on six main components, which are common to all information visualization systems: data representation, data abstraction, data objects- graphical objects mapping, presentation, interaction and visualization state. CityVis, a visualization system for city data, developed using Telescope architecture is described, showing the implementation of each Telescope component. Also, the implementation of several features in visualization systems, such as visualization techniques and management of abstraction levels and revealed information is described. Finally, current work and conclusions of the project are explained.	information visualization system; Telescope architecture 	Orosco, Ricardo Fabian	Electronic Journal of SADIO	https://sedici.unlp.edu.ar/handle/10915/135550		88-101		Google Scholar	1998	Telescope: an object-oriented architecture for visualization systems	https://sedici.unlp.edu.ar/handle/10915/135550	SEDICI
					https://blog.klipse.tech/databook/2020/09/25/data-book-chap1-part1.html						The complexity of object-oriented logic programming		
Towards a Pattern- based Model Transformation Approach for Design Quality Enhancement	Recently, the growing popularity of model driven frameworks and methodologies, as well as the Model Driven Architecture (MDA) initiated by Object Management Group (OMG) has implied an increasing focus on model transformation. Meanwhile, the impact of design patterns on software quality has attracted a gradual attention since design patterns encapsulate valuable knowledge to resolve design problems and improve design quality. As an attempt towards the investigation of applying goals and design patterns to realize the model transformation, we proposed, in this paper, a goal-driven model transformation by applying design patterns to transform an analysis model into its corresponding design model with an emphasis on the non-functional requirements. The use of goals makes it easier to transform the functional and non-functional requirements into the software models, and derives the candidate design patterns to help satisfy nonfunctional requirements for resolving the design problems and improving software quality.	Design patterns; model Transformation; goal-driven approach; design quality	Yong-Yi Fanjiang; Nien-Lin Hsueh; Jonathan Lee	Journal of Software Engineering Studies	https://www.csie.ntu.edu.tw/~jlee/publication/63-170-1-PB[1].pdf		14		Google Scholar	2007	Towards a Pattern-based Model Transformation Approach for Design Quality Enhancement	https://www.csie.ntu.edu.tw/~jlee/publication/63-170-1-PB[1].pdf	Software Engineering Association of Taiwan
Using Aspect Oriented Techniques to Build-in Software Quality	Today’s software systems are growing rapidly in number, size, complexity, amount of distribution and number of users with the evolving technologies being geared towards improving their quality. Aspect oriented software development is a new paradigm that claims to improve the quality of software using separation of concerns. In this paper we show how the aspect oriented paradigm has evolved from the object oriented paradigm, giving definitions of key aspect oriented terms to aid comprehension and clarification. We then show how techniques involved in aspect oriented design can help to improve software quality.	Aspect; Weavers; Crosscutting; Object Oriented Paradigm; Point_Cuts; Advice	Obeten O. Ekabua	IJCSI International Journal of Computer Science Issues	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0f0569af0b471230c814bf136a7ea25b49ea8044		6		Google Scholar	2012	Using aspect oriented techniques to build-in software quality	https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0f0569af0b471230c814bf136a7ea25b49ea8044	IJCSI
Using Patterns in Design and Documentation of Software	In software development, as well as in other areas of life and science, recurring themes and structures are encountered. By expressly describing them together with the underlying considerations and decisions in an abstract manner, i.e. in a pattern form, the knowledge and experience of the developers can be recorded in a structured and systematic way. Such patterns may then be used by other developers to reuse that recorded experience. Furthermore, patterns help to improve the quality of software documentation. This paper rst gives an introduction to the pattern subject. The authors then discuss their experience with using patterns. In this discussion particular attention is paid to some problems associated with pattern employment.		Thorsten Gerth; R. Schachtschabel; R. Schonefeld		https://api.semanticscholar.org/CorpusID:16226608			“@inproceedings{Gerth1996UsingPI,   title={Using Patterns in Design and Documentation of Software},   author={Thorsten Gerth and R. Schachtschabel and R. Schonefeld},   year={1996},   url={https://api.semanticscholar.org/CorpusID:16226608} }”	Google Scholar	1996	Using patterns in design and documentation of software	https://www.semanticscholar.org/paper/Using-Patterns-in-Design-and-Documentation-of-Gerth-Schachtschabel/c5083e3531e8c342c535db33d84a98e6f3d02de2	
AUTOMATIC TEST GENERATION WITH OCHIAI ALGORITHM	In software system application development, design and implementations are the two basic procedures. To improve better design for software system, patterns are required to improve reusability. In object oriented design patterns in software systems are basic answers for improve weak code design development & quality of service. Different anti-patterns & patterns techniques were introduced to solve recurring questions in software application development process using some metric and rule based anti pattern design approach in software system process. Metric rule based approach consists static analyzer, code analyzer filter mechanism based analyzer to detect weak implementation in code declaration and initialization with different parameters processing. These rules were used to implement for only detection of code design implementations in software implementation. To improve testing anti patterns in software implementation, we introduce a Novel test implementation that is generating test cases for effective fault detection anti-pattern implementation. We implement metamorphic relations in generation of automatic tests suites based on ochiai calculations in data representation for test generation. On average implementation process, time based test generation based on code constraints gives tests for each method implementation and parameter sequence in software implementation. Our experimental results show effective quality of service to generate automatic test suites in object oriented software systems.	Web; Desktop applications; Debugging and Testing; Ochiai Testing; Automatic testing; Ochiai relations	ASR Murthy; M.V.S Dhiraj; T. Ravindra; Y. Mohan; P. UmaMaheswara Rao	International Journal of Pure and Applied Mathematics	https://acadpubl.eu/jsi/2017-115-6-7/articles/8/48.pdf		6		Google Scholar	2017	Automatic test generation with ochiai algorithm	https://acadpubl.eu/jsi/2017-115-6-7/articles/8/48.pdf	IJPAM
A systematic literature review: Effects of Anti-Patterns on Software Maintenance 	Anti-patterns are the faultswhich negatively affect the performance of the device. An identification of anti-patterns is called code smell leading to software refactoring. Therefore it is difficult to manage the maintenance. As much the number of smells needs further refactoring. Various methods for detecting anti-patterns in the system have been established.This paper is analyzing the effect of antipatterns on classes and which seem to be other forms of anti-patterns that shows the greater effect than others. Eventually, the outcomes for the future research of open source systems have been concluded.This paper is partitioned into four parts, in which the forms of anti-patterns follow the introduction. In addition the associated study was thoroughly analyzed with a brief description. Thus the paper shows variousmethods to recognize code smells. Due to this, the identification of smells would be beneficial in ensuring greater reliability during maintenance andtesting processes by detecting faults and anti-patterns prior to product delivery.Therefore, the detection of anti-patterns will allow administrators and programmers to enhance the maintenance activities of software development. 		Navneet Chaudhry; Mandeep Singh	International Journal of Advanced Science and Technology	http://sersc.org/journals/index.php/IJAST/article/view/16867		2379-2386		Google Scholar	2020	A systematic literature review: Effects of anti-patterns on software maintenance	http://sersc.org/journals/index.php/IJAST/article/view/16867	SERSC
