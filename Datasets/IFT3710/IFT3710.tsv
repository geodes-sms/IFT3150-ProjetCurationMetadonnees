key	project	title	abstract	keywords	authors	venue	doi	references	pages	bibtex	screened_decision	final_decision	mode	inclusion_criteria	exclusion_criteria	reviewer_count	source	year	meta_title	link	publisher	metadata_missing
0	IFT3710	Physics of Language Models: Part 1, Learning Hierarchical Language Structures	Transformer-based language models are effective but complex, and understanding their inner workings is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models grasp complex, recursive language structures defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn this CFG language and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.This paper also presents several corollaries, including showing why positional embedding is inferior to relative attention or rotary embedding; demonstrating that encoder-based models (e.g., BERT, deBERTa) cannot learn very deeply nested CFGs as effectively as generative models (e.g., GPT); and highlighting the necessity of adding structural and syntactic errors to the pretraining data to make the model more robust to corrupted language prefixes.		Zeyuan Allen-Zhu; Yuanzhi Li	arXiv	https://doi.org/10.48550/arXiv.2305.13673										arXiv		Physics of Language Models: Part 1, Learning Hierarchical Language Structures	https://arxiv.org/abs/2305.13673	arXiv	nan; Keywords; References; Pages; Year; Bibtex
1	IFT3710	Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems	"Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to ""self-correct"" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating ""error-correction"" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others."		Tian Ye; Zicheng Xu; Yuanzhi Li; Zeyuan Allen-Zhu	arXiv	https://doi.org/10.48550/arXiv.2408.16293										arXiv		Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems	https://arxiv.org/abs/2408.16293	arXiv	nan; Keywords; References; Pages; Year; Bibtex
2	IFT3710	Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process	Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.		Tian Ye; Zicheng Xu; Yuanzhi Li; Zeyuan Allen-Zhu	arXiv	https://doi.org/10.48550/arXiv.2407.20311										arXiv		Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process	https://arxiv.org/abs/2407.20311	arXiv	nan; Keywords; References; Pages; Year; Bibtex
3	IFT3710	Physics of Language Models: Part 3.1, Knowledge Storage and Extraction	"Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., ""What is Abraham Lincoln's birthday?""). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia?In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data.Essentially, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations)during pretraining. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning.To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text.This paper providesseveral key recommendations for LLM pretraining in the industry: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late."		Zeyuan Allen-Zhu; Yuanzhi Li	arXiv	https://doi.org/10.48550/arXiv.2309.14316										arXiv		Physics of Language Models: Part 3.1, Knowledge Storage and Extraction	https://arxiv.org/abs/2309.14316	arXiv	nan; Keywords; References; Pages; Year; Bibtex
4	IFT3710	Physics of Language Models: Part 3.2, Knowledge Manipulation	"Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: retrieval (e.g., ""What is person A's attribute X?""), classification (e.g., ""Is A's attribute X even or odd?""), comparison (e.g., ""Is A greater than B in attribute X?""), and inverse search (e.g., ""Which person's attribute X equals T?"").We show that language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. Moreover, their performance in inverse knowledge search is virtually 0%, regardless of the prompts. Our primary contribution is a controlled, synthetic experiment that confirms these weaknesses are inherent to language models: they cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored in the models, despite adequate training and sufficient model size. Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs."		Zeyuan Allen-Zhu; Yuanzhi Li	arXiv	https://doi.org/10.48550/arXiv.2309.14402										arXiv		Physics of Language Models: Part 3.2, Knowledge Manipulation	https://arxiv.org/abs/2309.14402	arXiv	nan; Keywords; References; Pages; Year; Bibtex
5	IFT3710	Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws	Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include:* The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.* Prepending training data with domain names (e.g.,this http URL) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.		Zeyuan Allen-Zhu; Yuanzhi Li	arXiv	https://doi.org/10.48550/arXiv.2404.05405										arXiv		Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws	https://arxiv.org/abs/2404.05405	arXiv	nan; Keywords; References; Pages; Year; Bibtex
6	IFT3710	The First Law of Complexodynamics					https://scottaaronson.blog/?p=762													https://scottaaronson.blog/?p=762		
7	IFT3710	The Unreasonable Effectiveness of Recurrent Neural Networks					https://karpathy.github.io/2015/05/21/rnn-effectiveness/													https://karpathy.github.io/2015/05/21/rnn-effectiveness/		
8	IFT3710	Recurrent Neural Network Regularization					https://arxiv.org/pdf/1409.2329													https://arxiv.org/pdf/1409.2329		
9	IFT3710	Keeping Neural Networks Simple by Minimizing the Description Length of the Weights					https://www.cs.toronto.edu/~fritz/absps/colt93.pdf													https://www.cs.toronto.edu/~fritz/absps/colt93.pdf		
10	IFT3710	Order Matters: Sequence to sequence for sets	Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.		Oriol Vinyals; Samy Bengio; Manjunath Kudlur	arXiv	https://doi.org/10.48550/arXiv.1511.06391										arXiv		Order Matters: Sequence to Sequence for Sets	https://arxiv.org/abs/1511.06391	arXiv	nan; Keywords; References; Pages; Year; Bibtex
11	IFT3710	GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism					https://arxiv.org/pdf/1811.06965													https://arxiv.org/pdf/1811.06965		
12	IFT3710	Multi-Scale Context Aggregation by Dilated Convolutions	State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.		Fisher Yu; Vladlen Koltun	arXiv	https://doi.org/10.48550/arXiv.1511.07122										arXiv		Multi-Scale Context Aggregation by Dilated Convolutions	https://arxiv.org/abs/1511.07122	arXiv	nan; Keywords; References; Pages; Year; Bibtex
13	IFT3710	Identity Mappings in Deep Residual Networks	Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at:this https URL		Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun	arXiv	https://doi.org/10.48550/arXiv.1603.05027										arXiv		Identity Mappings in Deep Residual Networks	https://arxiv.org/abs/1603.05027	arXiv	nan; Keywords; References; Pages; Year; Bibtex
14	IFT3710	A simple neural network module for relational reasoning	Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.		Adam Santoro; David Raposo; David G.T. Barrett; Mateusz Malinowski; Razvan Pascanu; Peter Battaglia; Timothy Lillicrap	arXiv	https://doi.org/10.48550/arXiv.1706.01427										arXiv		A Simple Neural Network Module for Relational Reasoning	https://arxiv.org/abs/1706.01427	arXiv	nan; Keywords; References; Pages; Year; Bibtex
15	IFT3710	Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton					https://arxiv.org/pdf/1405.6903													https://arxiv.org/pdf/1405.6903		
16	IFT3710	Neural Turing Machines	We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.		Alex Graves; Greg Wayne; Ivo Danihelka	arXiv	https://doi.org/10.48550/arXiv.1410.5401										arXiv		Neural Turing Machines	https://arxiv.org/abs/1410.5401	arXiv	nan; Keywords; References; Pages; Year; Bibtex
17	IFT3710	Deep Speech 2: End-to-End Speech Recognition in English and Mandarin	We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.		Dario Amodei; Rishita Anubhai; Eric Battenberg; Carl Case; Jared Casper; Bryan Catanzaro; Jingdong Chen; Mike Chrzanowski; Adam Coates; Greg Diamos; Erich Elsen; Jesse Engel; Linxi Fan; Christopher Fougner; Tony Han; Awni Hannun; Billy Jun; Patrick LeGresley; Libby Lin; Sharan Narang; Andrew Ng; Sherjil Ozair; Ryan Prenger; Jonathan Raiman; Sanjeev Satheesh; David Seetapun; Shubho Sengupta; Yi Wang; Zhiqian Wang; Chong Wang; Bo Xiao; Dani Yogatama; Jun Zhan; Zhenyao Zhu	arXiv	https://doi.org/10.48550/arXiv.1512.02595										arXiv		Deep Speech 2: End-to-End Speech Recognition in English and Mandarin	https://arxiv.org/abs/1512.02595	arXiv	nan; Keywords; References; Pages; Year; Bibtex
18	IFT3710	Scaling Laws for Neural Language Models	We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.		Jared Kaplan; Sam McCandlish; Tom Henighan; Tom B. Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei	arXiv	https://doi.org/10.48550/arXiv.2001.08361										arXiv		Scaling Laws for Neural Language Models	https://arxiv.org/abs/2001.08361	arXiv	nan; Keywords; References; Pages; Year; Bibtex
19	IFT3710	A tutorial introduction to the minimum description length principle	"This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection ""Advances in Minimum Description Length: Theory and Application"" (edited bythis http URL, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005)."		Peter Grunwald	arXiv	https://doi.org/10.48550/arXiv.math/0406077										arXiv		A Tutorial Introduction to the Minimum Description Length Principle	https://arxiv.org/abs/math/0406077	arXiv	nan; Keywords; References; Pages; Year; Bibtex
20	IFT3710	Machine Super Intelligence					https://www.vetta.org/documents/Machine_Super_Intelligence.pdf													https://www.vetta.org/documents/Machine_Super_Intelligence.pdf		
21	IFT3710	Better & Faster Large Language Models via Multi-token Prediction	Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.		Fabian Gloeckle; Badr Youbi Idrissi; Baptiste Roziere; David Lopez-Paz; Gabriel Synnaeve	arXiv	https://doi.org/10.48550/arXiv.2404.19737										arXiv		Better & Faster Large Language Models Via Multi-token Prediction	https://arxiv.org/abs/2404.19737	arXiv	nan; Keywords; References; Pages; Year; Bibtex
22	IFT3710	Zephyr: Direct Distillation of LM Alignment	We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available atthis https URL.		Lewis Tunstall; Edward Beeching; Nathan Lambert; Nazneen Rajani; Kashif Rasul; Younes Belkada; Shengyi Huang; Leandro von Werra; Clementine Fourrier; Nathan Habib; Nathan Sarrazin; Omar Sanseviero; Alexander M. Rush; Thomas Wolf	arXiv	https://doi.org/10.48550/arXiv.2310.16944										arXiv		Zephyr: Direct Distillation of LM Alignment	https://arxiv.org/abs/2310.16944	arXiv	nan; Keywords; References; Pages; Year; Bibtex
23	IFT3710	DeepSeek V3 Technical Report					https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf													https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf		
24	IFT3710	Mixtral of Experts	We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.		Albert Q. Jiang; Alexandre Sablayrolles; Antoine Roux; Arthur Mensch; Blanche Savary; Chris Bamford; Devendra Singh Chaplot; Diego de las Casas; Emma Bou Hanna; Florian Bressand; Gianna Lengyel; Guillaume Bour; Guillaume Lample; Lelio Renard Lavaud; Lucile Saulnier; Marie-Anne Lachaux; Pierre Stock; Sandeep Subramanian; Sophia Yang; Szymon Antoniak; Teven Le Scao; Theophile Gervet; Thibaut Lavril; Thomas Wang; Timothee Lacroix; William El Sayed	arXiv	https://doi.org/10.48550/arXiv.2401.04088										arXiv		Mixtral of Experts	https://arxiv.org/abs/2401.04088	arXiv	nan; Keywords; References; Pages; Year; Bibtex
25	IFT3710	DoRA: Weight-Decomposed Low-Rank Adaptation	Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing \ours, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. \ours~consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code is available atthis https URL.		Shih-Yang Liu; Chien-Yi Wang; Hongxu Yin; Pavlo Molchanov; Yu-Chiang Frank Wang; Kwang-Ting Cheng; Min-Hung Chen	arXiv	https://doi.org/10.48550/arXiv.2402.09353										arXiv		DoRA: Weight-Decomposed Low-Rank Adaptation	https://arxiv.org/abs/2402.09353	arXiv	nan; Keywords; References; Pages; Year; Bibtex
26	IFT3710	Simple and Scalable Strategies to Continually Pre-train Large Language Models	Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English-English) and a stronger distribution shift (English-German) at the405M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.		Adam Ibrahim; Benjamin Therien; Kshitij Gupta; Mats L. Richter; Quentin Anthony; Timothee Lesort; Eugene Belilovsky; Irina Rish	arXiv	https://doi.org/10.48550/arXiv.2403.08763										arXiv		Simple and Scalable Strategies to Continually Pre-train Large Language Models	https://arxiv.org/abs/2403.08763	arXiv	nan; Keywords; References; Pages; Year; Bibtex
27	IFT3710	Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study	Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions. Our code is publicly available atthis https URL.		Shusheng Xu; Wei Fu; Jiaxuan Gao; Wenjie Ye; Weilin Liu; Zhiyu Mei; Guangju Wang; Chao Yu; Yi Wu	arXiv	https://doi.org/10.48550/arXiv.2404.10719										arXiv		Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study	https://arxiv.org/abs/2404.10719	arXiv	nan; Keywords; References; Pages; Year; Bibtex
28	IFT3710	Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction	"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine ""next-scale prediction"" or ""next-resolution prediction"", diverging from the standard raster-scan ""next-token prediction"". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning."		Keyu Tian; Yi Jiang; Zehuan Yuan; Bingyue Peng; Liwei Wang	arXiv	https://doi.org/10.48550/arXiv.2404.02905										arXiv		Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction	https://arxiv.org/abs/2404.02905	arXiv	nan; Keywords; References; Pages; Year; Bibtex
29	IFT3710	Vision Transformers Need Registers	Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.		Timothee Darcet; Maxime Oquab; Julien Mairal; Piotr Bojanowski	arXiv	https://doi.org/10.48550/arXiv.2309.16588										arXiv		Vision Transformers Need Registers	https://arxiv.org/abs/2309.16588	arXiv	nan; Keywords; References; Pages; Year; Bibtex
30	IFT3710	Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet					https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html													https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html		
31	IFT3710	Segment Anything	We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images atthis https URLto foster research into foundation models for computer vision.		Alexander Kirillov; Eric Mintun; Nikhila Ravi; Hanzi Mao; Chloe Rolland; Laura Gustafson; Tete Xiao; Spencer Whitehead; Alexander C. Berg; Wan-Yen Lo; Piotr Dollar; Ross Girshick	arXiv	https://doi.org/10.48550/arXiv.2304.02643										arXiv		Segment Anything	https://arxiv.org/abs/2304.02643	arXiv	nan; Keywords; References; Pages; Year; Bibtex
32	IFT3710	The Platonic Representation Hypothesis					https://arxiv.org/pdf/2405.07987													https://arxiv.org/pdf/2405.07987		
33	IFT3710	The Llama 3 Herd of Models	Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.		Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Ahmad Al-Dahle; Aiesha Letman; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Angela Fan; Anirudh Goyal; Anthony Hartshorn; Aobo Yang; Archi Mitra; Archie Sravankumar; Artem Korenev; Arthur Hinsvark; Arun Rao; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Bethany Biron; Binh Tang; Bobbie Chern; Charlotte Caucheteux; Chaya Nayak; Chloe Bi; Chris Marra; Chris McConnell; Christian Keller; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Cyrus Nikolaidis; Damien Allonsius; Daniel Song; Danielle Pintz; Danny Livshits; Danny Wyatt; David Esiobu; Dhruv Choudhary; Dhruv Mahajan; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Egor Lakomkin; Ehab AlBadawy; Elina Lobanova; Emily Dinan; Eric Michael Smith; Filip Radenovic; Francisco Guzman; Frank Zhang; Gabriel Synnaeve; Gabrielle Lee; Georgia Lewis Anderson; Govind Thattai; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hu Xu; Hugo Touvron; Iliyan Zarov; Imanol Arrieta Ibarra; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jack Zhang; Jade Copet; Jaewon Lee; Jan Geffert; Jana Vranes; Jason Park; Jay Mahadeokar; Jeet Shah; Jelmer van der Linde; Jennifer Billock; Jenny Hong; Jenya Lee; Jeremy Fu; Jianfeng Chi; Jianyu Huang; Jiawen Liu; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Joshua Saxe; Junteng Jia; Kalyan Vasuden Alwala; Karthik Prasad; Kartikeya Upasani; Kate Plawiak; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kushal Lakhotia; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Liang Tan; Liz Jenkins; Louis Martin; Lovish Madaan; Lubo Malo; Lukas Blecher; Lukas Landzaat; Luke de Oliveira; Madeline Muzzi; Mahesh Pasupuleti; Mannat Singh; Manohar Paluri; Marcin Kardas; Maria Tsimpoukelli; Mathew Oldham; Mathieu Rita; Maya Pavlova; Melanie Kambadur; Mike Lewis; Min Si; Mitesh Kumar Singh; Mona Hassan; Naman Goyal; Narjes Torabi; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Zhang; Olivier Duchenne; Onur Celebi; Patrick Alrassy; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Puxin Xu; Qing He; Qingxiao Dong; Ragavan Srinivasan; Raj Ganapathy; Ramon Calderer; Ricardo Silveira Cabral; Robert Stojnic; Roberta Raileanu; Rohan Maheswari; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Saghar Hosseini; Sahana Chennabasappa; Sanjay Singh; Sean Bell; Seohyun Sonia Kim; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Sheng Shen; Shengye Wan; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Soumya Batra; Spencer Whitman; Sten Sootla; Stephane Collot; Suchin Gururangan; Sydney Borodinsky; Tamar Herman; Tara Fowler; Tarek Sheasha; Thomas Georgiou; Thomas Scialom; Tobias Speckbacher; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vincent Gonguet; Virginie Do; Vish Vogeti; Vitor Albiero; Vladan Petrovic; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Whitney Meers; Xavier Martinet; Xiaodong Wang; Xiaofang Wang; Xiaoqing Ellen Tan; Xide Xia; Xinfeng Xie; Xuchao Jia; Xuewei Wang; Yaelle Goldschlag; Yashesh Gaur; Yasmine Babaei; Yi Wen; Yiwen Song; Yuchen Zhang; Yue Li; Yuning Mao; Zacharie Delpierre Coudert; Zheng Yan; Zhengxing Chen; Zoe Papakipos; Aaditya Singh; Aayushi Srivastava; Abha Jain; Adam Kelsey; Adam Shajnfeld; Adithya Gangidi; Adolfo Victoria; Ahuva Goldstand; Ajay Menon; Ajay Sharma; Alex Boesenberg; Alexei Baevski; Allie Feinstein; Amanda Kallet; Amit Sangani; Amos Teo; Anam Yunus; Andrei Lupu; Andres Alvarado; Andrew Caples; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Ankit Ramchandani; Annie Dong; Annie Franco; Anuj Goyal; Aparajita Saraf; Arkabandhu Chowdhury; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Azadeh Yazdan; Beau James; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Beth Loyd; Beto De Paola; Bhargavi Paranjape; Bing Liu; Bo Wu; Boyu Ni; Braden Hancock; Bram Wasti; Brandon Spence; Brani Stojkovic; Brian Gamido; Britt Montalvo; Carl Parker; Carly Burton; Catalina Mejia; Ce Liu; Changhan Wang; Changkyu Kim; Chao Zhou; Chester Hu; Ching-Hsiang Chu; Chris Cai; Chris Tindal; Christoph Feichtenhofer; Cynthia Gao; Damon Civin; Dana Beaty; Daniel Kreymer; Daniel Li; David Adkins; David Xu; Davide Testuggine; Delia David; Devi Parikh; Diana Liskovich; Didem Foss; Dingkang Wang; Duc Le; Dustin Holland; Edward Dowling; Eissa Jamil; Elaine Montgomery; Eleonora Presani; Emily Hahn; Emily Wood; Eric-Tuan Le; Erik Brinkman; Esteban Arcaute; Evan Dunbar; Evan Smothers; Fei Sun; Felix Kreuk; Feng Tian; Filippos Kokkinos; Firat Ozgenel; Francesco Caggioni; Frank Kanayet; Frank Seide; Gabriela Medina Florez; Gabriella Schwarz; Gada Badeer; Georgia Swee; Gil Halpern; Grant Herman; Grigory Sizov; Guangyi; Zhang; Guna Lakshminarayanan; Hakan Inan; Hamid Shojanazeri; Han Zou; Hannah Wang; Hanwen Zha; Haroun Habeeb; Harrison Rudolph; Helen Suk; Henry Aspegren; Hunter Goldman; Hongyuan Zhan; Ibrahim Damlaj; Igor Molybog; Igor Tufanov; Ilias Leontiadis; Irina-Elena Veliche; Itai Gat; Jake Weissman; James Geboski; James Kohli; Janice Lam; Japhet Asher; Jean-Baptiste Gaya; Jeff Marcus; Jeff Tang; Jennifer Chan; Jenny Zhen; Jeremy Reizenstein; Jeremy Teboul; Jessica Zhong; Jian Jin; Jingyi Yang; Joe Cummings; Jon Carvill; Jon Shepard; Jonathan McPhie; Jonathan Torres; Josh Ginsburg; Junjie Wang; Kai Wu; Kam Hou U; Karan Saxena; Kartikay Khandelwal; Katayoun Zand; Kathy Matosich; Kaushik Veeraraghavan; Kelly Michelena; Keqian Li; Kiran Jagadeesh; Kun Huang; Kunal Chawla; Kyle Huang; Lailin Chen; Lakshya Garg; Lavender A; Leandro Silva; Lee Bell; Lei Zhang; Liangpeng Guo; Licheng Yu; Liron Moshkovich; Luca Wehrstedt; Madian Khabsa; Manav Avalani; Manish Bhatt; Martynas Mankus; Matan Hasson; Matthew Lennie; Matthias Reso; Maxim Groshev; Maxim Naumov; Maya Lathi; Meghan Keneally; Miao Liu; Michael L. Seltzer; Michal Valko; Michelle Restrepo; Mihir Patel; Mik Vyatskov; Mikayel Samvelyan; Mike Clark; Mike Macey; Mike Wang; Miquel Jubert Hermoso; Mo Metanat; Mohammad Rastegari; Munish Bansal; Nandhini Santhanam; Natascha Parks; Natasha White; Navyata Bawa; Nayan Singhal; Nick Egebo; Nicolas Usunier; Nikhil Mehta; Nikolay Pavlovich Laptev; Ning Dong; Norman Cheng; Oleg Chernoguz; Olivia Hart; Omkar Salpekar; Ozlem Kalinli; Parkin Kent; Parth Parekh; Paul Saab; Pavan Balaji; Pedro Rittner; Philip Bontrager; Pierre Roux; Piotr Dollar; Polina Zvyagina; Prashant Ratanchandani; Pritish Yuvraj; Qian Liang; Rachad Alao; Rachel Rodriguez; Rafi Ayub; Raghotham Murthy; Raghu Nayani; Rahul Mitra; Rangaprabhu Parthasarathy; Raymond Li; Rebekkah Hogan; Robin Battey; Rocky Wang; Russ Howes; Ruty Rinott; Sachin Mehta; Sachin Siby; Sai Jayesh Bondu; Samyak Datta; Sara Chugh; Sara Hunt; Sargun Dhillon; Sasha Sidorov; Satadru Pan; Saurabh Mahajan; Saurabh Verma; Seiji Yamamoto; Sharadh Ramaswamy; Shaun Lindsay; Shaun Lindsay; Sheng Feng; Shenghao Lin; Shengxin Cindy Zha; Shishir Patil; Shiva Shankar; Shuqiang Zhang; Shuqiang Zhang; Sinong Wang; Sneha Agarwal; Soji Sajuyigbe; Soumith Chintala; Stephanie Max; Stephen Chen; Steve Kehoe; Steve Satterfield; Sudarshan Govindaprasad; Sumit Gupta; Summer Deng; Sungmin Cho; Sunny Virk; Suraj Subramanian; Sy Choudhury; Sydney Goldman; Tal Remez; Tamar Glaser; Tamara Best; Thilo Koehler; Thomas Robinson; Tianhe Li; Tianjun Zhang; Tim Matthews; Timothy Chou; Tzook Shaked; Varun Vontimitta; Victoria Ajayi; Victoria Montanez; Vijai Mohan; Vinay Satish Kumar; Vishal Mangla; Vlad Ionescu; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladimir Ivanov; Wei Li; Wenchen Wang; Wenwen Jiang; Wes Bouaziz; Will Constable; Xiaocheng Tang; Xiaojian Wu; Xiaolan Wang; Xilun Wu; Xinbo Gao; Yaniv Kleinman; Yanjun Chen; Ye Hu; Ye Jia; Ye Qi; Yenda Li; Yilin Zhang; Ying Zhang; Yossi Adi; Youngjin Nam; Wang; Yu Zhao; Yuchen Hao; Yundi Qian; Yunlu Li; Yuzi He; Zach Rait; Zachary DeVito; Zef Rosnbrick; Zhaoduo Wen; Zhenyu Yang; Zhiwei Zhao; Zhiyu Ma; et al. ( additional authors not shown)	arXiv	https://doi.org/10.48550/arXiv.2407.21783										arXiv		The Llama 3 Herd of Models	https://arxiv.org/abs/2407.21783	arXiv	nan; Keywords; References; Pages; Year; Bibtex
34	IFT3710	AFlow: Automating Agentic Workflow Generation					https://arxiv.org/pdf/2410.10762													https://arxiv.org/pdf/2410.10762		
35	IFT3710	The Alberta Plan for AI Research					https://arxiv.org/pdf/2208.11173													https://arxiv.org/pdf/2208.11173		
36	IFT3710	Automating the Search for Artificial Life with Foundation Models	With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.		Akarsh Kumar; Chris Lu; Louis Kirsch; Yujin Tang; Kenneth O. Stanley; Phillip Isola; David Ha	arXiv	https://doi.org/10.48550/arXiv.2412.17799										arXiv		Automating the Search for Artificial Life with Foundation Models	https://arxiv.org/abs/2412.17799	arXiv	nan; Keywords; References; Pages; Year; Bibtex
37	IFT3710	Loss of plasticity in deep continual learning					https://www.nature.com/articles/s41586-024-07711-7													https://www.nature.com/articles/s41586-024-07711-7		
38	IFT3710	Emergence and Causality in Complex Systems: A Survey of Causal Emergence and Related Quantitative Studies					https://www.mdpi.com/1099-4300/26/2/108													https://www.mdpi.com/1099-4300/26/2/108		
39	IFT3710	A Theory for Emergence of Complex Skills in Language Models	A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory.		Sanjeev Arora; Anirudh Goyal	arXiv	https://doi.org/10.48550/arXiv.2307.15936										arXiv		A Theory for Emergence of Complex Skills in Language Models	https://arxiv.org/abs/2307.15936	arXiv	nan; Keywords; References; Pages; Year; Bibtex
40	IFT3710	Genie: Generative Interactive Environments					https://arxiv.org/pdf/2402.15391													https://arxiv.org/pdf/2402.15391		
41	IFT3710	Understanding World or Predicting Future? A Comprehensive Survey of World Models	The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions.		Jingtao Ding; Yunke Zhang; Yu Shang; Yuheng Zhang; Zefang Zong; Jie Feng; Yuan Yuan; Hongyuan Su; Nian Li; Nicholas Sukiennik; Fengli Xu; Yong Li	arXiv	https://doi.org/10.48550/arXiv.2411.14499										arXiv		Understanding World or Predicting Future? A Comprehensive Survey of World Models	https://arxiv.org/abs/2411.14499	arXiv	nan; Keywords; References; Pages; Year; Bibtex
42	IFT3710	Locating and Editing Factual Associations in GPT	We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available atthis https URL		Kevin Meng; David Bau; Alex Andonian; Yonatan Belinkov	arXiv	https://doi.org/10.48550/arXiv.2202.05262										arXiv		Locating and Editing Factual Associations in GPT	https://arxiv.org/abs/2202.05262	arXiv	nan; Keywords; References; Pages; Year; Bibtex
43	IFT3710	Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning	Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open-source LLMs.		Chaojie Wang; Yanchen Deng; Zhiyi Lyu; Liang Zeng; Jujie He; Shuicheng Yan; Bo An	arXiv	https://doi.org/10.48550/arXiv.2406.14283										arXiv		Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning	https://arxiv.org/abs/2406.14283	arXiv	nan; Keywords; References; Pages; Year; Bibtex
44	IFT3710	Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective					[2412.14135] Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective													[2412.14135] Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective		
45	IFT3710	Neuroscience-Inspired Articial Intelligence					https://www.cell.com/neuron/pdf/S0896-6273%2817%2930509-3.pdf													https://www.cell.com/neuron/pdf/S0896-6273%2817%2930509-3.pdf		
46	IFT3710	ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing	Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.		Ian Arawjo; Chelse Swoopes; Priyan Vaithilingam; Martin Wattenberg; Elena Glassman	arXiv	https://doi.org/10.48550/arXiv.2309.09128										arXiv		ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing	https://arxiv.org/abs/2309.09128	arXiv	nan; Keywords; References; Pages; Year; Bibtex
